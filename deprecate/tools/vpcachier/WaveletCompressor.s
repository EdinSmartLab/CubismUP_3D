	.file	"WaveletCompressor.cpp"
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,comdat
	.align 2
.LCOLDB0:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,comdat
.LHOTB0:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv
	.type	_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv, @function
_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv:
.LFB1380:
	.cfi_startproc
	leaq	131104(%rdi), %rax
	ret
	.cfi_endproc
.LFE1380:
	.size	_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv, .-_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,comdat
.LCOLDE0:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv,comdat
.LHOTE0:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,comdat
	.align 2
.LCOLDB1:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,comdat
.LHOTB1:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f
	.type	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f, @function
_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f:
.LFB1384:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movzbl	%sil, %esi
	pushq	-8(%r10)
	pushq	%rbp
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	movq	%rdi, %r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	movq	%r8, %rbx
	subq	$24, %rsp
	movq	(%rdi), %rax
	call	*24(%rax)
	leaq	20(%r12), %rdx
	leaq	32(%rbx), %rax
	cmpq	%rax, %rdx
	jae	.L20
	leaq	52(%r12), %rax
	cmpq	%rax, %rbx
	jb	.L12
.L20:
	andl	$31, %edx
	movq	%rdx, %rax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	je	.L13
	vmovss	20(%r12), %xmm0
	vmovss	%xmm0, (%rbx)
	cmpl	$1, %eax
	je	.L14
	vmovss	24(%r12), %xmm0
	vmovss	%xmm0, 4(%rbx)
	cmpl	$2, %eax
	je	.L15
	vmovss	28(%r12), %xmm0
	vmovss	%xmm0, 8(%rbx)
	cmpl	$3, %eax
	je	.L16
	vmovss	32(%r12), %xmm0
	vmovss	%xmm0, 12(%rbx)
	cmpl	$4, %eax
	je	.L17
	vmovss	36(%r12), %xmm0
	vmovss	%xmm0, 16(%rbx)
	cmpl	$5, %eax
	je	.L18
	vmovss	40(%r12), %xmm0
	vmovss	%xmm0, 20(%rbx)
	cmpl	$7, %eax
	jne	.L19
	vmovss	44(%r12), %xmm0
	movl	$32761, %r10d
	movl	$7, %r11d
	vmovss	%xmm0, 24(%rbx)
.L6:
	movl	$32768, %r8d
	movl	%eax, %edx
	movl	$32760, %r9d
	subl	%eax, %r8d
	movl	$4095, %ecx
.L5:
	leaq	20(,%rdx,4), %rax
	xorl	%edx, %edx
	leaq	(%r12,%rax), %rdi
	leaq	-20(%rbx,%rax), %rsi
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L7:
	vmovaps	(%rdi,%rax), %ymm0
	addl	$1, %edx
	vmovups	%ymm0, (%rsi,%rax)
	addq	$32, %rax
	cmpl	%ecx, %edx
	jb	.L7
	leal	(%r11,%r9), %eax
	movl	%r10d, %edx
	subl	%r9d, %edx
	cmpl	%r9d, %r8d
	je	.L2
	movslq	%eax, %rcx
	vmovss	20(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	1(%rax), %ecx
	cmpl	$1, %edx
	je	.L2
	movslq	%ecx, %rcx
	vmovss	20(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	2(%rax), %ecx
	cmpl	$2, %edx
	je	.L2
	movslq	%ecx, %rcx
	vmovss	20(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	3(%rax), %ecx
	cmpl	$3, %edx
	je	.L2
	movslq	%ecx, %rcx
	vmovss	20(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	4(%rax), %ecx
	cmpl	$4, %edx
	je	.L2
	movslq	%ecx, %rcx
	vmovss	20(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	5(%rax), %ecx
	cmpl	$5, %edx
	je	.L2
	movslq	%ecx, %rcx
	addl	$6, %eax
	vmovss	20(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	cmpl	$6, %edx
	je	.L2
	cltq
	vmovss	20(%r12,%rax,4), %xmm0
	vmovss	%xmm0, (%rbx,%rax,4)
.L2:
	addq	$24, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L12:
	.cfi_restore_state
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L3:
	vmovss	(%rdx,%rax), %xmm0
	vmovss	%xmm0, (%rbx,%rax)
	addq	$4, %rax
	cmpq	$131072, %rax
	jne	.L3
	addq	$24, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L13:
	.cfi_restore_state
	movl	$32768, %r9d
	movl	$4096, %ecx
	movl	$32768, %r8d
	xorl	%edx, %edx
	movl	$32768, %r10d
	xorl	%r11d, %r11d
	jmp	.L5
.L19:
	movl	$32762, %r10d
	movl	$6, %r11d
	jmp	.L6
.L14:
	movl	$32767, %r10d
	movl	$1, %r11d
	jmp	.L6
.L15:
	movl	$32766, %r10d
	movl	$2, %r11d
	jmp	.L6
.L16:
	movl	$32765, %r10d
	movl	$3, %r11d
	jmp	.L6
.L17:
	movl	$32764, %r10d
	movl	$4, %r11d
	jmp	.L6
.L18:
	movl	$32763, %r10d
	movl	$5, %r11d
	jmp	.L6
	.cfi_endproc
.LFE1384:
	.size	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f, .-_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,comdat
.LCOLDE1:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f,comdat
.LHOTE1:
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,comdat
	.align 2
.LCOLDB2:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,comdat
.LHOTB2:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv
	.type	_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv, @function
_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv:
.LFB1387:
	.cfi_startproc
	leaq	266280(%rdi), %rax
	ret
	.cfi_endproc
.LFE1387:
	.size	_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv, .-_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,comdat
.LCOLDE2:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv,comdat
.LHOTE2:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,comdat
	.align 2
.LCOLDB3:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,comdat
.LHOTB3:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv
	.type	_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv, @function
_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv:
.LFB1392:
	.cfi_startproc
	leaq	67108896(%rdi), %rax
	ret
	.cfi_endproc
.LFE1392:
	.size	_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv, .-_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,comdat
.LCOLDE3:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv,comdat
.LHOTE3:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,comdat
	.align 2
.LCOLDB4:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,comdat
.LHOTB4:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f
	.type	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f, @function
_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f:
.LFB1396:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movzbl	%sil, %esi
	pushq	-8(%r10)
	pushq	%rbp
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	movq	%rdi, %r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	movq	%r8, %rbx
	subq	$24, %rsp
	movq	(%rdi), %rax
	call	*24(%rax)
	leaq	24(%r12), %rdx
	leaq	32(%rbx), %rax
	cmpq	%rax, %rdx
	jae	.L61
	leaq	56(%r12), %rax
	cmpq	%rax, %rbx
	jb	.L53
.L61:
	andl	$31, %edx
	movq	%rdx, %rax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	je	.L54
	vmovss	24(%r12), %xmm0
	vmovss	%xmm0, (%rbx)
	cmpl	$1, %eax
	je	.L55
	vmovss	28(%r12), %xmm0
	vmovss	%xmm0, 4(%rbx)
	cmpl	$2, %eax
	je	.L56
	vmovss	32(%r12), %xmm0
	vmovss	%xmm0, 8(%rbx)
	cmpl	$3, %eax
	je	.L57
	vmovss	36(%r12), %xmm0
	vmovss	%xmm0, 12(%rbx)
	cmpl	$4, %eax
	je	.L58
	vmovss	40(%r12), %xmm0
	vmovss	%xmm0, 16(%rbx)
	cmpl	$5, %eax
	je	.L59
	vmovss	44(%r12), %xmm0
	vmovss	%xmm0, 20(%rbx)
	cmpl	$7, %eax
	jne	.L60
	vmovss	48(%r12), %xmm0
	movl	$16777209, %r10d
	movl	$7, %r11d
	vmovss	%xmm0, 24(%rbx)
.L47:
	movl	$16777216, %r8d
	movl	%eax, %edx
	movl	$16777208, %r9d
	subl	%eax, %r8d
	movl	$2097151, %ecx
.L46:
	leaq	24(,%rdx,4), %rax
	xorl	%edx, %edx
	leaq	(%r12,%rax), %rdi
	leaq	-24(%rbx,%rax), %rsi
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L48:
	vmovaps	(%rdi,%rax), %ymm0
	addl	$1, %edx
	vmovups	%ymm0, (%rsi,%rax)
	addq	$32, %rax
	cmpl	%ecx, %edx
	jb	.L48
	leal	(%r11,%r9), %eax
	movl	%r10d, %edx
	subl	%r9d, %edx
	cmpl	%r9d, %r8d
	je	.L43
	movslq	%eax, %rcx
	vmovss	24(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	1(%rax), %ecx
	cmpl	$1, %edx
	je	.L43
	movslq	%ecx, %rcx
	vmovss	24(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	2(%rax), %ecx
	cmpl	$2, %edx
	je	.L43
	movslq	%ecx, %rcx
	vmovss	24(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	3(%rax), %ecx
	cmpl	$3, %edx
	je	.L43
	movslq	%ecx, %rcx
	vmovss	24(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	4(%rax), %ecx
	cmpl	$4, %edx
	je	.L43
	movslq	%ecx, %rcx
	vmovss	24(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	leal	5(%rax), %ecx
	cmpl	$5, %edx
	je	.L43
	movslq	%ecx, %rcx
	addl	$6, %eax
	vmovss	24(%r12,%rcx,4), %xmm0
	vmovss	%xmm0, (%rbx,%rcx,4)
	cmpl	$6, %edx
	je	.L43
	cltq
	vmovss	24(%r12,%rax,4), %xmm0
	vmovss	%xmm0, (%rbx,%rax,4)
.L43:
	addq	$24, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L53:
	.cfi_restore_state
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L44:
	vmovss	(%rdx,%rax), %xmm0
	vmovss	%xmm0, (%rbx,%rax)
	addq	$4, %rax
	cmpq	$67108864, %rax
	jne	.L44
	addq	$24, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L54:
	.cfi_restore_state
	movl	$16777216, %r9d
	movl	$2097152, %ecx
	movl	$16777216, %r8d
	xorl	%edx, %edx
	movl	$16777216, %r10d
	xorl	%r11d, %r11d
	jmp	.L46
.L60:
	movl	$16777210, %r10d
	movl	$6, %r11d
	jmp	.L47
.L55:
	movl	$16777215, %r10d
	movl	$1, %r11d
	jmp	.L47
.L56:
	movl	$16777214, %r10d
	movl	$2, %r11d
	jmp	.L47
.L57:
	movl	$16777213, %r10d
	movl	$3, %r11d
	jmp	.L47
.L58:
	movl	$16777212, %r10d
	movl	$4, %r11d
	jmp	.L47
.L59:
	movl	$16777211, %r10d
	movl	$5, %r11d
	jmp	.L47
	.cfi_endproc
.LFE1396:
	.size	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f, .-_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,comdat
.LCOLDE4:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f,comdat
.LHOTE4:
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,comdat
	.align 2
.LCOLDB5:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,comdat
.LHOTB5:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv
	.type	_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv, @function
_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv:
.LFB1399:
	.cfi_startproc
	leaq	136314920(%rdi), %rax
	ret
	.cfi_endproc
.LFE1399:
	.size	_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv, .-_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,comdat
.LCOLDE5:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv,comdat
.LHOTE5:
	.section	.text.unlikely,"ax",@progbits
.LCOLDB6:
	.text
.LHOTB6:
	.p2align 4,,15
	.globl	_Z9swapbytesPhi
	.type	_Z9swapbytesPhi, @function
_Z9swapbytesPhi:
.LFB1368:
	.cfi_startproc
	testl	%esi, %esi
	jle	.L108
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	leal	-1(%rsi), %edx
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	movq	%rdx, %r12
	movq	%rdi, %rbp
	addq	$1, %rdx
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movl	%esi, %ebx
	movq	%rdi, %rsi
	subq	$16, %rsp
	.cfi_def_cfa_offset 48
	movq	%rsp, %rdi
	call	memcpy
	movzbl	(%rsp), %edx
	movslq	%ebx, %rax
	movb	%dl, -1(%rbp,%rax)
	cmpl	$1, %ebx
	je	.L82
	movzbl	1(%rsp), %eax
	movslq	%r12d, %r12
	movb	%al, -1(%rbp,%r12)
	cmpl	$2, %ebx
	je	.L82
	movzbl	2(%rsp), %edx
	leal	-2(%rbx), %eax
	cltq
	movb	%dl, -1(%rbp,%rax)
	cmpl	$3, %ebx
	je	.L82
	movzbl	3(%rsp), %edx
	leal	-3(%rbx), %eax
	cltq
	movb	%dl, -1(%rbp,%rax)
	cmpl	$4, %ebx
	je	.L82
	movzbl	4(%rsp), %edx
	leal	-4(%rbx), %eax
	cltq
	movb	%dl, -1(%rbp,%rax)
	cmpl	$5, %ebx
	je	.L82
	movzbl	5(%rsp), %edx
	leal	-5(%rbx), %eax
	cltq
	movb	%dl, -1(%rbp,%rax)
	cmpl	$6, %ebx
	je	.L82
	movzbl	6(%rsp), %edx
	leal	-6(%rbx), %eax
	cltq
	movb	%dl, -1(%rbp,%rax)
	cmpl	$7, %ebx
	je	.L82
	movzbl	7(%rsp), %eax
	subl	$7, %ebx
	movslq	%ebx, %rbx
	movb	%al, -1(%rbp,%rbx)
.L82:
	addq	$16, %rsp
	.cfi_def_cfa_offset 32
	popq	%rbx
	.cfi_restore 3
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_restore 6
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_restore 12
	.cfi_def_cfa_offset 8
.L108:
	ret
	.cfi_endproc
.LFE1368:
	.size	_Z9swapbytesPhi, .-_Z9swapbytesPhi
	.section	.text.unlikely
.LCOLDE6:
	.text
.LHOTE6:
	.section	.text.unlikely
.LCOLDB7:
	.text
.LHOTB7:
	.p2align 4,,15
	.globl	_Z8_cvt2f16f
	.type	_Z8_cvt2f16f, @function
_Z8_cvt2f16f:
.LFB1371:
	.cfi_startproc
	vmovd	%xmm0, %ecx
	vmovd	%xmm0, %eax
	vmovd	%xmm0, %edx
	andl	$2147483647, %ecx
	andl	$-2147483648, %eax
	shrl	$23, %ecx
	shrl	$16, %eax
	subl	$112, %ecx
	movl	%eax, %esi
	cmpl	$30, %ecx
	jg	.L112
	xorl	%eax, %eax
	testl	%ecx, %ecx
	jg	.L114
	andl	$8388607, %edx
	orl	%esi, %eax
	shrl	$13, %edx
	orl	%edx, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L112:
	movl	$31, %eax
.L110:
	sall	$10, %eax
	andl	$8388607, %edx
	orl	%esi, %eax
	shrl	$13, %edx
	orl	%edx, %eax
	ret
	.p2align 4,,10
	.p2align 3
.L114:
	movl	%ecx, %eax
	jmp	.L110
	.cfi_endproc
.LFE1371:
	.size	_Z8_cvt2f16f, .-_Z8_cvt2f16f
	.section	.text.unlikely
.LCOLDE7:
	.text
.LHOTE7:
	.section	.text.unlikely
.LCOLDB8:
	.text
.LHOTB8:
	.p2align 4,,15
	.globl	_Z11_cvtfromf16t
	.type	_Z11_cvtfromf16t, @function
_Z11_cvtfromf16t:
.LFB1372:
	.cfi_startproc
	movl	%edi, %eax
	movl	%edi, %edx
	andl	$31744, %edi
	andl	$1023, %eax
	andw	$-32768, %dx
	sall	$13, %edi
	sall	$13, %eax
	movl	%eax, %ecx
	movzwl	%dx, %eax
	leal	939524096(%rdi), %edx
	sall	$16, %eax
	orl	%ecx, %eax
	orl	%edx, %eax
	vmovd	%eax, %xmm0
	ret
	.cfi_endproc
.LFE1372:
	.size	_Z11_cvtfromf16t, .-_Z11_cvtfromf16t
	.section	.text.unlikely
.LCOLDE8:
	.text
.LHOTE8:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,comdat
	.align 2
.LCOLDB9:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,comdat
.LHOTB9:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv
	.type	_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv, @function
_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv:
.LFB1379:
	.cfi_startproc
	leaq	20(%rdi), %rax
	ret
	.cfi_endproc
.LFE1379:
	.size	_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv, .-_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,comdat
.LCOLDE9:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE17uncompressed_dataEv,comdat
.LHOTE9:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,comdat
	.align 2
.LCOLDB10:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,comdat
.LHOTB10:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f
	.type	_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f, @function
_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f:
.LFB1385:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	pushq	-8(%r10)
	leaq	20(%rdi), %rdx
	pushq	%rbp
	leaq	32(%rsi), %rax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	cmpq	%rax, %rdx
	jae	.L135
	leaq	52(%rdi), %rax
	cmpq	%rax, %rsi
	jb	.L127
.L135:
	andl	$31, %edx
	movq	%rdx, %rax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	je	.L128
	vmovss	20(%rdi), %xmm0
	vmovss	%xmm0, (%rsi)
	cmpl	$1, %eax
	je	.L129
	vmovss	24(%rdi), %xmm0
	vmovss	%xmm0, 4(%rsi)
	cmpl	$2, %eax
	je	.L130
	vmovss	28(%rdi), %xmm0
	vmovss	%xmm0, 8(%rsi)
	cmpl	$3, %eax
	je	.L131
	vmovss	32(%rdi), %xmm0
	vmovss	%xmm0, 12(%rsi)
	cmpl	$4, %eax
	je	.L132
	vmovss	36(%rdi), %xmm0
	vmovss	%xmm0, 16(%rsi)
	cmpl	$5, %eax
	je	.L133
	vmovss	40(%rdi), %xmm0
	vmovss	%xmm0, 20(%rsi)
	cmpl	$7, %eax
	jne	.L134
	vmovss	44(%rdi), %xmm0
	movl	$32761, %ebx
	movl	$7, %r12d
	vmovss	%xmm0, 24(%rsi)
.L121:
	movl	$32768, %r10d
	movl	%eax, %edx
	movl	$32760, %r11d
	subl	%eax, %r10d
	movl	$4095, %ecx
.L120:
	leaq	20(,%rdx,4), %rax
	xorl	%edx, %edx
	leaq	(%rdi,%rax), %r9
	leaq	-20(%rsi,%rax), %r8
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L122:
	vmovaps	(%r9,%rax), %ymm0
	addl	$1, %edx
	vmovups	%ymm0, (%r8,%rax)
	addq	$32, %rax
	cmpl	%ecx, %edx
	jb	.L122
	leal	(%r12,%r11), %eax
	subl	%r11d, %ebx
	cmpl	%r11d, %r10d
	je	.L117
	movslq	%eax, %rcx
	vmovss	20(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	1(%rax), %ecx
	cmpl	$1, %ebx
	je	.L117
	movslq	%ecx, %rcx
	vmovss	20(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	2(%rax), %ecx
	cmpl	$2, %ebx
	je	.L117
	movslq	%ecx, %rcx
	vmovss	20(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	3(%rax), %ecx
	cmpl	$3, %ebx
	je	.L117
	movslq	%ecx, %rcx
	vmovss	20(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	4(%rax), %ecx
	cmpl	$4, %ebx
	je	.L117
	movslq	%ecx, %rcx
	vmovss	20(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	5(%rax), %ecx
	cmpl	$5, %ebx
	je	.L117
	movslq	%ecx, %rcx
	addl	$6, %eax
	vmovss	20(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	cmpl	$6, %ebx
	je	.L117
	cltq
	vmovss	20(%rdi,%rax,4), %xmm0
	vmovss	%xmm0, (%rsi,%rax,4)
.L117:
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L127:
	.cfi_restore_state
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L118:
	vmovss	(%rdx,%rax), %xmm0
	vmovss	%xmm0, (%rsi,%rax)
	addq	$4, %rax
	cmpq	$131072, %rax
	jne	.L118
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L128:
	.cfi_restore_state
	movl	$32768, %r11d
	movl	$4096, %ecx
	movl	$32768, %r10d
	xorl	%edx, %edx
	movl	$32768, %ebx
	xorl	%r12d, %r12d
	jmp	.L120
.L134:
	movl	$32762, %ebx
	movl	$6, %r12d
	jmp	.L121
.L129:
	movl	$32767, %ebx
	movl	$1, %r12d
	jmp	.L121
.L130:
	movl	$32766, %ebx
	movl	$2, %r12d
	jmp	.L121
.L131:
	movl	$32765, %ebx
	movl	$3, %r12d
	jmp	.L121
.L132:
	movl	$32764, %ebx
	movl	$4, %r12d
	jmp	.L121
.L133:
	movl	$32763, %ebx
	movl	$5, %r12d
	jmp	.L121
	.cfi_endproc
.LFE1385:
	.size	_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f, .-_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,comdat
.LCOLDE10:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE7copy_toEPA32_A32_f,comdat
.LHOTE10:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,comdat
	.align 2
.LCOLDB11:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,comdat
.LHOTB11:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf
	.type	_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf, @function
_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf:
.LFB1386:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	pushq	-8(%r10)
	leaq	20(%rdi), %rdx
	pushq	%rbp
	leaq	32(%rsi), %rax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	cmpq	%rax, %rdx
	jae	.L173
	leaq	52(%rdi), %rax
	cmpq	%rax, %rsi
	jb	.L165
.L173:
	movq	%rsi, %rax
	andl	$31, %eax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	je	.L166
	vmovss	(%rsi), %xmm0
	vmovss	%xmm0, 20(%rdi)
	cmpl	$1, %eax
	je	.L167
	vmovss	4(%rsi), %xmm0
	vmovss	%xmm0, 24(%rdi)
	cmpl	$2, %eax
	je	.L168
	vmovss	8(%rsi), %xmm0
	vmovss	%xmm0, 28(%rdi)
	cmpl	$3, %eax
	je	.L169
	vmovss	12(%rsi), %xmm0
	vmovss	%xmm0, 32(%rdi)
	cmpl	$4, %eax
	je	.L170
	vmovss	16(%rsi), %xmm0
	vmovss	%xmm0, 36(%rdi)
	cmpl	$5, %eax
	je	.L171
	vmovss	20(%rsi), %xmm0
	vmovss	%xmm0, 40(%rdi)
	cmpl	$7, %eax
	jne	.L172
	vmovss	24(%rsi), %xmm0
	movl	$32761, %ebx
	movl	$7, %r12d
	vmovss	%xmm0, 44(%rdi)
.L159:
	movl	$32768, %r10d
	movl	%eax, %r8d
	movl	$32760, %r11d
	subl	%eax, %r10d
	movl	$4095, %ecx
.L158:
	salq	$2, %r8
	xorl	%eax, %eax
	xorl	%edx, %edx
	leaq	(%rsi,%r8), %r9
	leaq	20(%rdi,%r8), %r8
	.p2align 4,,10
	.p2align 3
.L160:
	vmovaps	(%r9,%rax), %ymm0
	addl	$1, %edx
	vmovups	%ymm0, (%r8,%rax)
	addq	$32, %rax
	cmpl	%ecx, %edx
	jb	.L160
	leal	(%r12,%r11), %eax
	subl	%r11d, %ebx
	cmpl	%r11d, %r10d
	je	.L155
	movslq	%eax, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 20(%rdi,%rcx,4)
	leal	1(%rax), %ecx
	cmpl	$1, %ebx
	je	.L155
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 20(%rdi,%rcx,4)
	leal	2(%rax), %ecx
	cmpl	$2, %ebx
	je	.L155
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 20(%rdi,%rcx,4)
	leal	3(%rax), %ecx
	cmpl	$3, %ebx
	je	.L155
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 20(%rdi,%rcx,4)
	leal	4(%rax), %ecx
	cmpl	$4, %ebx
	je	.L155
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 20(%rdi,%rcx,4)
	leal	5(%rax), %ecx
	cmpl	$5, %ebx
	je	.L155
	movslq	%ecx, %rcx
	addl	$6, %eax
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 20(%rdi,%rcx,4)
	cmpl	$6, %ebx
	je	.L155
	cltq
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm0, 20(%rdi,%rax,4)
.L155:
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L165:
	.cfi_restore_state
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L156:
	vmovss	(%rsi,%rax), %xmm0
	vmovss	%xmm0, (%rdx,%rax)
	addq	$4, %rax
	cmpq	$131072, %rax
	jne	.L156
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L166:
	.cfi_restore_state
	movl	$32768, %r11d
	movl	$4096, %ecx
	movl	$32768, %r10d
	xorl	%r8d, %r8d
	movl	$32768, %ebx
	xorl	%r12d, %r12d
	jmp	.L158
.L172:
	movl	$32762, %ebx
	movl	$6, %r12d
	jmp	.L159
.L167:
	movl	$32767, %ebx
	movl	$1, %r12d
	jmp	.L159
.L168:
	movl	$32766, %ebx
	movl	$2, %r12d
	jmp	.L159
.L169:
	movl	$32765, %ebx
	movl	$3, %r12d
	jmp	.L159
.L170:
	movl	$32764, %ebx
	movl	$4, %r12d
	jmp	.L159
.L171:
	movl	$32763, %ebx
	movl	$5, %r12d
	jmp	.L159
	.cfi_endproc
.LFE1386:
	.size	_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf, .-_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,comdat
.LCOLDE11:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE9copy_fromEPA32_A32_Kf,comdat
.LHOTE11:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,comdat
	.align 2
.LCOLDB12:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,comdat
.LHOTB12:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv
	.type	_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv, @function
_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv:
.LFB1391:
	.cfi_startproc
	leaq	24(%rdi), %rax
	ret
	.cfi_endproc
.LFE1391:
	.size	_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv, .-_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,comdat
.LCOLDE12:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE17uncompressed_dataEv,comdat
.LHOTE12:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,comdat
	.align 2
.LCOLDB13:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,comdat
.LHOTB13:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f
	.type	_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f, @function
_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f:
.LFB1397:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	pushq	-8(%r10)
	leaq	24(%rdi), %rdx
	pushq	%rbp
	leaq	32(%rsi), %rax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	cmpq	%rax, %rdx
	jae	.L212
	leaq	56(%rdi), %rax
	cmpq	%rax, %rsi
	jb	.L204
.L212:
	andl	$31, %edx
	movq	%rdx, %rax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	je	.L205
	vmovss	24(%rdi), %xmm0
	vmovss	%xmm0, (%rsi)
	cmpl	$1, %eax
	je	.L206
	vmovss	28(%rdi), %xmm0
	vmovss	%xmm0, 4(%rsi)
	cmpl	$2, %eax
	je	.L207
	vmovss	32(%rdi), %xmm0
	vmovss	%xmm0, 8(%rsi)
	cmpl	$3, %eax
	je	.L208
	vmovss	36(%rdi), %xmm0
	vmovss	%xmm0, 12(%rsi)
	cmpl	$4, %eax
	je	.L209
	vmovss	40(%rdi), %xmm0
	vmovss	%xmm0, 16(%rsi)
	cmpl	$5, %eax
	je	.L210
	vmovss	44(%rdi), %xmm0
	vmovss	%xmm0, 20(%rsi)
	cmpl	$7, %eax
	jne	.L211
	vmovss	48(%rdi), %xmm0
	movl	$16777209, %ebx
	movl	$7, %r12d
	vmovss	%xmm0, 24(%rsi)
.L198:
	movl	$16777216, %r10d
	movl	%eax, %edx
	movl	$16777208, %r11d
	subl	%eax, %r10d
	movl	$2097151, %ecx
.L197:
	leaq	24(,%rdx,4), %rax
	xorl	%edx, %edx
	leaq	(%rdi,%rax), %r9
	leaq	-24(%rsi,%rax), %r8
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L199:
	vmovaps	(%r9,%rax), %ymm0
	addl	$1, %edx
	vmovups	%ymm0, (%r8,%rax)
	addq	$32, %rax
	cmpl	%ecx, %edx
	jb	.L199
	leal	(%r12,%r11), %eax
	subl	%r11d, %ebx
	cmpl	%r11d, %r10d
	je	.L194
	movslq	%eax, %rcx
	vmovss	24(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	1(%rax), %ecx
	cmpl	$1, %ebx
	je	.L194
	movslq	%ecx, %rcx
	vmovss	24(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	2(%rax), %ecx
	cmpl	$2, %ebx
	je	.L194
	movslq	%ecx, %rcx
	vmovss	24(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	3(%rax), %ecx
	cmpl	$3, %ebx
	je	.L194
	movslq	%ecx, %rcx
	vmovss	24(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	4(%rax), %ecx
	cmpl	$4, %ebx
	je	.L194
	movslq	%ecx, %rcx
	vmovss	24(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	leal	5(%rax), %ecx
	cmpl	$5, %ebx
	je	.L194
	movslq	%ecx, %rcx
	addl	$6, %eax
	vmovss	24(%rdi,%rcx,4), %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	cmpl	$6, %ebx
	je	.L194
	cltq
	vmovss	24(%rdi,%rax,4), %xmm0
	vmovss	%xmm0, (%rsi,%rax,4)
.L194:
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L204:
	.cfi_restore_state
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L195:
	vmovss	(%rdx,%rax), %xmm0
	vmovss	%xmm0, (%rsi,%rax)
	addq	$4, %rax
	cmpq	$67108864, %rax
	jne	.L195
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L205:
	.cfi_restore_state
	movl	$16777216, %r11d
	movl	$2097152, %ecx
	movl	$16777216, %r10d
	xorl	%edx, %edx
	movl	$16777216, %ebx
	xorl	%r12d, %r12d
	jmp	.L197
.L211:
	movl	$16777210, %ebx
	movl	$6, %r12d
	jmp	.L198
.L206:
	movl	$16777215, %ebx
	movl	$1, %r12d
	jmp	.L198
.L207:
	movl	$16777214, %ebx
	movl	$2, %r12d
	jmp	.L198
.L208:
	movl	$16777213, %ebx
	movl	$3, %r12d
	jmp	.L198
.L209:
	movl	$16777212, %ebx
	movl	$4, %r12d
	jmp	.L198
.L210:
	movl	$16777211, %ebx
	movl	$5, %r12d
	jmp	.L198
	.cfi_endproc
.LFE1397:
	.size	_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f, .-_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,comdat
.LCOLDE13:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE7copy_toEPA256_A256_f,comdat
.LHOTE13:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,comdat
	.align 2
.LCOLDB14:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,comdat
.LHOTB14:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf
	.type	_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf, @function
_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf:
.LFB1398:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	pushq	-8(%r10)
	leaq	24(%rdi), %rdx
	pushq	%rbp
	leaq	32(%rsi), %rax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	cmpq	%rax, %rdx
	jae	.L250
	leaq	56(%rdi), %rax
	cmpq	%rax, %rsi
	jb	.L242
.L250:
	movq	%rsi, %rax
	andl	$31, %eax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	je	.L243
	vmovss	(%rsi), %xmm0
	vmovss	%xmm0, 24(%rdi)
	cmpl	$1, %eax
	je	.L244
	vmovss	4(%rsi), %xmm0
	vmovss	%xmm0, 28(%rdi)
	cmpl	$2, %eax
	je	.L245
	vmovss	8(%rsi), %xmm0
	vmovss	%xmm0, 32(%rdi)
	cmpl	$3, %eax
	je	.L246
	vmovss	12(%rsi), %xmm0
	vmovss	%xmm0, 36(%rdi)
	cmpl	$4, %eax
	je	.L247
	vmovss	16(%rsi), %xmm0
	vmovss	%xmm0, 40(%rdi)
	cmpl	$5, %eax
	je	.L248
	vmovss	20(%rsi), %xmm0
	vmovss	%xmm0, 44(%rdi)
	cmpl	$7, %eax
	jne	.L249
	vmovss	24(%rsi), %xmm0
	movl	$16777209, %ebx
	movl	$7, %r12d
	vmovss	%xmm0, 48(%rdi)
.L236:
	movl	$16777216, %r10d
	movl	%eax, %r8d
	movl	$16777208, %r11d
	subl	%eax, %r10d
	movl	$2097151, %ecx
.L235:
	salq	$2, %r8
	xorl	%eax, %eax
	xorl	%edx, %edx
	leaq	(%rsi,%r8), %r9
	leaq	24(%rdi,%r8), %r8
	.p2align 4,,10
	.p2align 3
.L237:
	vmovaps	(%r9,%rax), %ymm0
	addl	$1, %edx
	vmovups	%ymm0, (%r8,%rax)
	addq	$32, %rax
	cmpl	%ecx, %edx
	jb	.L237
	leal	(%r12,%r11), %eax
	subl	%r11d, %ebx
	cmpl	%r11d, %r10d
	je	.L232
	movslq	%eax, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 24(%rdi,%rcx,4)
	leal	1(%rax), %ecx
	cmpl	$1, %ebx
	je	.L232
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 24(%rdi,%rcx,4)
	leal	2(%rax), %ecx
	cmpl	$2, %ebx
	je	.L232
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 24(%rdi,%rcx,4)
	leal	3(%rax), %ecx
	cmpl	$3, %ebx
	je	.L232
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 24(%rdi,%rcx,4)
	leal	4(%rax), %ecx
	cmpl	$4, %ebx
	je	.L232
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 24(%rdi,%rcx,4)
	leal	5(%rax), %ecx
	cmpl	$5, %ebx
	je	.L232
	movslq	%ecx, %rcx
	addl	$6, %eax
	vmovss	(%rsi,%rcx,4), %xmm0
	vmovss	%xmm0, 24(%rdi,%rcx,4)
	cmpl	$6, %ebx
	je	.L232
	cltq
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm0, 24(%rdi,%rax,4)
.L232:
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L242:
	.cfi_restore_state
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L233:
	vmovss	(%rsi,%rax), %xmm0
	vmovss	%xmm0, (%rdx,%rax)
	addq	$4, %rax
	cmpq	$67108864, %rax
	jne	.L233
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L243:
	.cfi_restore_state
	movl	$16777216, %r11d
	movl	$2097152, %ecx
	movl	$16777216, %r10d
	xorl	%r8d, %r8d
	movl	$16777216, %ebx
	xorl	%r12d, %r12d
	jmp	.L235
.L249:
	movl	$16777210, %ebx
	movl	$6, %r12d
	jmp	.L236
.L244:
	movl	$16777215, %ebx
	movl	$1, %r12d
	jmp	.L236
.L245:
	movl	$16777214, %ebx
	movl	$2, %r12d
	jmp	.L236
.L246:
	movl	$16777213, %ebx
	movl	$3, %r12d
	jmp	.L236
.L247:
	movl	$16777212, %ebx
	movl	$4, %r12d
	jmp	.L236
.L248:
	movl	$16777211, %ebx
	movl	$5, %r12d
	jmp	.L236
	.cfi_endproc
.LFE1398:
	.size	_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf, .-_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,comdat
.LCOLDE14:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE9copy_fromEPA256_A256_Kf,comdat
.LHOTE14:
	.section	.text.unlikely._ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,comdat
	.align 2
.LCOLDB16:
	.section	.text._ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,comdat
.LHOTB16:
	.align 2
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf
	.type	_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf, @function
_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf:
.LFB1442:
	.cfi_startproc
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	xorl	%eax, %eax
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	movq	%rdx, %r14
	movl	$1, %edx
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	movq	%rsi, %r13
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	movq	%rcx, -40(%rsp)
.L271:
	movq	%rax, %rsi
	movl	%eax, %ecx
	movq	%rdx, %rbx
	shrq	$6, %rsi
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	1(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	1(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	2(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	2(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	3(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	3(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	32(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	32(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	33(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	33(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	34(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	34(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	35(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	35(%rax), %ecx
	salq	%cl, %rbx
	movl	%eax, %ecx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	64(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	65(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	65(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	66(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	66(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	67(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	67(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	96(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	96(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	97(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	97(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	98(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	98(%rax), %ecx
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	leaq	99(%rax), %rsi
	movq	%rdx, %rbx
	shrq	$6, %rsi
	leal	99(%rax), %ecx
	addq	$1024, %rax
	salq	%cl, %rbx
	orq	%rbx, 0(%r13,%rsi,8)
	cmpq	$4096, %rax
	jne	.L271
	movq	-40(%rsp), %rdx
	movq	%r14, %rax
	movl	$4, %ecx
.L272:
	vmovss	(%rdx), %xmm1
	addq	$64, %rax
	addq	$4096, %rdx
	vmovss	%xmm1, -64(%rax)
	vmovss	-4092(%rdx), %xmm1
	vmovss	%xmm1, -60(%rax)
	vmovss	-4088(%rdx), %xmm1
	vmovss	%xmm1, -56(%rax)
	vmovss	-4084(%rdx), %xmm1
	vmovss	%xmm1, -52(%rax)
	vmovss	-3968(%rdx), %xmm1
	vmovss	%xmm1, -48(%rax)
	vmovss	-3964(%rdx), %xmm1
	vmovss	%xmm1, -44(%rax)
	vmovss	-3960(%rdx), %xmm1
	vmovss	%xmm1, -40(%rax)
	vmovss	-3956(%rdx), %xmm1
	vmovss	%xmm1, -36(%rax)
	vmovss	-3840(%rdx), %xmm1
	vmovss	%xmm1, -32(%rax)
	vmovss	-3836(%rdx), %xmm1
	vmovss	%xmm1, -28(%rax)
	vmovss	-3832(%rdx), %xmm1
	vmovss	%xmm1, -24(%rax)
	vmovss	-3828(%rdx), %xmm1
	vmovss	%xmm1, -20(%rax)
	vmovss	-3712(%rdx), %xmm1
	vmovss	%xmm1, -16(%rax)
	vmovss	-3708(%rdx), %xmm1
	vmovss	%xmm1, -12(%rax)
	vmovss	-3704(%rdx), %xmm1
	vmovss	%xmm1, -8(%rax)
	vmovss	-3700(%rdx), %xmm1
	vmovss	%xmm1, -4(%rax)
	subl	$1, %ecx
	jne	.L272
	movl	$1, -8(%rsp)
	movl	-8(%rsp), %eax
	xorl	%r15d, %r15d
	vmovss	.LC15(%rip), %xmm3
	movl	$1, %r12d
.L286:
	leal	(%rax,%rax), %r11d
	movl	%eax, %esi
	sarl	$2, %eax
	leal	0(,%rax,4), %edx
	andl	$4, %r11d
	sall	$7, %eax
	andl	$1, %esi
	addl	%r11d, %eax
	movslq	%edx, %rdx
	sall	$2, %esi
	sall	$5, %eax
	movslq	%esi, %r9
	movslq	%eax, %rcx
	salq	$12, %rdx
	addq	-40(%rsp), %rdx
	leaq	(%rcx,%r9), %rbx
	leal	3(%rsi), %ecx
	movq	%rbx, -16(%rsp)
	leal	1(%rsi), %r10d
	movslq	%ecx, %rdi
	leal	(%rcx,%rax), %ebx
	movslq	%r10d, %r10
	movq	%rdi, -64(%rsp)
	subq	%r9, %rdi
	leal	4099(%rsi,%rax), %eax
	movq	%r10, %rcx
	movq	%rdi, -24(%rsp)
	movl	%eax, -4(%rsp)
	leal	2(%rsi), %eax
	subq	%r9, %rcx
	cltq
	movq	%rcx, -56(%rsp)
	movq	%rax, -48(%rsp)
	subq	%r9, %rax
	movq	%rax, -32(%rsp)
.L273:
	movq	-16(%rsp), %rcx
	movl	%ebx, %esi
	xorl	%r8d, %r8d
.L285:
	leal	(%r11,%r8), %eax
	movq	%rcx, %rbp
	cltq
	shrq	$6, %rbp
	movq	%rax, %rdi
	salq	$5, %rdi
	addq	%r9, %rdi
	vmovss	(%rdx,%rdi,4), %xmm1
	leal	-3(%rsi), %edi
	andl	$63, %edi
	vmovaps	%xmm1, %xmm2
	shlx	%rdi, %r12, %rdi
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	ja	.L276
	notq	%rdi
	andq	%rdi, 0(%r13,%rbp,8)
.L277:
	movq	%rax, %rdi
	leal	-2(%rsi), %ebp
	salq	$5, %rdi
	addq	%r10, %rdi
	andl	$63, %ebp
	vmovss	(%rdx,%rdi,4), %xmm1
	shlx	%rbp, %r12, %rbp
	movq	-56(%rsp), %rdi
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	addq	%rcx, %rdi
	shrq	$6, %rdi
	vucomiss	%xmm0, %xmm2
	ja	.L278
	notq	%rbp
	andq	%rbp, 0(%r13,%rdi,8)
.L279:
	movq	%rax, %rdi
	leal	-1(%rsi), %ebp
	salq	$5, %rdi
	addq	-48(%rsp), %rdi
	andl	$63, %ebp
	shlx	%rbp, %r12, %rbp
	vmovss	(%rdx,%rdi,4), %xmm1
	movq	-32(%rsp), %rdi
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	addq	%rcx, %rdi
	shrq	$6, %rdi
	vucomiss	%xmm0, %xmm2
	ja	.L280
	notq	%rbp
	andq	%rbp, 0(%r13,%rdi,8)
.L281:
	salq	$5, %rax
	addq	-64(%rsp), %rax
	movl	%esi, %edi
	andl	$63, %edi
	vmovss	(%rdx,%rax,4), %xmm1
	movq	-24(%rsp), %rax
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	addq	%rcx, %rax
	shrq	$6, %rax
	vucomiss	%xmm0, %xmm2
	ja	.L318
	shlx	%rdi, %r12, %rdi
	notq	%rdi
	andq	%rdi, 0(%r13,%rax,8)
.L274:
	addq	$1, %r8
	addq	$32, %rcx
	addl	$32, %esi
	cmpq	$4, %r8
	jne	.L285
	addq	$4096, %rdx
	addl	$1024, %ebx
	addq	$1024, -16(%rsp)
	cmpl	-4(%rsp), %ebx
	jne	.L273
	addl	$1, -8(%rsp)
	movl	-8(%rsp), %eax
	cmpl	$8, %eax
	jne	.L286
	leal	64(%r15), %eax
	movl	$1, %r12d
	xorl	%r15d, %r15d
	movl	$1, -56(%rsp)
	movl	%eax, -24(%rsp)
	cltq
	leaq	0(,%rax,4), %rbp
.L295:
	movl	-56(%rsp), %ebx
	movl	%ebx, %eax
	andl	$1, %eax
	sall	$3, %eax
	movl	%eax, %edi
	movl	%eax, -64(%rsp)
	movl	%ebx, %eax
	sall	$2, %eax
	andl	$8, %eax
	movl	%eax, %ecx
	movl	%eax, -32(%rsp)
	movl	%ebx, %eax
	sarl	$2, %eax
	leal	0(,%rax,8), %r9d
	sall	$8, %eax
	addl	%ecx, %eax
	movslq	%r9d, %r9
	movslq	%edi, %rcx
	leal	8(%rax), %edx
	salq	$12, %r9
	addq	-40(%rsp), %r9
	sall	$5, %edx
	movl	%edx, -8(%rsp)
	leal	8(%rcx), %r10d
	movl	%eax, %edx
	addl	$264, %eax
	sall	$5, %eax
	sall	$5, %edx
	movl	%eax, -16(%rsp)
	movl	-8(%rsp), %eax
	movslq	%edx, %rdx
	leaq	(%rdx,%rcx), %rdi
	movq	%rdi, -48(%rsp)
.L287:
	movl	-32(%rsp), %ebx
	leal	-256(%rax), %edi
	movq	-48(%rsp), %r11
	.p2align 4,,10
	.p2align 3
.L294:
	movslq	%ebx, %r8
	movl	-64(%rsp), %eax
	movq	%r11, %rcx
	salq	$5, %r8
	jmp	.L291
	.p2align 4,,10
	.p2align 3
.L315:
	addl	$1, %eax
	notq	%rdx
	addq	$1, %rcx
	andq	%rdx, 0(%r13,%rsi,8)
	cmpl	%r10d, %eax
	je	.L319
.L291:
	movslq	%eax, %rdx
	movq	%rcx, %rsi
	addq	%r8, %rdx
	shrq	$6, %rsi
	vmovss	(%r9,%rdx,4), %xmm1
	leal	(%rax,%rdi), %edx
	andl	$63, %edx
	vmovaps	%xmm1, %xmm2
	shlx	%rdx, %r12, %rdx
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	jbe	.L315
	orq	%rdx, 0(%r13,%rsi,8)
	movslq	%r15d, %rdx
	addl	$1, %eax
	addl	$1, %r15d
	addq	$1, %rcx
	leaq	0(%rbp,%rdx,4), %rdx
	vmovss	%xmm1, (%r14,%rdx)
	cmpl	%r10d, %eax
	jne	.L291
.L319:
	addq	$32, %r11
	addl	$1, %ebx
	addl	$32, %edi
	cmpl	-8(%rsp), %edi
	jne	.L294
	leal	1024(%rdi), %eax
	addq	$4096, %r9
	addq	$1024, -48(%rsp)
	movl	%eax, -8(%rsp)
	cmpl	-16(%rsp), %eax
	jne	.L287
	addl	$1, -56(%rsp)
	movl	-56(%rsp), %eax
	cmpl	$8, %eax
	jne	.L295
	addl	-24(%rsp), %r15d
	xorl	%r9d, %r9d
	movl	$1, -48(%rsp)
	movl	$1, %r11d
	movslq	%r15d, %rax
	movl	%r15d, -16(%rsp)
	leaq	0(,%rax,4), %r15
.L304:
	movl	-48(%rsp), %ebx
	movl	%ebx, %eax
	andl	$1, %eax
	sall	$4, %eax
	movl	%eax, %edi
	movl	%eax, -64(%rsp)
	movl	%ebx, %eax
	sall	$3, %eax
	andl	$16, %eax
	movl	%eax, %ecx
	movl	%eax, -32(%rsp)
	movl	%ebx, %eax
	sarl	$2, %eax
	movl	%eax, %r10d
	sall	$9, %eax
	addl	%ecx, %eax
	sall	$4, %r10d
	movslq	%edi, %rcx
	leal	16(%rax), %edx
	movslq	%r10d, %r10
	sall	$5, %edx
	salq	$12, %r10
	addq	-40(%rsp), %r10
	movl	%edx, -8(%rsp)
	movl	%eax, %edx
	addl	$528, %eax
	sall	$5, %eax
	sall	$5, %edx
	movl	%eax, -24(%rsp)
	movl	-8(%rsp), %eax
	leal	16(%rcx), %ebx
	movslq	%edx, %rdx
	leaq	(%rdx,%rcx), %rdi
	movq	%rdi, -56(%rsp)
.L296:
	movl	-32(%rsp), %r12d
	leal	-512(%rax), %edi
	movq	-56(%rsp), %rbp
	.p2align 4,,10
	.p2align 3
.L303:
	movslq	%r12d, %r8
	movl	-64(%rsp), %eax
	movq	%rbp, %rcx
	salq	$5, %r8
	jmp	.L300
	.p2align 4,,10
	.p2align 3
.L316:
	addl	$1, %eax
	notq	%rsi
	addq	$1, %rcx
	andq	%rsi, 0(%r13,%rdx,8)
	cmpl	%ebx, %eax
	je	.L320
.L300:
	movslq	%eax, %rdx
	leal	(%rax,%rdi), %esi
	addq	%r8, %rdx
	vmovss	(%r10,%rdx,4), %xmm1
	movq	%rcx, %rdx
	andl	$63, %esi
	shrq	$6, %rdx
	shlx	%rsi, %r11, %rsi
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	jbe	.L316
	orq	%rsi, 0(%r13,%rdx,8)
	movslq	%r9d, %rdx
	addl	$1, %eax
	addl	$1, %r9d
	addq	$1, %rcx
	leaq	(%r15,%rdx,4), %rdx
	vmovss	%xmm1, (%r14,%rdx)
	cmpl	%ebx, %eax
	jne	.L300
.L320:
	addq	$32, %rbp
	addl	$1, %r12d
	addl	$32, %edi
	cmpl	-8(%rsp), %edi
	jne	.L303
	leal	1024(%rdi), %eax
	addq	$4096, %r10
	addq	$1024, -56(%rsp)
	movl	%eax, -8(%rsp)
	cmpl	-24(%rsp), %eax
	jne	.L296
	addl	$1, -48(%rsp)
	movl	-48(%rsp), %eax
	cmpl	$8, %eax
	jne	.L304
	movl	-16(%rsp), %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	addl	%r9d, %eax
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
	.p2align 4,,10
	.p2align 3
.L318:
	.cfi_restore_state
	shlx	%rdi, %r12, %rdi
	orq	%rdi, 0(%r13,%rax,8)
	movslq	%r15d, %rax
	addl	$1, %r15d
	vmovss	%xmm1, 256(%r14,%rax,4)
	jmp	.L274
.L278:
	orq	%rbp, 0(%r13,%rdi,8)
	movslq	%r15d, %rdi
	addl	$1, %r15d
	vmovss	%xmm1, 256(%r14,%rdi,4)
	jmp	.L279
.L276:
	orq	%rdi, 0(%r13,%rbp,8)
	movslq	%r15d, %rdi
	addl	$1, %r15d
	vmovss	%xmm1, 256(%r14,%rdi,4)
	jmp	.L277
.L280:
	orq	%rbp, 0(%r13,%rdi,8)
	movslq	%r15d, %rdi
	addl	$1, %r15d
	vmovss	%xmm1, 256(%r14,%rdi,4)
	jmp	.L281
	.cfi_endproc
.LFE1442:
	.size	_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf, .-_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf
	.section	.text.unlikely._ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,comdat
.LCOLDE16:
	.section	.text._ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf,comdat
.LHOTE16:
	.section	.text.unlikely._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,comdat
	.align 2
.LCOLDB17:
	.section	.text._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,comdat
.LHOTB17:
	.align 2
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf
	.type	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf, @function
_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf:
.LFB1475:
	.cfi_startproc
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	xorl	%eax, %eax
	movq	%rsi, %r15
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	movq	%rdx, %r13
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	movq	%rcx, -56(%rsp)
.L322:
	movq	%rax, %rdx
	shrq	$6, %rdx
	orq	$1, (%r15,%rdx,8)
	leaq	1(%rax), %rdx
	shrq	$6, %rdx
	orq	$2, (%r15,%rdx,8)
	leaq	2(%rax), %rdx
	shrq	$6, %rdx
	orq	$4, (%r15,%rdx,8)
	leaq	3(%rax), %rdx
	shrq	$6, %rdx
	orq	$8, (%r15,%rdx,8)
	leaq	256(%rax), %rdx
	shrq	$6, %rdx
	orq	$1, (%r15,%rdx,8)
	leaq	257(%rax), %rdx
	shrq	$6, %rdx
	orq	$2, (%r15,%rdx,8)
	leaq	258(%rax), %rdx
	shrq	$6, %rdx
	orq	$4, (%r15,%rdx,8)
	leaq	259(%rax), %rdx
	shrq	$6, %rdx
	orq	$8, (%r15,%rdx,8)
	leaq	512(%rax), %rdx
	shrq	$6, %rdx
	orq	$1, (%r15,%rdx,8)
	leaq	513(%rax), %rdx
	shrq	$6, %rdx
	orq	$2, (%r15,%rdx,8)
	leaq	514(%rax), %rdx
	shrq	$6, %rdx
	orq	$4, (%r15,%rdx,8)
	leaq	515(%rax), %rdx
	shrq	$6, %rdx
	orq	$8, (%r15,%rdx,8)
	leaq	768(%rax), %rdx
	shrq	$6, %rdx
	orq	$1, (%r15,%rdx,8)
	leaq	769(%rax), %rdx
	shrq	$6, %rdx
	orq	$2, (%r15,%rdx,8)
	leaq	770(%rax), %rdx
	shrq	$6, %rdx
	orq	$4, (%r15,%rdx,8)
	leaq	771(%rax), %rdx
	addq	$65536, %rax
	shrq	$6, %rdx
	orq	$8, (%r15,%rdx,8)
	cmpq	$262144, %rax
	jne	.L322
	movq	-56(%rsp), %rdx
	movq	%r13, %rax
	movl	$4, %ecx
.L323:
	vmovss	(%rdx), %xmm1
	addq	$64, %rax
	addq	$262144, %rdx
	vmovss	%xmm1, -64(%rax)
	vmovss	-262140(%rdx), %xmm1
	vmovss	%xmm1, -60(%rax)
	vmovss	-262136(%rdx), %xmm1
	vmovss	%xmm1, -56(%rax)
	vmovss	-262132(%rdx), %xmm1
	vmovss	%xmm1, -52(%rax)
	vmovss	-261120(%rdx), %xmm1
	vmovss	%xmm1, -48(%rax)
	vmovss	-261116(%rdx), %xmm1
	vmovss	%xmm1, -44(%rax)
	vmovss	-261112(%rdx), %xmm1
	vmovss	%xmm1, -40(%rax)
	vmovss	-261108(%rdx), %xmm1
	vmovss	%xmm1, -36(%rax)
	vmovss	-260096(%rdx), %xmm1
	vmovss	%xmm1, -32(%rax)
	vmovss	-260092(%rdx), %xmm1
	vmovss	%xmm1, -28(%rax)
	vmovss	-260088(%rdx), %xmm1
	vmovss	%xmm1, -24(%rax)
	vmovss	-260084(%rdx), %xmm1
	vmovss	%xmm1, -20(%rax)
	vmovss	-259072(%rdx), %xmm1
	vmovss	%xmm1, -16(%rax)
	vmovss	-259068(%rdx), %xmm1
	vmovss	%xmm1, -12(%rax)
	vmovss	-259064(%rdx), %xmm1
	vmovss	%xmm1, -8(%rax)
	vmovss	-259060(%rdx), %xmm1
	vmovss	%xmm1, -4(%rax)
	subl	$1, %ecx
	jne	.L323
	xorl	%ebp, %ebp
	movl	$1, %ebx
	vmovss	.LC15(%rip), %xmm3
	movl	$1, %r11d
.L337:
	leal	(%rbx,%rbx), %eax
	movl	%ebx, %edx
	movl	%ebx, %edi
	andl	$4, %eax
	sarl	$2, %edx
	andl	$1, %edi
	movl	%eax, %ecx
	movl	%eax, -72(%rsp)
	sall	$2, %edi
	leal	0(,%rdx,4), %eax
	sall	$10, %edx
	movslq	%edi, %r8
	addl	%ecx, %edx
	cltq
	sall	$8, %edx
	salq	$18, %rax
	addq	-56(%rsp), %rax
	movslq	%edx, %r9
	leal	262147(%rdi,%rdx), %esi
	addq	%r8, %r9
	leal	3(%rdi), %ecx
	movl	%esi, -64(%rsp)
	leal	1(%rdi), %esi
	leal	(%rcx,%rdx), %r10d
	leal	2(%rdi), %edx
	movslq	%ecx, %rdi
	movslq	%esi, %rcx
	movq	%rcx, -40(%rsp)
	subq	%r8, %rcx
	movq	%rcx, -32(%rsp)
	movslq	%edx, %rcx
	movq	%rdi, -48(%rsp)
	subq	%r8, %rdi
	movq	%rcx, -24(%rsp)
	subq	%r8, %rcx
	movq	%rcx, -16(%rsp)
	movq	%rdi, -8(%rsp)
.L324:
	movl	%r10d, %esi
	movq	%r9, %rcx
	xorl	%edi, %edi
.L336:
	movl	-72(%rsp), %edx
	leal	-3(%rsi), %r14d
	andl	$63, %r14d
	shlx	%r14, %r11, %r14
	addl	%edi, %edx
	movslq	%edx, %rdx
	movq	%rdx, %r12
	salq	$8, %r12
	addq	%r8, %r12
	vmovss	(%rax,%r12,4), %xmm1
	movq	%rcx, %r12
	shrq	$6, %r12
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	ja	.L327
	notq	%r14
	andq	%r14, (%r15,%r12,8)
.L328:
	movq	%rdx, %r12
	movq	-32(%rsp), %r14
	salq	$8, %r12
	addq	-40(%rsp), %r12
	vmovss	(%rax,%r12,4), %xmm1
	leaq	(%r14,%rcx), %r12
	leal	-2(%rsi), %r14d
	shrq	$6, %r12
	vmovaps	%xmm1, %xmm2
	andl	$63, %r14d
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	shlx	%r14, %r11, %r14
	ja	.L329
	notq	%r14
	andq	%r14, (%r15,%r12,8)
.L330:
	movq	%rdx, %r12
	movq	-16(%rsp), %r14
	salq	$8, %r12
	addq	-24(%rsp), %r12
	vmovss	(%rax,%r12,4), %xmm1
	leaq	(%r14,%rcx), %r12
	leal	-1(%rsi), %r14d
	shrq	$6, %r12
	vmovaps	%xmm1, %xmm2
	andl	$63, %r14d
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	shlx	%r14, %r11, %r14
	ja	.L331
	notq	%r14
	andq	%r14, (%r15,%r12,8)
.L332:
	salq	$8, %rdx
	addq	-48(%rsp), %rdx
	movl	%esi, %r12d
	andl	$63, %r12d
	vmovss	(%rax,%rdx,4), %xmm1
	movq	-8(%rsp), %rdx
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	addq	%rcx, %rdx
	shrq	$6, %rdx
	vucomiss	%xmm0, %xmm2
	ja	.L408
	shlx	%r12, %r11, %r12
	notq	%r12
	andq	%r12, (%r15,%rdx,8)
.L325:
	addq	$1, %rdi
	addq	$256, %rcx
	addl	$256, %esi
	cmpq	$4, %rdi
	jne	.L336
	addq	$262144, %rax
	addq	$65536, %r9
	addl	$65536, %r10d
	cmpl	-64(%rsp), %r10d
	jne	.L324
	addl	$1, %ebx
	cmpl	$8, %ebx
	jne	.L337
	leal	64(%rbp), %eax
	xorl	%r14d, %r14d
	movb	$1, %bl
	movl	%eax, -40(%rsp)
	cltq
	salq	$2, %rax
	movq	%rax, -64(%rsp)
.L346:
	leal	0(,%rbx,4), %eax
	movl	%ebx, %edx
	movl	%ebx, %ecx
	andl	$8, %eax
	sarl	$2, %edx
	andl	$1, %ecx
	movl	%eax, %edi
	movl	%eax, -48(%rsp)
	sall	$3, %ecx
	leal	0(,%rdx,8), %eax
	sall	$11, %edx
	movslq	%ecx, %r8
	leal	(%rdx,%rdi), %esi
	cltq
	leal	8(%rsi), %edx
	movl	%esi, %edi
	salq	$18, %rax
	addq	-56(%rsp), %rax
	sall	$8, %edi
	addl	$2056, %esi
	sall	$8, %esi
	movslq	%edi, %rdi
	sall	$8, %edx
	leaq	(%rdi,%r8), %r10
	movl	%esi, -32(%rsp)
	leal	8(%rcx), %edi
	movl	%edi, -24(%rsp)
.L338:
	movl	-48(%rsp), %r11d
	leal	-2048(%rdx), %esi
	movq	%r10, -72(%rsp)
.L345:
	movq	-72(%rsp), %r12
	movslq	%r11d, %rbp
	movl	%ecx, %edi
	movl	%edx, -16(%rsp)
	salq	$8, %rbp
.L342:
	leal	(%rdi,%rsi), %r9d
	movslq	%edi, %r8
	movl	$1, %edx
	addq	%rbp, %r8
	andl	$63, %r9d
	vmovss	(%rax,%r8,4), %xmm1
	movq	%r12, %r8
	shlx	%r9, %rdx, %r9
	shrq	$6, %r8
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	ja	.L409
	notq	%r9
	andq	%r9, (%r15,%r8,8)
.L341:
	addl	$1, %edi
	addq	$1, %r12
	cmpl	-24(%rsp), %edi
	jne	.L342
	movl	-16(%rsp), %edx
	addl	$256, %esi
	addl	$1, %r11d
	addq	$256, -72(%rsp)
	cmpl	%edx, %esi
	jne	.L345
	leal	65536(%rsi), %edx
	addq	$262144, %rax
	addq	$65536, %r10
	cmpl	-32(%rsp), %edx
	jne	.L338
	addl	$1, %ebx
	cmpl	$8, %ebx
	jne	.L346
	addl	-40(%rsp), %r14d
	movl	$0, -72(%rsp)
	movslq	%r14d, %rax
	movl	%r14d, -32(%rsp)
	movl	$1, %r14d
	salq	$2, %rax
	movq	%rax, -48(%rsp)
.L355:
	leal	0(,%r14,8), %eax
	movl	%r14d, %r11d
	andl	$16, %eax
	andl	$1, %r11d
	movl	%eax, %edi
	movl	%eax, -40(%rsp)
	movl	%r14d, %eax
	sall	$4, %r11d
	leal	16(%r11), %ebx
	sarl	$2, %eax
	movslq	%r11d, %rsi
	movl	%eax, %ecx
	sall	$12, %eax
	sall	$4, %ecx
	addl	%edi, %eax
	leal	16(%rax), %r9d
	movslq	%ecx, %rcx
	movl	%eax, %edx
	salq	$18, %rcx
	addq	-56(%rsp), %rcx
	sall	$8, %edx
	addl	$4112, %eax
	movslq	%edx, %rdx
	sall	$8, %eax
	leaq	(%rdx,%rsi), %rbp
	sall	$8, %r9d
	movl	%eax, -24(%rsp)
.L347:
	movl	-40(%rsp), %r12d
	leal	-4096(%r9), %r8d
	movq	%rbp, -64(%rsp)
.L354:
	movslq	%r12d, %r10
	movl	%r11d, %eax
	movq	-64(%rsp), %rsi
	movl	%r9d, -16(%rsp)
	salq	$8, %r10
	jmp	.L351
.L403:
	addl	$1, %eax
	notq	%rdx
	addq	$1, %rsi
	andq	%rdx, (%r15,%rdi,8)
	cmpl	%ebx, %eax
	je	.L410
.L351:
	movslq	%eax, %rdx
	movq	%rsi, %rdi
	addq	%r10, %rdx
	shrq	$6, %rdi
	movl	$1, %r9d
	vmovss	(%rcx,%rdx,4), %xmm1
	leal	(%rax,%r8), %edx
	andl	$63, %edx
	vmovaps	%xmm1, %xmm2
	shlx	%rdx, %r9, %rdx
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	jbe	.L403
	orq	%rdx, (%r15,%rdi,8)
	addl	$1, %eax
	addq	$1, %rsi
	movslq	-72(%rsp), %rdx
	movq	-48(%rsp), %r9
	movq	%rdx, %rdi
	leaq	(%r9,%rdx,4), %rdx
	addl	$1, %edi
	vmovss	%xmm1, 0(%r13,%rdx)
	movl	%edi, -72(%rsp)
	cmpl	%ebx, %eax
	jne	.L351
.L410:
	movl	-16(%rsp), %r9d
	addl	$256, %r8d
	addl	$1, %r12d
	addq	$256, -64(%rsp)
	cmpl	%r9d, %r8d
	jne	.L354
	leal	65536(%r8), %r9d
	addq	$262144, %rcx
	addq	$65536, %rbp
	cmpl	-24(%rsp), %r9d
	jne	.L347
	addl	$1, %r14d
	cmpl	$8, %r14d
	jne	.L355
	movl	-32(%rsp), %eax
	xorb	%r14b, %r14b
	movl	$1, %r12d
	addl	-72(%rsp), %eax
	movl	$1, -72(%rsp)
	movl	%eax, -24(%rsp)
	cltq
	leaq	0(,%rax,4), %rbx
.L364:
	movl	-72(%rsp), %edi
	movl	%edi, %eax
	andl	$1, %eax
	sall	$5, %eax
	movl	%eax, %r11d
	movl	%eax, -40(%rsp)
	movl	%edi, %eax
	leal	32(%r11), %r10d
	sall	$4, %eax
	movslq	%r11d, %rcx
	andl	$32, %eax
	movl	%eax, %esi
	movl	%eax, -32(%rsp)
	movl	%edi, %eax
	sarl	$2, %eax
	movl	%eax, %edx
	sall	$13, %eax
	sall	$5, %edx
	addl	%esi, %eax
	movslq	%edx, %rdi
	sall	$8, %eax
	salq	$18, %rdi
	addq	-56(%rsp), %rdi
	movslq	%eax, %rsi
	movl	%eax, -48(%rsp)
	leaq	2105344(%rsi,%rcx), %rax
	leaq	8192(%rsi,%rcx), %rbp
	movq	%rax, -16(%rsp)
.L356:
	movl	-32(%rsp), %eax
	leaq	-8192(%rbp), %r11
	movl	-48(%rsp), %r8d
	movl	%eax, -64(%rsp)
.L363:
	movslq	-64(%rsp), %r9
	movq	%r11, %rcx
	movl	-40(%rsp), %eax
	salq	$8, %r9
	jmp	.L360
	.p2align 4,,10
	.p2align 3
.L404:
	addl	$1, %eax
	notq	%rsi
	addq	$1, %rcx
	andq	%rsi, (%r15,%rdx,8)
	cmpl	%r10d, %eax
	je	.L411
.L360:
	movslq	%eax, %rdx
	leal	(%rax,%r8), %esi
	addq	%r9, %rdx
	vmovss	(%rdi,%rdx,4), %xmm1
	movq	%rcx, %rdx
	andl	$63, %esi
	shrq	$6, %rdx
	shlx	%rsi, %r12, %rsi
	vmovaps	%xmm1, %xmm2
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	jbe	.L404
	orq	%rsi, (%r15,%rdx,8)
	movslq	%r14d, %rdx
	addl	$1, %eax
	addl	$1, %r14d
	addq	$1, %rcx
	leaq	(%rbx,%rdx,4), %rdx
	vmovss	%xmm1, 0(%r13,%rdx)
	cmpl	%r10d, %eax
	jne	.L360
.L411:
	addq	$256, %r11
	addl	$256, %r8d
	addl	$1, -64(%rsp)
	cmpq	%rbp, %r11
	jne	.L363
	leaq	65536(%r11), %rbp
	addq	$262144, %rdi
	addl	$65536, -48(%rsp)
	cmpq	-16(%rsp), %rbp
	jne	.L356
	addl	$1, -72(%rsp)
	movl	-72(%rsp), %eax
	cmpl	$8, %eax
	jne	.L364
	addl	-24(%rsp), %r14d
	xorl	%edi, %edi
	movl	$1, %ebx
	movl	$1, -48(%rsp)
	movslq	%r14d, %rax
	movl	%r14d, -24(%rsp)
	leaq	0(,%rax,4), %r14
.L373:
	movl	-48(%rsp), %ecx
	movl	%ecx, %eax
	andl	$1, %eax
	sall	$6, %eax
	movl	%eax, %r11d
	movl	%eax, -64(%rsp)
	movl	%ecx, %eax
	sall	$5, %eax
	movslq	%r11d, %rdx
	leal	64(%r11), %r11d
	andl	$64, %eax
	movl	%eax, %r10d
	movl	%eax, -32(%rsp)
	movl	%ecx, %eax
	sarl	$2, %eax
	movl	%eax, %ecx
	sall	$14, %eax
	sall	$6, %ecx
	addl	%r10d, %eax
	movslq	%ecx, %r9
	sall	$8, %eax
	salq	$18, %r9
	addq	-56(%rsp), %r9
	movslq	%eax, %rsi
	movl	%eax, -40(%rsp)
	leaq	16384(%rdx,%rsi), %r10
	leaq	4210688(%rdx,%rsi), %rax
	movq	%r10, -72(%rsp)
	movq	%rax, -16(%rsp)
	movq	%r10, %rax
.L365:
	movl	-32(%rsp), %r12d
	leaq	-16384(%rax), %rbp
	movl	-40(%rsp), %r10d
.L372:
	movslq	%r12d, %r8
	movl	-64(%rsp), %eax
	movq	%rbp, %rcx
	salq	$8, %r8
	jmp	.L369
	.p2align 4,,10
	.p2align 3
.L405:
	addl	$1, %eax
	notq	%rdx
	addq	$1, %rcx
	andq	%rdx, (%r15,%rsi,8)
	cmpl	%r11d, %eax
	je	.L412
.L369:
	movslq	%eax, %rdx
	movq	%rcx, %rsi
	addq	%r8, %rdx
	shrq	$6, %rsi
	vmovss	(%r9,%rdx,4), %xmm1
	leal	(%rax,%r10), %edx
	andl	$63, %edx
	vmovaps	%xmm1, %xmm2
	shlx	%rdx, %rbx, %rdx
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	jbe	.L405
	orq	%rdx, (%r15,%rsi,8)
	movslq	%edi, %rdx
	addl	$1, %eax
	addl	$1, %edi
	addq	$1, %rcx
	leaq	(%r14,%rdx,4), %rdx
	vmovss	%xmm1, 0(%r13,%rdx)
	cmpl	%r11d, %eax
	jne	.L369
.L412:
	addq	$256, %rbp
	addl	$1, %r12d
	addl	$256, %r10d
	cmpq	-72(%rsp), %rbp
	jne	.L372
	leaq	65536(%rbp), %rax
	addq	$262144, %r9
	addl	$65536, -40(%rsp)
	movq	%rax, -72(%rsp)
	cmpq	-16(%rsp), %rax
	jne	.L365
	addl	$1, -48(%rsp)
	movl	-48(%rsp), %eax
	cmpl	$8, %eax
	jne	.L373
	addl	-24(%rsp), %edi
	movl	$1, -48(%rsp)
	movl	$1, %ebx
	movslq	%edi, %rbp
	movl	%edi, -24(%rsp)
	xorl	%edi, %edi
	salq	$2, %rbp
.L382:
	movl	-48(%rsp), %ecx
	movl	%ecx, %eax
	andl	$1, %eax
	sall	$7, %eax
	movl	%eax, %r14d
	movl	%eax, -64(%rsp)
	movl	%ecx, %eax
	sall	$6, %eax
	movslq	%r14d, %rdx
	andl	$128, %eax
	movl	%eax, %r11d
	movl	%eax, -32(%rsp)
	movl	%ecx, %eax
	sarl	$2, %eax
	movl	%eax, %ecx
	sall	$15, %eax
	addl	%r11d, %eax
	sall	$7, %ecx
	sall	$8, %eax
	movslq	%ecx, %r10
	movslq	%eax, %rsi
	movl	%eax, -40(%rsp)
	salq	$18, %r10
	addq	-56(%rsp), %r10
	leaq	32768(%rsi,%rdx), %r11
	leaq	8421376(%rsi,%rdx), %rax
	movq	%r11, -72(%rsp)
	movq	%rax, -16(%rsp)
	movq	-72(%rsp), %rax
	leal	128(%r14), %r11d
.L374:
	movl	-32(%rsp), %r14d
	leaq	-32768(%rax), %r12
	movl	-40(%rsp), %r9d
	.p2align 4,,10
	.p2align 3
.L381:
	movslq	%r14d, %r8
	movl	-64(%rsp), %eax
	movq	%r12, %rcx
	salq	$8, %r8
	jmp	.L378
	.p2align 4,,10
	.p2align 3
.L406:
	addl	$1, %eax
	notq	%rdx
	addq	$1, %rcx
	andq	%rdx, (%r15,%rsi,8)
	cmpl	%r11d, %eax
	je	.L413
.L378:
	movslq	%eax, %rdx
	movq	%rcx, %rsi
	addq	%r8, %rdx
	shrq	$6, %rsi
	vmovss	(%r10,%rdx,4), %xmm1
	leal	(%rax,%r9), %edx
	andl	$63, %edx
	vmovaps	%xmm1, %xmm2
	shlx	%rdx, %rbx, %rdx
	vandps	%xmm3, %xmm2, %xmm2
	vucomiss	%xmm0, %xmm2
	jbe	.L406
	orq	%rdx, (%r15,%rsi,8)
	movslq	%edi, %rdx
	addl	$1, %eax
	addl	$1, %edi
	addq	$1, %rcx
	leaq	0(%rbp,%rdx,4), %rdx
	vmovss	%xmm1, 0(%r13,%rdx)
	cmpl	%r11d, %eax
	jne	.L378
.L413:
	addq	$256, %r12
	addl	$1, %r14d
	addl	$256, %r9d
	cmpq	-72(%rsp), %r12
	jne	.L381
	leaq	65536(%r12), %rax
	addq	$262144, %r10
	addl	$65536, -40(%rsp)
	movq	%rax, -72(%rsp)
	cmpq	-16(%rsp), %rax
	jne	.L374
	addl	$1, -48(%rsp)
	movl	-48(%rsp), %eax
	cmpl	$8, %eax
	jne	.L382
	movl	-24(%rsp), %eax
	popq	%rbx
	.cfi_remember_state
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	addl	%edi, %eax
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
.L327:
	.cfi_restore_state
	orq	%r14, (%r15,%r12,8)
	movslq	%ebp, %r12
	addl	$1, %ebp
	vmovss	%xmm1, 256(%r13,%r12,4)
	jmp	.L328
.L408:
	shlx	%r12, %r11, %r12
	orq	%r12, (%r15,%rdx,8)
	movslq	%ebp, %rdx
	addl	$1, %ebp
	vmovss	%xmm1, 256(%r13,%rdx,4)
	jmp	.L325
.L331:
	orq	%r14, (%r15,%r12,8)
	movslq	%ebp, %r12
	addl	$1, %ebp
	vmovss	%xmm1, 256(%r13,%r12,4)
	jmp	.L332
.L329:
	orq	%r14, (%r15,%r12,8)
	movslq	%ebp, %r12
	addl	$1, %ebp
	vmovss	%xmm1, 256(%r13,%r12,4)
	jmp	.L330
	.p2align 4,,10
	.p2align 3
.L409:
	movq	-64(%rsp), %rdx
	orq	%r9, (%r15,%r8,8)
	movslq	%r14d, %r8
	addl	$1, %r14d
	leaq	(%rdx,%r8,4), %r8
	vmovss	%xmm1, 0(%r13,%r8)
	jmp	.L341
	.cfi_endproc
.LFE1475:
	.size	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf, .-_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf
	.section	.text.unlikely._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,comdat
.LCOLDE17:
	.section	.text._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf,comdat
.LHOTE17:
	.section	.text.unlikely._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,comdat
	.align 2
.LCOLDB19:
	.section	.text._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,comdat
.LHOTB19:
	.align 2
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f
	.type	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f, @function
_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f:
.LFB1482:
	.cfi_startproc
	pushq	%r15
	.cfi_def_cfa_offset 16
	.cfi_offset 15, -16
	movl	$1, %r11d
	pushq	%r14
	.cfi_def_cfa_offset 24
	.cfi_offset 14, -24
	pushq	%r13
	.cfi_def_cfa_offset 32
	.cfi_offset 13, -32
	pushq	%r12
	.cfi_def_cfa_offset 40
	.cfi_offset 12, -40
	pushq	%rbp
	.cfi_def_cfa_offset 48
	.cfi_offset 6, -48
	movq	%rsi, %rbp
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset 3, -56
	subq	$599288, %rsp
	.cfi_def_cfa_offset 599344
	movq	%rdx, (%rsp)
	movl	$7, 8(%rsp)
.L422:
	movl	8(%rsp), %ebx
	movl	$1, %r14d
	movl	%ebx, %esi
	movl	%ebx, %eax
	movl	%ebx, %ecx
	sarl	$2, %esi
	sall	$6, %eax
	andl	$1, %ecx
	movl	%esi, %edx
	andl	$128, %eax
	sall	$7, %ecx
	sall	$7, %edx
	movl	%eax, %edi
	subl	%ecx, %r14d
	leal	127(%rdx), %eax
	movslq	%edx, %r9
	leal	127(%rdi), %ebx
	sall	$8, %eax
	addq	$127, %r9
	leal	(%rax,%rdi), %r8d
	salq	$18, %r9
	addq	(%rsp), %r9
	movl	%ebx, 32(%rsp)
	sall	$8, %r8d
	leal	(%rax,%rbx), %edx
	leal	-257(%rcx,%r8), %r13d
	sall	$8, %edx
	movslq	%ecx, %r8
	movslq	%edx, %rdx
	leaq	127(%r8,%rdx), %rax
	movq	%rax, 16(%rsp)
	movl	%esi, %eax
	sall	$15, %eax
	addl	%edi, %eax
	sall	$8, %eax
	leal	-65793(%rcx,%rax), %eax
	movl	%eax, 24(%rsp)
.L415:
	movl	32(%rsp), %r12d
	leal	32768(%r13), %r8d
	movq	16(%rsp), %rbx
	.p2align 4,,10
	.p2align 3
.L421:
	leal	128(%r8), %ecx
	movslq	%r12d, %rsi
	movq	%rbx, %rdx
	leal	(%r14,%r8), %edi
	salq	$8, %rsi
	jmp	.L418
	.p2align 4,,10
	.p2align 3
.L537:
	movq	8(%rbp), %r15
	cltq
	subl	$1, %ecx
	subq	$1, %rdx
	leaq	(%rsi,%rax), %r10
	vmovss	-4(%r15), %xmm0
	leaq	-4(%r15), %rax
	vmovss	%xmm0, (%r9,%r10,4)
	movq	%rax, 8(%rbp)
	cmpl	%r8d, %ecx
	je	.L536
.L418:
	movq	%rdx, %r10
	movl	%ecx, %eax
	movq	%r11, %r15
	shrq	$6, %r10
	subl	%edi, %eax
	salq	%cl, %r15
	testq	%r15, 599344(%rsp,%r10,8)
	jne	.L537
	cltq
	subl	$1, %ecx
	addq	%rsi, %rax
	subq	$1, %rdx
	movl	$0x00000000, (%r9,%rax,4)
	cmpl	%r8d, %ecx
	jne	.L418
.L536:
	leal	-256(%rcx), %r8d
	subl	$1, %r12d
	subq	$256, %rbx
	cmpl	%r13d, %r8d
	jne	.L421
	leal	-65792(%rcx), %r13d
	subq	$262144, %r9
	subq	$65536, 16(%rsp)
	cmpl	24(%rsp), %r13d
	jne	.L415
	subl	$1, 8(%rsp)
	jne	.L422
	xorl	%esi, %esi
	movl	$262144, %edx
	xorl	%ebx, %ebx
	leaq	74992(%rsp), %rdi
	call	memset
	xorl	%r11d, %r11d
	movl	$1, %esi
.L423:
	movl	%r11d, %edi
	movl	%ebx, %r10d
	movq	%rbx, %r9
	leal	32768(%r11), %r12d
	movq	%r11, %r8
.L429:
	xorl	%eax, %eax
	jmp	.L426
	.p2align 4,,10
	.p2align 3
.L424:
	andn	74992(%rsp,%rdx,8), %rcx, %rcx
.L425:
	addq	$1, %rax
	movq	%rcx, 74992(%rsp,%rdx,8)
	cmpq	$128, %rax
	je	.L538
.L426:
	leal	(%rdi,%rax), %ecx
	movq	%rsi, %r14
	leaq	(%r9,%rax), %rdx
	salq	%cl, %r14
	leaq	(%r8,%rax), %rcx
	shrq	$6, %rdx
	leal	(%r10,%rax), %r13d
	shrq	$6, %rcx
	andl	$63, %r13d
	testq	%r14, 599344(%rsp,%rcx,8)
	shlx	%r13, %rsi, %rcx
	je	.L424
	orq	74992(%rsp,%rdx,8), %rcx
	jmp	.L425
.L538:
	addl	$256, %edi
	subl	$-128, %r10d
	addq	$256, %r8
	subq	$-128, %r9
	cmpl	%r12d, %edi
	jne	.L429
	addq	$65536, %r11
	addq	$16384, %rbx
	cmpq	$8388608, %r11
	jne	.L423
	leaq	74992(%rsp), %rsi
	movl	$262144, %edx
	leaq	337136(%rsp), %rdi
	call	memcpy
	movl	$7, 8(%rsp)
	movl	$1, %r11d
.L437:
	movl	8(%rsp), %ebx
	movl	$1, %r14d
	movl	%ebx, %esi
	movl	%ebx, %eax
	movl	%ebx, %ecx
	sarl	$2, %esi
	sall	$5, %eax
	andl	$1, %ecx
	movl	%esi, %edx
	andl	$64, %eax
	sall	$6, %ecx
	sall	$6, %edx
	movl	%eax, %edi
	subl	%ecx, %r14d
	leal	63(%rdx), %eax
	movslq	%edx, %r9
	leal	63(%rdi), %ebx
	sall	$7, %eax
	addq	$63, %r9
	leal	(%rax,%rdi), %r8d
	salq	$18, %r9
	addq	(%rsp), %r9
	movl	%ebx, 32(%rsp)
	sall	$7, %r8d
	leal	(%rax,%rbx), %edx
	leal	-129(%rcx,%r8), %r13d
	sall	$7, %edx
	movslq	%ecx, %r8
	movslq	%edx, %rdx
	leaq	63(%r8,%rdx), %rax
	movq	%rax, 16(%rsp)
	movl	%esi, %eax
	sall	$13, %eax
	addl	%edi, %eax
	sall	$7, %eax
	leal	-16513(%rcx,%rax), %eax
	movl	%eax, 24(%rsp)
.L430:
	movl	32(%rsp), %r12d
	leal	8192(%r13), %r8d
	movq	16(%rsp), %rbx
.L436:
	leal	64(%r8), %ecx
	movslq	%r12d, %rsi
	movq	%rbx, %rdx
	leal	(%r14,%r8), %edi
	salq	$8, %rsi
	jmp	.L433
	.p2align 4,,10
	.p2align 3
.L540:
	movq	8(%rbp), %r10
	cltq
	subl	$1, %ecx
	subq	$1, %rdx
	addq	%rsi, %rax
	vmovss	-4(%r10), %xmm0
	vmovss	%xmm0, (%r9,%rax,4)
	subq	$4, 8(%rbp)
	cmpl	%r8d, %ecx
	je	.L539
.L433:
	movq	%rdx, %r10
	movl	%ecx, %eax
	movq	%r11, %r15
	shrq	$6, %r10
	subl	%edi, %eax
	salq	%cl, %r15
	testq	%r15, 337136(%rsp,%r10,8)
	jne	.L540
	cltq
	subl	$1, %ecx
	addq	%rsi, %rax
	subq	$1, %rdx
	movl	$0x00000000, (%r9,%rax,4)
	cmpl	%r8d, %ecx
	jne	.L433
.L539:
	leal	-128(%rcx), %r8d
	subl	$1, %r12d
	addq	$-128, %rbx
	cmpl	%r13d, %r8d
	jne	.L436
	leal	-16512(%rcx), %r13d
	subq	$262144, %r9
	subq	$16384, 16(%rsp)
	cmpl	24(%rsp), %r13d
	jne	.L430
	subl	$1, 8(%rsp)
	jne	.L437
	leaq	9456(%rsp), %rdi
	movl	$32768, %edx
	xorl	%esi, %esi
	call	memset
	xorl	%r14d, %r14d
	xorl	%ebx, %ebx
	movl	$1, %edi
.L438:
	movl	%ebx, %r8d
	movl	%r14d, %r11d
	movq	%r14, %r10
	leal	8192(%rbx), %r12d
	movq	%rbx, %r9
.L444:
	xorl	%eax, %eax
	jmp	.L441
	.p2align 4,,10
	.p2align 3
.L439:
	andn	9456(%rsp,%rdx,8), %rcx, %rcx
.L440:
	addq	$1, %rax
	movq	%rcx, 9456(%rsp,%rdx,8)
	cmpq	$64, %rax
	je	.L541
.L441:
	leal	(%r8,%rax), %ecx
	movq	%rdi, %r13
	leaq	(%r10,%rax), %rdx
	salq	%cl, %r13
	leaq	(%r9,%rax), %rcx
	shrq	$6, %rdx
	leal	(%r11,%rax), %esi
	shrq	$6, %rcx
	andl	$63, %esi
	testq	%r13, 337136(%rsp,%rcx,8)
	shlx	%rsi, %rdi, %rcx
	je	.L439
	orq	9456(%rsp,%rdx,8), %rcx
	jmp	.L440
.L541:
	subl	$-128, %r8d
	addl	$64, %r11d
	subq	$-128, %r9
	addq	$64, %r10
	cmpl	%r8d, %r12d
	jne	.L444
	addq	$16384, %rbx
	addq	$4096, %r14
	cmpq	$1048576, %rbx
	jne	.L438
	movl	$32768, %edx
	movl	$7, %r12d
	movl	$1, %r15d
	leaq	9456(%rsp), %rsi
	leaq	42224(%rsp), %rdi
	call	memcpy
.L452:
	movl	%r12d, %esi
	movl	%r12d, %eax
	movl	%r12d, %ecx
	sarl	$2, %esi
	sall	$4, %eax
	andl	$1, %ecx
	movl	%esi, %edx
	andl	$32, %eax
	sall	$5, %ecx
	sall	$5, %edx
	movl	%eax, %r8d
	movslq	%ecx, %r9
	leal	31(%rdx), %eax
	leal	31(%r8), %ebx
	sall	$6, %eax
	leal	(%rax,%r8), %edi
	movl	%ebx, 24(%rsp)
	sall	$6, %edi
	leal	-65(%rcx,%rdi), %r11d
	movslq	%edx, %rdi
	leal	(%rax,%rbx), %edx
	movl	%esi, %eax
	addq	$31, %rdi
	sall	$11, %eax
	salq	$18, %rdi
	addq	(%rsp), %rdi
	addl	%r8d, %eax
	sall	$6, %edx
	sall	$6, %eax
	movslq	%edx, %rdx
	leal	-4161(%rcx,%rax), %eax
	movl	%eax, 16(%rsp)
	leaq	31(%r9,%rdx), %rbx
	movl	$1, %eax
	subl	%ecx, %eax
	movl	%eax, 32(%rsp)
.L445:
	movl	24(%rsp), %r13d
	leal	2048(%r11), %r8d
	movq	%rbx, 8(%rsp)
.L451:
	movl	32(%rsp), %eax
	leal	32(%r8), %ecx
	movslq	%r13d, %r9
	movq	8(%rsp), %rdx
	salq	$8, %r9
	leal	(%rax,%r8), %r10d
	jmp	.L448
	.p2align 4,,10
	.p2align 3
.L543:
	movq	8(%rbp), %rsi
	cltq
	subl	$1, %ecx
	subq	$1, %rdx
	addq	%r9, %rax
	vmovss	-4(%rsi), %xmm0
	vmovss	%xmm0, (%rdi,%rax,4)
	subq	$4, 8(%rbp)
	cmpl	%r8d, %ecx
	je	.L542
.L448:
	movq	%rdx, %rsi
	movl	%ecx, %eax
	movq	%r15, %r14
	shrq	$6, %rsi
	subl	%r10d, %eax
	salq	%cl, %r14
	testq	%r14, 42224(%rsp,%rsi,8)
	jne	.L543
	cltq
	subl	$1, %ecx
	addq	%r9, %rax
	subq	$1, %rdx
	movl	$0x00000000, (%rdi,%rax,4)
	cmpl	%r8d, %ecx
	jne	.L448
.L542:
	subq	$64, 8(%rsp)
	leal	-64(%rcx), %r8d
	subl	$1, %r13d
	cmpl	%r11d, %r8d
	jne	.L451
	leal	-4160(%rcx), %r11d
	subq	$262144, %rdi
	subq	$4096, %rbx
	cmpl	16(%rsp), %r11d
	jne	.L445
	subl	$1, %r12d
	jne	.L452
	leaq	1264(%rsp), %rdi
	movl	$4096, %edx
	xorl	%esi, %esi
	call	memset
	xorl	%r14d, %r14d
	xorl	%r9d, %r9d
	movl	$1, %edi
.L453:
	movl	%r9d, %r8d
	movl	%r14d, %ebx
	movq	%r14, %r11
	leal	2048(%r9), %r12d
	movq	%r9, %r10
.L459:
	xorl	%eax, %eax
	jmp	.L456
.L454:
	andn	1264(%rsp,%rdx,8), %rcx, %rcx
.L455:
	addq	$1, %rax
	movq	%rcx, 1264(%rsp,%rdx,8)
	cmpq	$32, %rax
	je	.L544
.L456:
	leal	(%r8,%rax), %ecx
	movq	%rdi, %r13
	leaq	(%r11,%rax), %rdx
	salq	%cl, %r13
	leaq	(%r10,%rax), %rcx
	shrq	$6, %rdx
	leal	(%rbx,%rax), %esi
	shrq	$6, %rcx
	andl	$63, %esi
	testq	%r13, 42224(%rsp,%rcx,8)
	shlx	%rsi, %rdi, %rcx
	je	.L454
	orq	1264(%rsp,%rdx,8), %rcx
	jmp	.L455
.L544:
	addl	$64, %r8d
	addl	$32, %ebx
	addq	$64, %r10
	addq	$32, %r11
	cmpl	%r8d, %r12d
	jne	.L459
	addq	$4096, %r9
	addq	$1024, %r14
	cmpq	$131072, %r9
	jne	.L453
	movl	$4096, %edx
	movl	$7, %r12d
	movl	$1, %r15d
	leaq	1264(%rsp), %rsi
	leaq	5360(%rsp), %rdi
	call	memcpy
.L467:
	leal	0(,%r12,8), %esi
	movl	%r12d, %edi
	movl	%r12d, %ecx
	sarl	$2, %edi
	andl	$16, %esi
	andl	$1, %ecx
	leal	15(%rsi), %ebx
	movl	%edi, %edx
	sall	$4, %ecx
	sall	$4, %edx
	movslq	%ecx, %r8
	sall	$9, %edi
	movl	%ebx, 16(%rsp)
	leal	15(%rdx), %eax
	movslq	%edx, %rdx
	sall	$5, %eax
	addq	$15, %rdx
	leal	134217727(%rsi,%rax), %r9d
	addl	%ebx, %eax
	salq	$18, %rdx
	addq	(%rsp), %rdx
	sall	$5, %eax
	cltq
	sall	$5, %r9d
	leaq	15(%r8,%rax), %rbx
	leal	134217695(%rsi,%rdi), %eax
	sall	$5, %eax
	movl	%eax, 24(%rsp)
	leal	15(%rcx), %eax
	movl	%eax, 40(%rsp)
	leal	-1(%rcx), %eax
	movl	%eax, 32(%rsp)
.L460:
	movl	16(%rsp), %r13d
	leal	512(%r9), %edi
	movq	%rbx, 8(%rsp)
.L466:
	movl	40(%rsp), %eax
	movslq	%r13d, %r10
	movq	8(%rsp), %rsi
	salq	$8, %r10
	leal	(%rax,%rdi), %ecx
	movl	32(%rsp), %eax
	leal	(%rax,%rdi), %r11d
	jmp	.L463
.L546:
	movq	8(%rbp), %r8
	cltq
	subl	$1, %ecx
	subq	$1, %rsi
	addq	%r10, %rax
	vmovss	-4(%r8), %xmm0
	vmovss	%xmm0, (%rdx,%rax,4)
	subq	$4, 8(%rbp)
	cmpl	%ecx, %r11d
	je	.L545
.L463:
	movq	%rsi, %r8
	movl	%ecx, %eax
	movq	%r15, %r14
	shrq	$6, %r8
	subl	%edi, %eax
	salq	%cl, %r14
	testq	%r14, 5360(%rsp,%r8,8)
	jne	.L546
	cltq
	subl	$1, %ecx
	addq	%r10, %rax
	subq	$1, %rsi
	movl	$0x00000000, (%rdx,%rax,4)
	cmpl	%ecx, %r11d
	jne	.L463
.L545:
	subq	$32, 8(%rsp)
	subl	$32, %edi
	subl	$1, %r13d
	cmpl	%r9d, %edi
	jne	.L466
	leal	-1024(%rdi), %r9d
	subq	$262144, %rdx
	subq	$1024, %rbx
	cmpl	24(%rsp), %r9d
	jne	.L460
	subl	$1, %r12d
	jne	.L467
	leaq	240(%rsp), %rsi
	movl	$64, %ecx
	xorl	%eax, %eax
	movq	%rsi, %rdi
	xorl	%r9d, %r9d
	xorl	%edx, %edx
	rep; stosq
	movl	$1, %edi
.L468:
	movl	%edx, %r10d
	movl	%r9d, %r12d
	movq	%r9, %rbx
	leal	512(%rdx), %r13d
	movq	%rdx, %r11
.L474:
	xorl	%eax, %eax
	jmp	.L471
	.p2align 4,,10
	.p2align 3
.L469:
	andn	240(%rsp,%r8,8), %rcx, %rcx
.L470:
	addq	$1, %rax
	movq	%rcx, 240(%rsp,%r8,8)
	cmpq	$16, %rax
	je	.L547
.L471:
	leal	(%r10,%rax), %ecx
	movq	%rdi, %r15
	leaq	(%rax,%rbx), %r8
	salq	%cl, %r15
	leaq	(%rax,%r11), %rcx
	shrq	$6, %r8
	leal	(%r12,%rax), %r14d
	shrq	$6, %rcx
	andl	$63, %r14d
	testq	%r15, 5360(%rsp,%rcx,8)
	shlx	%r14, %rdi, %rcx
	je	.L469
	orq	240(%rsp,%r8,8), %rcx
	jmp	.L470
.L547:
	addl	$32, %r10d
	addl	$16, %r12d
	addq	$32, %r11
	addq	$16, %rbx
	cmpl	%r10d, %r13d
	jne	.L474
	addq	$1024, %rdx
	addq	$256, %r9
	cmpq	$16384, %rdx
	jne	.L468
	movl	$512, %edx
	movl	$1, %r12d
	leaq	752(%rsp), %rdi
	call	memcpy
	movl	$7, 16(%rsp)
.L496:
	movl	16(%rsp), %eax
	leal	0(,%rax,4), %esi
	movl	%eax, %ecx
	movl	%eax, %r8d
	sarl	$2, %ecx
	andl	$8, %esi
	andl	$1, %r8d
	leal	0(,%rcx,8), %eax
	sall	$3, %r8d
	leal	7(%rsi), %ebx
	movslq	%eax, %rdx
	addl	$7, %eax
	sall	$4, %eax
	movslq	%r8d, %rdi
	movl	%ebx, 48(%rsp)
	addq	$7, %rdx
	leal	7(%r8), %r10d
	addl	%ebx, %eax
	salq	$18, %rdx
	addq	(%rsp), %rdx
	sall	$4, %eax
	leal	(%rax,%r10), %r14d
	cltq
	movslq	%r10d, %r10
	leal	6(%r8), %ebx
	addq	%rdi, %rax
	movq	%rax, 8(%rsp)
	leal	5(%r8), %r11d
	movl	%ecx, %eax
	movslq	%ebx, %rbx
	leal	3(%r8), %ecx
	sall	$7, %eax
	movslq	%r11d, %r11
	leal	2(%r8), %r9d
	addl	%esi, %eax
	subl	$1, %esi
	sall	$4, %eax
	movl	%esi, 104(%rsp)
	movq	%rbx, %rsi
	movslq	%r9d, %r15
	leal	-137(%r8,%rax), %eax
	subq	%rdi, %rsi
	movl	%eax, 40(%rsp)
	leal	4(%r8), %eax
	addl	$1, %r8d
	cltq
	movq	%rsi, 64(%rsp)
	movq	%r11, %rsi
	movq	%rax, 24(%rsp)
	subq	%rdi, %rax
	subq	%rdi, %rsi
	movq	%rax, 80(%rsp)
	movslq	%ecx, %rax
	movq	%rax, 32(%rsp)
	subq	%rdi, %rax
	movq	%rax, 88(%rsp)
	movq	%r15, %rax
	subq	%rdi, %rax
	movl	%r8d, 60(%rsp)
	movq	%rsi, 72(%rsp)
	movq	%rax, 96(%rsp)
.L475:
	movl	48(%rsp), %r13d
	movl	%r14d, %r8d
	movl	%r14d, 108(%rsp)
	movq	8(%rsp), %r9
	jmp	.L495
	.p2align 4,,10
	.p2align 3
.L549:
	movq	8(%rbp), %rax
	vmovss	-4(%rax), %xmm0
	movslq	%r13d, %rax
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r10, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L479:
	movq	%r12, %rsi
	leal	-1(%r8), %ecx
	salq	%cl, %rsi
	movq	64(%rsp), %rcx
	addq	%r9, %rcx
	shrq	$6, %rcx
	testq	%rsi, 752(%rsp,%rcx,8)
	je	.L480
	movq	8(%rbp), %rcx
	vmovss	-4(%rcx), %xmm0
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%rbx, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L481:
	movq	%r12, %rsi
	leal	-2(%r8), %ecx
	salq	%cl, %rsi
	movq	72(%rsp), %rcx
	addq	%r9, %rcx
	shrq	$6, %rcx
	testq	%rsi, 752(%rsp,%rcx,8)
	je	.L482
	movq	8(%rbp), %rcx
	vmovss	-4(%rcx), %xmm0
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r11, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L483:
	movq	%r12, %rsi
	leal	-3(%r8), %ecx
	salq	%cl, %rsi
	movq	80(%rsp), %rcx
	addq	%r9, %rcx
	shrq	$6, %rcx
	testq	%rsi, 752(%rsp,%rcx,8)
	je	.L484
	movq	8(%rbp), %rcx
	vmovss	-4(%rcx), %xmm0
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	24(%rsp), %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L485:
	movq	%r12, %rsi
	leal	-4(%r8), %ecx
	salq	%cl, %rsi
	movq	88(%rsp), %rcx
	addq	%r9, %rcx
	shrq	$6, %rcx
	testq	%rsi, 752(%rsp,%rcx,8)
	je	.L486
	movq	8(%rbp), %rcx
	vmovss	-4(%rcx), %xmm0
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	32(%rsp), %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L487:
	movq	%r12, %rsi
	leal	-5(%r8), %ecx
	salq	%cl, %rsi
	movq	96(%rsp), %rcx
	addq	%r9, %rcx
	shrq	$6, %rcx
	testq	%rsi, 752(%rsp,%rcx,8)
	je	.L488
	movq	8(%rbp), %rcx
	vmovss	-4(%rcx), %xmm0
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r15, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L489:
	movq	%r12, %rsi
	leal	-6(%r8), %ecx
	salq	%cl, %rsi
	movslq	60(%rsp), %rcx
	movq	%rsi, %r14
	movq	%rcx, %rsi
	subq	%rdi, %rsi
	addq	%r9, %rsi
	shrq	$6, %rsi
	testq	%r14, 752(%rsp,%rsi,8)
	je	.L490
	movq	8(%rbp), %rsi
	vmovss	-4(%rsi), %xmm0
	movq	%rax, %rsi
	salq	$8, %rsi
	addq	%rsi, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L491:
	movq	%r12, %rsi
	leal	-7(%r8), %ecx
	salq	%cl, %rsi
	movq	%r9, %rcx
	shrq	$6, %rcx
	testq	%rsi, 752(%rsp,%rcx,8)
	je	.L548
	movq	8(%rbp), %rcx
	salq	$8, %rax
	addq	%rdi, %rax
	vmovss	-4(%rcx), %xmm0
	vmovss	%xmm0, (%rdx,%rax,4)
	subq	$4, 8(%rbp)
.L476:
	subl	$16, %r8d
	subl	$1, %r13d
	subq	$16, %r9
	cmpl	104(%rsp), %r13d
	je	.L477
.L495:
	movq	%r10, %rax
	movq	%r12, %rsi
	subq	%rdi, %rax
	movl	%r8d, %ecx
	addq	%r9, %rax
	salq	%cl, %rsi
	shrq	$6, %rax
	testq	%rsi, 752(%rsp,%rax,8)
	jne	.L549
	movslq	%r13d, %rax
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r10, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L479
.L548:
	salq	$8, %rax
	addq	%rdi, %rax
	movl	$0x00000000, (%rdx,%rax,4)
	jmp	.L476
.L490:
	movq	%rax, %rsi
	salq	$8, %rsi
	addq	%rsi, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L491
.L488:
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r15, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L489
.L486:
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	32(%rsp), %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L487
.L484:
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	24(%rsp), %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L485
.L482:
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r11, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L483
.L480:
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%rbx, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L481
.L477:
	movl	108(%rsp), %r14d
	subq	$262144, %rdx
	subq	$256, 8(%rsp)
	subl	$256, %r14d
	cmpl	40(%rsp), %r14d
	jne	.L475
	subl	$1, 16(%rsp)
	jne	.L496
	leaq	112(%rsp), %rdx
	movl	$8, %ecx
	xorl	%eax, %eax
	movq	%rdx, %rdi
	movl	$1, %r9d
	xorl	%edx, %edx
	rep; stosq
	xorl	%edi, %edi
.L497:
	movl	%edx, %r8d
	movl	%edi, %ebx
	movq	%rdi, %r11
	leal	128(%rdx), %r12d
	movq	%rdx, %r10
.L503:
	xorl	%eax, %eax
	jmp	.L500
.L498:
	addq	$1, %rax
	andn	112(%rsp,%rsi,8), %rcx, %rcx
	movq	%rcx, 112(%rsp,%rsi,8)
	cmpq	$8, %rax
	je	.L550
.L500:
	leal	(%r8,%rax), %ecx
	movq	%r9, %r14
	leaq	(%rax,%r11), %rsi
	salq	%cl, %r14
	leaq	(%rax,%r10), %rcx
	shrq	$6, %rsi
	leal	(%rbx,%rax), %r13d
	shrq	$6, %rcx
	andl	$63, %r13d
	testq	%r14, 752(%rsp,%rcx,8)
	shlx	%r13, %r9, %rcx
	je	.L498
	orq	112(%rsp,%rsi,8), %rcx
	addq	$1, %rax
	movq	%rcx, 112(%rsp,%rsi,8)
	cmpq	$8, %rax
	jne	.L500
.L550:
	addl	$16, %r8d
	addl	$8, %ebx
	addq	$16, %r10
	addq	$8, %r11
	cmpl	%r8d, %r12d
	jne	.L503
	addq	$256, %rdx
	addq	$64, %rdi
	cmpq	$2048, %rdx
	jne	.L497
	movq	112(%rsp), %rax
	movl	$7, %r13d
	movl	$1, %r15d
	movq	%rax, 176(%rsp)
	movq	120(%rsp), %rax
	movq	%rax, 184(%rsp)
	movq	128(%rsp), %rax
	movq	%rax, 192(%rsp)
	movq	136(%rsp), %rax
	movq	%rax, 200(%rsp)
	movq	144(%rsp), %rax
	movq	%rax, 208(%rsp)
	movq	152(%rsp), %rax
	movq	%rax, 216(%rsp)
	movq	160(%rsp), %rax
	movq	%rax, 224(%rsp)
	movq	168(%rsp), %rax
	movq	%rax, 232(%rsp)
.L515:
	leal	(%r13,%r13), %ecx
	movl	%r13d, %eax
	andl	$4, %ecx
	andl	$1, %eax
	movl	%ecx, %r8d
	movl	%r13d, %ecx
	sall	$2, %eax
	leal	3(%r8), %edi
	sarl	$2, %ecx
	leal	0(,%rcx,4), %esi
	sall	$5, %ecx
	movl	%edi, 24(%rsp)
	leal	3(%rax), %ebx
	movslq	%esi, %rdx
	addl	%r8d, %ecx
	leal	24(%rdi,%rsi,8), %esi
	movslq	%ebx, %rbx
	movslq	%eax, %rdi
	leal	2(%rax), %r11d
	sall	$3, %esi
	addq	$3, %rdx
	leal	1(%rax), %r10d
	movslq	%esi, %r9
	addl	%eax, %esi
	leal	-40(%rax,%rcx,8), %eax
	movslq	%r11d, %r11
	salq	$18, %rdx
	addq	(%rsp), %rdx
	movl	%eax, 16(%rsp)
	movq	%rbx, %rax
	movslq	%r10d, %r10
	leaq	(%r9,%rdi), %r14
	subq	%rdi, %rax
	movq	%rax, 32(%rsp)
	movq	%r11, %rax
	subq	%rdi, %rax
	movq	%r14, 8(%rsp)
	movq	%rax, 40(%rsp)
	movq	%r10, %rax
	subq	%rdi, %rax
	movq	%rax, 48(%rsp)
.L504:
	movq	8(%rsp), %r8
	movl	%esi, %r9d
	xorl	%r12d, %r12d
.L513:
	leal	3(%r9), %ecx
	movq	%r15, %r14
	movl	24(%rsp), %eax
	salq	%cl, %r14
	movq	32(%rsp), %rcx
	subl	%r12d, %eax
	addq	%r8, %rcx
	shrq	$6, %rcx
	testq	%r14, 176(%rsp,%rcx,8)
	je	.L505
	movq	8(%rbp), %rcx
	cltq
	vmovss	-4(%rcx), %xmm0
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%rbx, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	subq	$4, 8(%rbp)
.L506:
	movq	%r15, %r14
	leal	2(%r9), %ecx
	salq	%cl, %r14
	movq	40(%rsp), %rcx
	addq	%r8, %rcx
	shrq	$6, %rcx
	testq	%r14, 176(%rsp,%rcx,8)
	je	.L507
	movq	8(%rbp), %rcx
	movq	%rax, %r14
	salq	$8, %r14
	addq	%r11, %r14
	vmovss	-4(%rcx), %xmm0
	subq	$4, %rcx
	vmovss	%xmm0, (%rdx,%r14,4)
	movq	%rcx, 8(%rbp)
.L508:
	leal	1(%r9), %ecx
	movq	%r15, %r14
	salq	%cl, %r14
	movq	48(%rsp), %rcx
	addq	%r8, %rcx
	shrq	$6, %rcx
	testq	%r14, 176(%rsp,%rcx,8)
	je	.L509
	movq	8(%rbp), %rcx
	movq	%rax, %r14
	salq	$8, %r14
	addq	%r10, %r14
	vmovss	-4(%rcx), %xmm0
	subq	$4, %rcx
	vmovss	%xmm0, (%rdx,%r14,4)
	movq	%rcx, 8(%rbp)
.L510:
	movl	%r9d, %ecx
	movq	%r15, %r14
	salq	%cl, %r14
	movq	%r8, %rcx
	shrq	$6, %rcx
	testq	%r14, 176(%rsp,%rcx,8)
	je	.L511
	movq	8(%rbp), %rcx
	salq	$8, %rax
	addq	%rdi, %rax
	vmovss	-4(%rcx), %xmm0
	subq	$4, %rcx
	vmovss	%xmm0, (%rdx,%rax,4)
	movq	%rcx, 8(%rbp)
.L512:
	addq	$1, %r12
	subq	$8, %r8
	subl	$8, %r9d
	cmpq	$4, %r12
	jne	.L513
	subq	$64, 8(%rsp)
	subq	$262144, %rdx
	subl	$64, %esi
	cmpl	16(%rsp), %esi
	jne	.L504
	subl	$1, %r13d
	jne	.L515
	movq	0(%rbp), %rsi
	movl	$4, %ecx
	movq	(%rsp), %rax
	movq	%rsi, %rdx
.L516:
	vmovss	(%rdx), %xmm0
	addq	$262144, %rax
	addq	$64, %rdx
	vmovss	%xmm0, -262144(%rax)
	vmovss	-60(%rdx), %xmm0
	vmovss	%xmm0, -262140(%rax)
	vmovss	-56(%rdx), %xmm0
	vmovss	%xmm0, -262136(%rax)
	vmovss	-52(%rdx), %xmm0
	vmovss	%xmm0, -262132(%rax)
	vmovss	-48(%rdx), %xmm0
	vmovss	%xmm0, -261120(%rax)
	vmovss	-44(%rdx), %xmm0
	vmovss	%xmm0, -261116(%rax)
	vmovss	-40(%rdx), %xmm0
	vmovss	%xmm0, -261112(%rax)
	vmovss	-36(%rdx), %xmm0
	vmovss	%xmm0, -261108(%rax)
	vmovss	-32(%rdx), %xmm0
	vmovss	%xmm0, -260096(%rax)
	vmovss	-28(%rdx), %xmm0
	vmovss	%xmm0, -260092(%rax)
	vmovss	-24(%rdx), %xmm0
	vmovss	%xmm0, -260088(%rax)
	vmovss	-20(%rdx), %xmm0
	vmovss	%xmm0, -260084(%rax)
	vmovss	-16(%rdx), %xmm0
	vmovss	%xmm0, -259072(%rax)
	vmovss	-12(%rdx), %xmm0
	vmovss	%xmm0, -259068(%rax)
	vmovss	-8(%rdx), %xmm0
	vmovss	%xmm0, -259064(%rax)
	vmovss	-4(%rdx), %xmm0
	vmovss	%xmm0, -259060(%rax)
	subl	$1, %ecx
	jne	.L516
	movq	%rsi, 8(%rbp)
	addq	$599288, %rsp
	.cfi_remember_state
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%rbp
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r13
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	ret
.L505:
	.cfi_restore_state
	cltq
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%rbx, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L506
.L511:
	salq	$8, %rax
	addq	%rdi, %rax
	movl	$0x00000000, (%rdx,%rax,4)
	jmp	.L512
.L509:
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r10, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L510
.L507:
	movq	%rax, %rcx
	salq	$8, %rcx
	addq	%r11, %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L508
	.cfi_endproc
.LFE1482:
	.size	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f, .-_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f
	.section	.text.unlikely._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,comdat
.LCOLDE19:
	.section	.text._ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,"axG",@progbits,_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f,comdat
.LHOTE19:
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,comdat
.LCOLDB41:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,comdat
.LHOTB41:
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	.type	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi, @function
_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi:
.LFB1534:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	pushq	-8(%r10)
	pushq	%rbp
	leal	-1(%rsi), %eax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x78,0x6
	subq	$16, %rsp
	cmpl	$1, %eax
	jbe	.L557
	vmovups	32(%rdi), %ymm4
	vxorpd	%xmm6, %xmm6, %xmm6
	vmovups	(%rdi), %ymm1
	vmovups	96(%rdi), %ymm5
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm2
	vmovups	40(%rdi), %ymm14
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vmovaps	.LC32(%rip), %ymm12
	vshufps	$68, %ymm0, %ymm1, %ymm3
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovups	64(%rdi), %ymm3
	vaddps	%ymm0, %ymm2, %ymm0
	vmovsd	.LC33(%rip), %xmm13
	vmovapd	.LC38(%rip), %ymm8
	vshufps	$136, %ymm5, %ymm3, %ymm4
	vshufps	$221, %ymm5, %ymm3, %ymm3
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm1
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vinsertf128	$1, %xmm2, %ymm1, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm1
	vshufps	$68, %ymm1, %ymm3, %ymm4
	vshufps	$238, %ymm1, %ymm3, %ymm1
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vaddps	%ymm1, %ymm2, %ymm1
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -144(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtps2pd	%xmm0, %ymm10
	vcvtss2sd	-144(%rbp), %xmm6, %xmm6
	vcvtss2sd	-136(%rbp), %xmm5, %xmm5
	vextractf128	$0x1, %ymm0, %xmm0
	vmulsd	.LC35(%rip), %xmm6, %xmm3
	vmulps	%ymm12, %ymm1, %ymm1
	vmovaps	%ymm1, -112(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-140(%rbp), %xmm1, %xmm1
	vmulsd	%xmm13, %xmm1, %xmm7
	vmovss	4(%rdi), %xmm1
	vcvtps2pd	%xmm0, %ymm0
	vsubss	(%rdi), %xmm1, %xmm4
	vmulsd	.LC36(%rip), %xmm6, %xmm1
	vmovsd	.LC34(%rip), %xmm2
	vmovups	8(%rdi), %ymm6
	vmulsd	%xmm2, %xmm5, %xmm5
	vaddsd	%xmm7, %xmm3, %xmm3
	vshufps	$221, %ymm14, %ymm6, %ymm11
	vshufps	$136, %ymm14, %ymm6, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm14
	vsubsd	%xmm7, %xmm1, %xmm1
	vperm2f128	$3, %ymm11, %ymm11, %ymm7
	vshufps	$68, %ymm7, %ymm11, %ymm15
	vshufps	$238, %ymm7, %ymm11, %ymm7
	vshufps	$68, %ymm14, %ymm6, %ymm11
	vinsertf128	$1, %xmm7, %ymm15, %ymm7
	vshufps	$238, %ymm14, %ymm6, %ymm14
	vinsertf128	$1, %xmm14, %ymm11, %ymm11
	vsubps	%ymm11, %ymm7, %ymm7
	vmovapd	.LC39(%rip), %ymm11
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm5, %xmm1
	vmulpd	%ymm11, %ymm10, %ymm6
	vmulpd	%ymm8, %ymm10, %ymm10
	vmulpd	%ymm11, %ymm0, %ymm11
	vmovss	.LC37(%rip), %xmm5
	vmulpd	%ymm8, %ymm0, %ymm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm3, %xmm1
	vsubss	%xmm1, %xmm4, %xmm1
	vmovups	-136(%rbp), %ymm4
	vcvtps2pd	%xmm4, %ymm9
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm8, %ymm9, %ymm9
	vcvtps2pd	%xmm4, %ymm4
	vmulss	%xmm5, %xmm1, %xmm1
	vmulpd	%ymm8, %ymm4, %ymm4
	vmovss	%xmm1, -80(%rbp)
	vmovups	-140(%rbp), %ymm1
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vaddpd	%ymm3, %ymm6, %ymm6
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm10, %ymm3, %ymm3
	vaddpd	%ymm1, %ymm11, %ymm11
	vaddpd	%ymm0, %ymm1, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-112(%rbp), %xmm1, %xmm1
	vaddpd	%ymm9, %ymm6, %ymm6
	vaddpd	%ymm4, %ymm11, %ymm11
	vsubpd	%ymm9, %ymm3, %ymm9
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-104(%rbp), %xmm3, %xmm3
	vmulsd	%xmm2, %xmm3, %xmm8
	vsubpd	%ymm4, %ymm0, %ymm0
	vcvtpd2psy	%ymm6, %xmm6
	vmovss	76(%rdi), %xmm4
	vcvtpd2psy	%ymm11, %xmm11
	vinsertf128	$0x1, %xmm11, %ymm6, %ymm6
	vcvtpd2psy	%ymm9, %xmm9
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm9, %ymm0
	vsubps	%ymm0, %ymm6, %ymm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vsubss	72(%rdi), %xmm4, %xmm9
	vcvtss2sd	-108(%rbp), %xmm0, %xmm0
	vsubps	%ymm6, %ymm7, %ymm7
	vmovsd	.LC40(%rip), %xmm6
	vmulsd	%xmm6, %xmm1, %xmm4
	vmulps	%ymm12, %ymm7, %ymm12
	vxorps	%xmm7, %xmm7, %xmm7
	vmovups	%ymm12, -76(%rbp)
	vaddsd	%xmm0, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm7, %xmm7
	vmulsd	%xmm2, %xmm1, %xmm4
	vxorps	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm4
	vsubsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vxorps	%xmm7, %xmm7, %xmm7
	vsubss	%xmm1, %xmm9, %xmm1
	vmulss	%xmm5, %xmm1, %xmm1
	vmovss	%xmm1, -44(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-100(%rbp), %xmm1, %xmm1
	vmovss	84(%rdi), %xmm4
	vmulsd	%xmm2, %xmm1, %xmm9
	vsubss	80(%rdi), %xmm4, %xmm10
	vmulsd	%xmm6, %xmm0, %xmm4
	vaddsd	%xmm1, %xmm8, %xmm8
	movq	-144(%rbp), %rax
	movq	%rax, (%rdi)
	movq	-136(%rbp), %rax
	vaddsd	%xmm4, %xmm3, %xmm4
	vaddsd	%xmm9, %xmm4, %xmm4
	movq	%rax, 8(%rdi)
	movq	-128(%rbp), %rax
	vcvtsd2ss	%xmm4, %xmm7, %xmm7
	vmulsd	%xmm2, %xmm0, %xmm4
	vxorps	%xmm0, %xmm0, %xmm0
	movq	%rax, 16(%rdi)
	movq	-120(%rbp), %rax
	vaddsd	%xmm4, %xmm3, %xmm4
	movq	%rax, 24(%rdi)
	movq	-112(%rbp), %rax
	vsubsd	%xmm9, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm0, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmovss	92(%rdi), %xmm4
	vsubss	%xmm0, %xmm10, %xmm0
	vsubss	88(%rdi), %xmm4, %xmm10
	vmulsd	%xmm6, %xmm3, %xmm4
	vmulss	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm1, %xmm4
	vmovss	%xmm0, -40(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-96(%rbp), %xmm0, %xmm0
	vmulsd	%xmm2, %xmm0, %xmm7
	vaddsd	%xmm9, %xmm0, %xmm9
	vaddsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm7, %xmm8, %xmm8
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm4, %xmm3
	vmovss	100(%rdi), %xmm8
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-92(%rbp), %xmm4, %xmm4
	vaddsd	%xmm7, %xmm4, %xmm7
	vsubss	%xmm3, %xmm10, %xmm3
	vsubss	96(%rdi), %xmm8, %xmm10
	vmulsd	%xmm6, %xmm1, %xmm8
	vmulss	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm8, %xmm8
	vmovss	%xmm3, -36(%rbp)
	vmulsd	%xmm2, %xmm4, %xmm3
	vaddsd	%xmm3, %xmm8, %xmm8
	vsubsd	%xmm3, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-88(%rbp), %xmm8, %xmm8
	vmulsd	%xmm2, %xmm8, %xmm9
	vmulsd	%xmm13, %xmm8, %xmm13
	vsubss	%xmm1, %xmm10, %xmm1
	vmulss	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm1, -32(%rbp)
	vmovss	108(%rdi), %xmm1
	vsubss	104(%rdi), %xmm1, %xmm10
	vmulsd	%xmm6, %xmm0, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm4, %xmm6
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm7, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-84(%rbp), %xmm1, %xmm1
	vmulsd	%xmm2, %xmm1, %xmm7
	vxorps	%xmm2, %xmm2, %xmm2
	vsubss	%xmm0, %xmm10, %xmm0
	vaddsd	%xmm7, %xmm6, %xmm6
	vmulss	%xmm5, %xmm0, %xmm0
	vcvtsd2ss	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm8, %xmm6
	vmovss	%xmm0, -28(%rbp)
	vmovss	116(%rdi), %xmm0
	vsubsd	%xmm13, %xmm3, %xmm3
	vsubss	112(%rdi), %xmm0, %xmm0
	vsubsd	%xmm7, %xmm6, %xmm6
	vaddsd	%xmm4, %xmm13, %xmm13
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vsubss	%xmm2, %xmm0, %xmm2
	vmovss	124(%rdi), %xmm0
	vmulss	%xmm5, %xmm2, %xmm2
	vmovss	%xmm2, -24(%rbp)
	vsubss	120(%rdi), %xmm0, %xmm2
	vmulsd	.LC36(%rip), %xmm1, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm13, %xmm8
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulss	%xmm5, %xmm0, %xmm0
	vmovss	%xmm0, -20(%rbp)
	movq	%rax, 32(%rdi)
	movq	-80(%rbp), %rdx
	movq	-104(%rbp), %rax
	movq	%rdx, 64(%rdi)
	movq	-72(%rbp), %rdx
	movq	%rax, 40(%rdi)
	movq	-96(%rbp), %rax
	movq	%rdx, 72(%rdi)
	movq	-64(%rbp), %rdx
	movq	%rax, 48(%rdi)
	movq	-88(%rbp), %rax
	movq	%rdx, 80(%rdi)
	movq	-56(%rbp), %rdx
	movq	%rax, 56(%rdi)
	movq	%rdx, 88(%rdi)
	movq	-48(%rbp), %rdx
	movq	%rdx, 96(%rdi)
	movq	-40(%rbp), %rdx
	movq	%rdx, 104(%rdi)
	movq	-32(%rbp), %rdx
	movq	%rdx, 112(%rdi)
	movq	-24(%rbp), %rdx
	movq	%rdx, 120(%rdi)
	addq	$16, %rsp
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L557:
	.cfi_restore_state
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rdi), %xmm0, %xmm0
	vcvtss2sd	8(%rdi), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm9
	leaq	64(%rdi), %rax
	vmovsd	.LC21(%rip), %xmm10
	vmulsd	%xmm9, %xmm0, %xmm0
	vmovups	40(%rdi), %ymm4
	vmovsd	.LC22(%rip), %xmm12
	vmulsd	%xmm10, %xmm1, %xmm1
	vmovapd	.LC24(%rip), %ymm15
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rdi), %xmm1, %xmm1
	vmulsd	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	24(%rdi), %xmm0, %xmm0
	vmulsd	%xmm12, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	4(%rdi), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovups	8(%rdi), %ymm1
	vshufps	$136, %ymm4, %ymm1, %ymm2
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vmovss	%xmm0, -144(%rbp)
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm3
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vmovups	24(%rdi), %ymm2
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vcvtps2pd	%xmm0, %ymm4
	vshufps	$136, 56(%rdi), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vshufps	$68, %ymm3, %ymm2, %ymm8
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vmovups	16(%rdi), %ymm2
	vinsertf128	$1, %xmm3, %ymm8, %ymm8
	vshufps	$136, 48(%rdi), %ymm2, %ymm2
	vmulpd	%ymm15, %ymm4, %ymm4
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vshufps	$68, %ymm3, %ymm2, %ymm11
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vmovups	(%rdi), %ymm2
	vinsertf128	$1, %xmm3, %ymm11, %ymm11
	vshufps	$136, 32(%rdi), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vcvtps2pd	%xmm11, %ymm7
	vshufps	$68, %ymm3, %ymm2, %ymm6
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm13
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm13, %ymm13
	vinsertf128	$1, %xmm3, %ymm6, %ymm6
	vcvtps2pd	%xmm6, %ymm5
	vmovapd	.LC23(%rip), %ymm2
	vmulpd	%ymm15, %ymm7, %ymm7
	vextractf128	$0x1, %ymm11, %xmm11
	vcvtps2pd	%xmm11, %ymm11
	vmulpd	%ymm2, %ymm5, %ymm1
	vaddpd	%ymm4, %ymm1, %ymm14
	vextractf128	$0x1, %ymm0, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	%ymm15, %ymm1, %ymm1
	vmulpd	%ymm15, %ymm11, %ymm0
	vmovsd	.LC26(%rip), %xmm11
	vaddpd	%ymm7, %ymm14, %ymm4
	vcvtps2pd	%xmm8, %ymm7
	vextractf128	$0x1, %ymm8, %xmm8
	vmovapd	.LC25(%rip), %ymm14
	vcvtps2pd	%xmm8, %ymm8
	vmulpd	%ymm14, %ymm7, %ymm7
	vsubpd	%ymm7, %ymm4, %ymm7
	vextractf128	$0x1, %ymm6, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vmulpd	%ymm2, %ymm4, %ymm3
	vaddpd	%ymm1, %ymm3, %ymm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	80(%rdi), %xmm3, %xmm3
	vcvtpd2psy	%ymm7, %xmm7
	vaddpd	%ymm0, %ymm2, %ymm1
	vmulpd	%ymm14, %ymm8, %ymm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	64(%rdi), %xmm2, %xmm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	72(%rdi), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm8
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm7, %ymm0
	vsubps	%ymm0, %ymm13, %ymm0
	vmulsd	%xmm11, %xmm3, %xmm7
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	88(%rdi), %xmm13, %xmm13
	vmulsd	%xmm11, %xmm13, %xmm14
	vmovups	%ymm0, -140(%rbp)
	vmovsd	.LC27(%rip), %xmm0
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm2, %xmm2
	vmulsd	%xmm12, %xmm13, %xmm8
	vaddsd	%xmm7, %xmm1, %xmm1
	vmulsd	%xmm0, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm3, %xmm3
	vaddsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm2, %xmm2
	vmovss	76(%rdi), %xmm8
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm8, %xmm2
	vmovss	84(%rdi), %xmm8
	vmovss	%xmm2, -108(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	96(%rdi), %xmm2, %xmm2
	vmulsd	%xmm12, %xmm2, %xmm7
	vmulsd	%xmm0, %xmm2, %xmm0
	vsubsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	104(%rdi), %xmm8, %xmm8
	vmulsd	%xmm12, %xmm8, %xmm14
	vmovss	%xmm1, -104(%rbp)
	vmulsd	%xmm11, %xmm2, %xmm1
	vaddsd	%xmm1, %xmm3, %xmm3
	vaddsd	%xmm13, %xmm1, %xmm1
	vsubsd	%xmm14, %xmm3, %xmm3
	vmovss	92(%rdi), %xmm14
	vmovaps	%ymm6, -80(%rbp)
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm14, %xmm3
	vmulsd	%xmm11, %xmm8, %xmm14
	vmovss	%xmm3, -100(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	112(%rdi), %xmm3, %xmm3
	vaddsd	%xmm14, %xmm1, %xmm13
	vmulsd	%xmm12, %xmm3, %xmm1
	vsubsd	%xmm1, %xmm13, %xmm13
	vmovss	100(%rdi), %xmm1
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vsubss	%xmm13, %xmm1, %xmm13
	vaddsd	%xmm14, %xmm0, %xmm1
	vmulsd	%xmm11, %xmm3, %xmm0
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	120(%rdi), %xmm11, %xmm11
	vmulsd	%xmm12, %xmm11, %xmm12
	vmovss	%xmm13, -96(%rbp)
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	108(%rdi), %xmm1
	vsubsd	%xmm12, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	116(%rdi), %xmm1
	vmovss	%xmm0, -92(%rbp)
	vmulsd	%xmm9, %xmm8, %xmm0
	vsubsd	%xmm0, %xmm7, %xmm7
	vmulsd	%xmm10, %xmm3, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm7
	vmulsd	%xmm9, %xmm11, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm0
	vmovsd	.LC30(%rip), %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmulsd	.LC29(%rip), %xmm8, %xmm1
	vmovss	%xmm0, -88(%rbp)
	vmulsd	%xmm7, %xmm3, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm11, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	124(%rdi), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -84(%rbp)
	vmovss	64(%rdi), %xmm0
	vmovss	%xmm0, -48(%rbp)
	vmovss	72(%rdi), %xmm0
	vmovss	%xmm0, -44(%rbp)
	vmovss	80(%rdi), %xmm0
	vmovss	%xmm0, -40(%rbp)
	vmovss	88(%rdi), %xmm0
	vmovss	%xmm0, -36(%rbp)
	vmovss	96(%rdi), %xmm0
	vmovss	%xmm0, -32(%rbp)
	vmovss	104(%rdi), %xmm0
	vmovss	%xmm0, -28(%rbp)
	vmovss	112(%rdi), %xmm0
	vmovss	%xmm0, -24(%rbp)
	vmovss	120(%rdi), %xmm0
	vmovss	%xmm0, -20(%rbp)
	cmpl	$2, %esi
	jne	.L555
	vmovapd	.LC31(%rip), %ymm3
	vmovaps	-144(%rbp), %ymm2
	vcvtps2pd	%xmm2, %ymm0
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm3, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm0, %ymm5, %ymm0
	vmulpd	%ymm3, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm4, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-112(%rbp), %ymm1
	vmovaps	%ymm0, -80(%rbp)
	vmovaps	-48(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm4
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm3, %ymm4, %ymm4
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vmulpd	%ymm3, %ymm1, %ymm1
	vaddpd	%ymm4, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -48(%rbp)
.L555:
	movq	-80(%rbp), %rdx
	movq	%rdx, (%rdi)
	movq	-72(%rbp), %rdx
	movq	%rdx, 8(%rdi)
	movq	-64(%rbp), %rdx
	movq	%rdx, 16(%rdi)
	movq	-56(%rbp), %rdx
	movq	%rdx, 24(%rdi)
	movq	-48(%rbp), %rdx
	movq	%rdx, 32(%rdi)
	movq	-40(%rbp), %rdx
	movq	%rdx, 40(%rdi)
	movq	-32(%rbp), %rdx
	movq	%rdx, 48(%rdi)
	movq	-24(%rbp), %rdx
	movq	%rdx, 56(%rdi)
	movq	-144(%rbp), %rdx
	movq	%rdx, 64(%rdi)
	movq	-136(%rbp), %rdx
	movq	%rdx, 8(%rax)
	movq	-128(%rbp), %rdx
	movq	%rdx, 16(%rax)
	movq	-120(%rbp), %rdx
	movq	%rdx, 24(%rax)
	movq	-112(%rbp), %rdx
	movq	%rdx, 32(%rax)
	movq	-104(%rbp), %rdx
	movq	%rdx, 40(%rax)
	movq	-96(%rbp), %rdx
	movq	%rdx, 48(%rax)
	movq	-88(%rbp), %rdx
	movq	%rdx, 56(%rax)
	addq	$16, %rsp
	popq	%r10
	.cfi_def_cfa 10, 0
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1534:
	.size	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi, .-_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,comdat
.LCOLDE41:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi,comdat
.LHOTE41:
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,comdat
.LCOLDB42:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,comdat
.LHOTB42:
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi
	.type	_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi, @function
_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi:
.LFB1535:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movl	$64, %edx
	pushq	-8(%r10)
	pushq	%rbp
	leal	-1(%rsi), %eax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	movl	%esi, %r12d
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	leaq	64(%rdi), %rsi
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	movq	%rdi, %rbx
	subq	$152, %rsp
	cmpl	$1, %eax
	movq	(%rdi), %rax
	movq	%rax, -176(%rbp)
	movq	8(%rdi), %rax
	movq	%rax, -168(%rbp)
	movq	16(%rdi), %rax
	movq	%rax, -160(%rbp)
	movq	24(%rdi), %rax
	movq	%rax, -152(%rbp)
	movq	32(%rdi), %rax
	movq	%rax, -144(%rbp)
	movq	40(%rdi), %rax
	movq	%rax, -136(%rbp)
	movq	48(%rdi), %rax
	movq	%rax, -128(%rbp)
	movq	56(%rdi), %rax
	leaq	-112(%rbp), %rdi
	movq	%rax, -120(%rbp)
	jbe	.L572
	call	memcpy
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	-176(%rbp), %xmm8, %xmm8
	vcvtss2sd	-172(%rbp), %xmm1, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vmovsd	.LC36(%rip), %xmm4
	vmulsd	%xmm6, %xmm1, %xmm1
	vcvtss2sd	-168(%rbp), %xmm7, %xmm7
	vmovss	-112(%rbp), %xmm3
	vmovsd	.LC35(%rip), %xmm5
	vmulsd	%xmm4, %xmm8, %xmm2
	vmovsd	.LC34(%rip), %xmm0
	vmulsd	%xmm5, %xmm8, %xmm8
	vmovaps	-176(%rbp), %ymm9
	vmulsd	%xmm0, %xmm7, %xmm7
	vmovups	-108(%rbp), %ymm10
	vcvtps2pd	%xmm9, %ymm12
	vextractf128	$0x1, %ymm9, %xmm9
	vcvtps2pd	%xmm9, %ymm9
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vmovups	-172(%rbp), %ymm8
	vaddsd	%xmm7, %xmm2, %xmm2
	vcvtps2pd	%xmm8, %ymm11
	vextractf128	$0x1, %ymm8, %xmm8
	vcvtps2pd	%xmm8, %ymm8
	vsubsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	%xmm2, (%rbx)
	vmovups	-168(%rbp), %ymm2
	vmovss	%xmm1, 4(%rbx)
	vcvtps2pd	%xmm2, %ymm7
	vextractf128	$0x1, %ymm2, %xmm3
	vmovapd	.LC38(%rip), %ymm1
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm1, %ymm7, %ymm7
	vmulpd	%ymm1, %ymm3, %ymm3
	vmulpd	%ymm1, %ymm12, %ymm2
	vmulpd	%ymm1, %ymm9, %ymm1
	vaddpd	%ymm11, %ymm2, %ymm2
	vaddpd	%ymm8, %ymm1, %ymm1
	vsubpd	%ymm7, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm1, %ymm1
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm2
	vsubps	%ymm10, %ymm2, %ymm2
	vmovapd	.LC39(%rip), %ymm1
	vmulpd	%ymm1, %ymm9, %ymm9
	vmulpd	%ymm1, %ymm12, %ymm12
	vaddpd	%ymm9, %ymm8, %ymm8
	vmovss	-76(%rbp), %xmm9
	vaddpd	%ymm12, %ymm11, %ymm11
	vaddpd	%ymm8, %ymm3, %ymm3
	vmovsd	.LC40(%rip), %xmm8
	vaddpd	%ymm11, %ymm7, %ymm7
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm7, %xmm7
	vinsertf128	$0x1, %xmm3, %ymm7, %ymm3
	vaddps	%ymm3, %ymm10, %ymm10
	vunpcklps	%ymm10, %ymm2, %ymm3
	vunpckhps	%ymm10, %ymm2, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm1
	vperm2f128	$49, %ymm2, %ymm3, %ymm2
	vmovups	%ymm2, 40(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-144(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm7
	vxorpd	%xmm3, %xmm3, %xmm3
	vmulsd	%xmm8, %xmm2, %xmm2
	vcvtss2sd	-136(%rbp), %xmm3, %xmm3
	vmovups	%ymm1, 8(%rbx)
	vmulsd	%xmm0, %xmm3, %xmm11
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-140(%rbp), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm1, %xmm2
	vsubsd	%xmm11, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm11, %xmm2
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm9, %xmm2
	vmovss	-72(%rbp), %xmm9
	vmovss	%xmm7, 72(%rbx)
	vmulsd	%xmm0, %xmm1, %xmm7
	vmovss	%xmm2, 76(%rbx)
	vmulsd	%xmm8, %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-132(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm10
	vaddsd	%xmm2, %xmm11, %xmm11
	vaddsd	%xmm7, %xmm3, %xmm7
	vaddsd	%xmm1, %xmm3, %xmm1
	vmulsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm10, %xmm7, %xmm7
	vaddsd	%xmm1, %xmm10, %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-128(%rbp), %xmm9, %xmm9
	vaddsd	%xmm3, %xmm2, %xmm3
	vmulsd	%xmm8, %xmm2, %xmm2
	vmovss	%xmm7, 80(%rbx)
	vmulsd	%xmm0, %xmm9, %xmm7
	vaddsd	%xmm9, %xmm10, %xmm10
	vmovss	%xmm1, 84(%rbx)
	vmovss	-68(%rbp), %xmm1
	vaddsd	%xmm2, %xmm9, %xmm2
	vmulsd	%xmm8, %xmm9, %xmm9
	vsubsd	%xmm7, %xmm11, %xmm11
	vaddsd	%xmm3, %xmm7, %xmm3
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm1, %xmm11, %xmm11
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm1, %xmm3
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovss	%xmm11, 88(%rbx)
	vmovss	%xmm3, 92(%rbx)
	vcvtss2sd	-124(%rbp), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm3
	vmovss	-64(%rbp), %xmm11
	vaddsd	%xmm9, %xmm1, %xmm9
	vaddsd	%xmm1, %xmm7, %xmm7
	vmulsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm3, %xmm10, %xmm10
	vaddsd	%xmm2, %xmm3, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm11, %xmm10, %xmm10
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm11, %xmm2
	vmovss	-60(%rbp), %xmm11
	vmovss	%xmm10, 96(%rbx)
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-120(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm6
	vmovss	%xmm2, 100(%rbx)
	vmulsd	%xmm0, %xmm10, %xmm2
	vsubsd	%xmm2, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm2, %xmm2
	vmovss	-56(%rbp), %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm11, %xmm7, %xmm7
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm11, %xmm2
	vmovss	%xmm7, 104(%rbx)
	vaddsd	%xmm10, %xmm3, %xmm7
	vmovss	%xmm2, 108(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-116(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm0
	vmulsd	%xmm5, %xmm2, %xmm5
	vsubsd	%xmm6, %xmm3, %xmm3
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm3, %xmm2
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vmovss	%xmm7, 112(%rbx)
	vaddsd	%xmm1, %xmm10, %xmm7
	vaddsd	%xmm6, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm0, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vmovss	%xmm0, 116(%rbx)
	vmovss	-52(%rbp), %xmm0
	vsubss	%xmm0, %xmm1, %xmm1
	vaddss	%xmm2, %xmm0, %xmm2
	vmovss	%xmm1, 120(%rbx)
	vmovss	%xmm2, 124(%rbx)
.L558:
	addq	$152, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L572:
	.cfi_restore_state
	call	memcpy
	cmpl	$2, %r12d
	jne	.L563
	vmovapd	.LC31(%rip), %ymm3
	vmovaps	-176(%rbp), %ymm0
	vmovaps	-112(%rbp), %ymm2
	vcvtps2pd	%xmm0, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm3, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vsubpd	%ymm4, %ymm1, %ymm1
	vmulpd	%ymm3, %ymm2, %ymm2
	vsubpd	%ymm2, %ymm0, %ymm0
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-80(%rbp), %ymm1
	vmovaps	%ymm0, -176(%rbp)
	vmovaps	-144(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm4
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm3, %ymm4, %ymm4
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vmulpd	%ymm3, %ymm1, %ymm1
	vsubpd	%ymm4, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -144(%rbp)
.L563:
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-176(%rbp), %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vmovss	-176(%rbp), %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm12, %xmm12, %xmm12
	vmovss	%xmm0, (%rbx)
	vmovss	-172(%rbp), %xmm0
	vmovss	%xmm0, 8(%rbx)
	vmovss	-168(%rbp), %xmm0
	vmovss	%xmm0, 16(%rbx)
	vmovss	-164(%rbp), %xmm0
	vmovss	%xmm0, 24(%rbx)
	vmovss	-160(%rbp), %xmm0
	vmovss	%xmm0, 32(%rbx)
	vmovss	-156(%rbp), %xmm0
	vmovss	%xmm0, 40(%rbx)
	vmovss	-152(%rbp), %xmm0
	vmovss	%xmm0, 48(%rbx)
	vmovss	-148(%rbp), %xmm0
	vmovss	%xmm0, 56(%rbx)
	vmovss	-144(%rbp), %xmm0
	vmovss	%xmm0, 64(%rbx)
	vmovss	-140(%rbp), %xmm0
	vmovss	%xmm0, 72(%rbx)
	vmovss	-136(%rbp), %xmm0
	vmovss	%xmm0, 80(%rbx)
	vmovss	-132(%rbp), %xmm0
	vmovss	%xmm0, 88(%rbx)
	vmovss	-128(%rbp), %xmm0
	vmovss	%xmm0, 96(%rbx)
	vmovss	-124(%rbp), %xmm0
	vmovss	%xmm0, 104(%rbx)
	vmovss	-120(%rbp), %xmm0
	vmovss	%xmm0, 112(%rbx)
	vmovss	-116(%rbp), %xmm0
	vmovss	%xmm0, 120(%rbx)
	vmovsd	.LC20(%rip), %xmm7
	vcvtss2sd	-172(%rbp), %xmm10, %xmm10
	vcvtss2sd	-168(%rbp), %xmm9, %xmm9
	vmovsd	.LC21(%rip), %xmm8
	vcvtss2sd	-164(%rbp), %xmm5, %xmm5
	vcvtss2sd	-160(%rbp), %xmm12, %xmm12
	vmovsd	.LC22(%rip), %xmm3
	vmulsd	%xmm7, %xmm1, %xmm0
	vmulsd	%xmm8, %xmm10, %xmm2
	vmulsd	%xmm3, %xmm5, %xmm11
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	%xmm7, %xmm9, %xmm2
	vsubsd	%xmm2, %xmm0, %xmm0
	vmovsd	.LC26(%rip), %xmm2
	vmulsd	%xmm2, %xmm10, %xmm4
	vaddsd	%xmm11, %xmm0, %xmm0
	vmulsd	%xmm2, %xmm9, %xmm6
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-112(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 4(%rbx)
	vmovsd	.LC27(%rip), %xmm0
	vmulsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm0, %xmm10, %xmm10
	vmulsd	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm4, %xmm1, %xmm4
	vaddsd	%xmm6, %xmm4, %xmm1
	vmulsd	%xmm3, %xmm12, %xmm4
	vaddsd	%xmm10, %xmm6, %xmm6
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-136(%rbp), %xmm10, %xmm10
	vsubsd	%xmm11, %xmm1, %xmm1
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-148(%rbp), %xmm11, %xmm11
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-108(%rbp), %xmm1, %xmm1
	vmovss	%xmm1, 12(%rbx)
	vmulsd	%xmm2, %xmm5, %xmm1
	vmulsd	%xmm0, %xmm5, %xmm5
	vaddsd	%xmm1, %xmm6, %xmm6
	vaddsd	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm4, %xmm6, %xmm4
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-156(%rbp), %xmm6, %xmm6
	vmulsd	%xmm3, %xmm6, %xmm9
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-104(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 20(%rbx)
	vmulsd	%xmm2, %xmm12, %xmm4
	vmulsd	%xmm0, %xmm12, %xmm12
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm5, %xmm4, %xmm4
	vsubsd	%xmm9, %xmm1, %xmm1
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-152(%rbp), %xmm9, %xmm9
	vmulsd	%xmm3, %xmm9, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-100(%rbp), %xmm1, %xmm1
	vmovss	%xmm1, 28(%rbx)
	vmulsd	%xmm2, %xmm6, %xmm1
	vmulsd	%xmm0, %xmm6, %xmm6
	vaddsd	%xmm1, %xmm4, %xmm4
	vaddsd	%xmm12, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm4, %xmm4
	vmulsd	%xmm3, %xmm11, %xmm5
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-96(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 36(%rbx)
	vmulsd	%xmm2, %xmm9, %xmm4
	vmulsd	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm4, %xmm4
	vsubsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm2, %xmm11, %xmm5
	vmulsd	%xmm0, %xmm11, %xmm11
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-92(%rbp), %xmm1, %xmm1
	vaddsd	%xmm5, %xmm4, %xmm4
	vmovss	%xmm1, 44(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-144(%rbp), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm6
	vaddsd	%xmm9, %xmm5, %xmm5
	vsubsd	%xmm6, %xmm4, %xmm4
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-140(%rbp), %xmm6, %xmm6
	vmulsd	%xmm3, %xmm6, %xmm9
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-88(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 52(%rbx)
	vmulsd	%xmm2, %xmm1, %xmm4
	vmulsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm5, %xmm5
	vaddsd	%xmm11, %xmm4, %xmm4
	vsubsd	%xmm9, %xmm5, %xmm5
	vmulsd	%xmm2, %xmm6, %xmm9
	vmulsd	%xmm0, %xmm6, %xmm6
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-84(%rbp), %xmm5, %xmm5
	vaddsd	%xmm9, %xmm4, %xmm4
	vmovss	%xmm5, 60(%rbx)
	vmulsd	%xmm3, %xmm10, %xmm5
	vaddsd	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm4, %xmm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-80(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 68(%rbx)
	vmulsd	%xmm2, %xmm10, %xmm4
	vcvtss2sd	-132(%rbp), %xmm5, %xmm5
	vmulsd	%xmm3, %xmm5, %xmm9
	vmulsd	%xmm2, %xmm5, %xmm11
	vmulsd	%xmm0, %xmm10, %xmm10
	vmulsd	%xmm0, %xmm5, %xmm5
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm4, %xmm4
	vsubsd	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm11, %xmm4, %xmm6
	vaddsd	%xmm11, %xmm10, %xmm10
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-76(%rbp), %xmm1, %xmm1
	vmovss	%xmm1, 76(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-128(%rbp), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm4
	vmulsd	%xmm2, %xmm1, %xmm9
	vmulsd	%xmm0, %xmm1, %xmm0
	vmulsd	.LC28(%rip), %xmm1, %xmm1
	vsubsd	%xmm4, %xmm6, %xmm6
	vaddsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm9, %xmm5, %xmm5
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vaddss	-72(%rbp), %xmm6, %xmm6
	vmovss	%xmm6, 84(%rbx)
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-124(%rbp), %xmm6, %xmm6
	vmulsd	%xmm3, %xmm6, %xmm11
	vsubsd	%xmm11, %xmm10, %xmm10
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vaddss	-68(%rbp), %xmm10, %xmm10
	vmovss	%xmm10, 92(%rbx)
	vmulsd	%xmm2, %xmm6, %xmm10
	vaddsd	%xmm10, %xmm5, %xmm9
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-120(%rbp), %xmm5, %xmm5
	vmulsd	%xmm2, %xmm5, %xmm2
	vaddsd	%xmm10, %xmm0, %xmm0
	vmulsd	%xmm3, %xmm5, %xmm11
	vmulsd	%xmm8, %xmm5, %xmm8
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-116(%rbp), %xmm2, %xmm2
	vmulsd	%xmm3, %xmm2, %xmm3
	vsubsd	%xmm11, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vaddss	-64(%rbp), %xmm9, %xmm9
	vsubsd	%xmm3, %xmm0, %xmm0
	vmovsd	.LC30(%rip), %xmm3
	vmovss	%xmm9, 100(%rbx)
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-60(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 108(%rbx)
	vmulsd	%xmm7, %xmm6, %xmm0
	vmulsd	.LC29(%rip), %xmm6, %xmm6
	vmulsd	%xmm7, %xmm2, %xmm7
	vsubsd	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm3, %xmm5, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm0
	vaddsd	%xmm7, %xmm4, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vaddss	-56(%rbp), %xmm7, %xmm7
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	%xmm7, 116(%rbx)
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-52(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 124(%rbx)
	jmp	.L558
	.cfi_endproc
.LFE1535:
	.size	_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi, .-_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,comdat
.LCOLDE42:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi,comdat
.LHOTE42:
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,comdat
.LCOLDB43:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,comdat
.LHOTB43:
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	.type	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi, @function
_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi:
.LFB1573:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movq	%rdi, %rcx
	pushq	-8(%r10)
	pushq	%rbp
	leal	-1(%rsi), %eax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	pushq	%rbx
	subq	$1048, %rsp
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	cmpl	$1, %eax
	jbe	.L587
	vmovaps	.LC32(%rip), %ymm12
	leaq	-1072(%rbp), %rbx
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L574:
	vmovups	(%rcx,%rax,2), %ymm2
	vmovups	32(%rcx,%rax,2), %ymm4
	vshufps	$136, %ymm4, %ymm2, %ymm3
	vshufps	$221, %ymm4, %ymm2, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm1
	vshufps	$68, %ymm1, %ymm3, %ymm0
	vshufps	$238, %ymm1, %ymm3, %ymm1
	vinsertf128	$1, %xmm1, %ymm0, %ymm1
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm3
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vaddps	%ymm0, %ymm1, %ymm0
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, (%rbx,%rax)
	addq	$32, %rax
	cmpq	$512, %rax
	jne	.L574
	vxorpd	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-1072(%rbp), %xmm4, %xmm4
	vcvtss2sd	-1068(%rbp), %xmm0, %xmm0
	vmulsd	.LC35(%rip), %xmm4, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-1064(%rbp), %xmm2, %xmm2
	xorw	%ax, %ax
	vmovsd	.LC34(%rip), %xmm14
	leaq	-560(%rbp), %r12
	vmovss	.LC37(%rip), %xmm15
	leaq	8(%rbx), %r8
	vmovapd	.LC38(%rip), %ymm11
	vmulsd	%xmm14, %xmm2, %xmm2
	vmovapd	.LC39(%rip), %ymm13
	leaq	4(%rbx), %rdi
	vmulsd	.LC33(%rip), %xmm0, %xmm5
	vmovss	4(%rcx), %xmm0
	leaq	4(%r12), %rdx
	vmulsd	.LC36(%rip), %xmm4, %xmm4
	vsubss	(%rcx), %xmm0, %xmm3
	vaddsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm4, %xmm0
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmulss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -560(%rbp)
	.p2align 4,,10
	.p2align 3
.L579:
	vmovups	40(%rcx,%rax,2), %ymm9
	vmovups	(%rdi,%rax), %ymm2
	vmovups	8(%rcx,%rax,2), %ymm8
	vmovaps	(%rbx,%rax), %ymm5
	vcvtps2pd	%xmm2, %ymm1
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vshufps	$221, %ymm9, %ymm8, %ymm10
	vshufps	$136, %ymm9, %ymm8, %ymm8
	vperm2f128	$3, %ymm10, %ymm10, %ymm2
	vshufps	$68, %ymm2, %ymm10, %ymm3
	vcvtps2pd	%xmm5, %ymm7
	vperm2f128	$3, %ymm8, %ymm8, %ymm9
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vshufps	$238, %ymm2, %ymm10, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vshufps	$68, %ymm9, %ymm8, %ymm3
	vshufps	$238, %ymm9, %ymm8, %ymm9
	vmulpd	%ymm13, %ymm7, %ymm8
	vinsertf128	$1, %xmm9, %ymm3, %ymm3
	vaddpd	%ymm1, %ymm8, %ymm8
	vmulpd	%ymm11, %ymm7, %ymm7
	vsubps	%ymm3, %ymm2, %ymm3
	vmulpd	%ymm13, %ymm5, %ymm2
	vmulpd	%ymm11, %ymm5, %ymm5
	vmovups	(%r8,%rax), %ymm4
	vaddpd	%ymm0, %ymm2, %ymm2
	vaddpd	%ymm7, %ymm1, %ymm1
	vcvtps2pd	%xmm4, %ymm6
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm11, %ymm6, %ymm6
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm4, %ymm4
	vmulpd	%ymm11, %ymm4, %ymm4
	vaddpd	%ymm6, %ymm8, %ymm8
	vaddpd	%ymm4, %ymm2, %ymm2
	vsubpd	%ymm6, %ymm1, %ymm1
	vsubpd	%ymm4, %ymm0, %ymm0
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm8, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm2, %ymm0
	vsubps	%ymm0, %ymm3, %ymm3
	vmulps	%ymm12, %ymm3, %ymm3
	vmovups	%ymm3, (%rdx,%rax)
	addq	$32, %rax
	cmpq	$480, %rax
	jne	.L579
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-592(%rbp), %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-584(%rbp), %xmm2, %xmm2
	vmulsd	%xmm14, %xmm2, %xmm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-588(%rbp), %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmovsd	.LC40(%rip), %xmm4
	movl	$512, %edx
	movq	%rbx, %rsi
	movq	%rcx, %rdi
	vmovss	972(%rcx), %xmm3
	vsubss	968(%rcx), %xmm3, %xmm7
	vmulsd	%xmm4, %xmm1, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	%xmm14, %xmm1, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm1
	vsubsd	%xmm6, %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovss	980(%rcx), %xmm3
	vxorps	%xmm5, %xmm5, %xmm5
	vsubss	976(%rcx), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm3
	vsubss	%xmm1, %xmm7, %xmm1
	vmulss	%xmm15, %xmm1, %xmm1
	vaddsd	%xmm3, %xmm2, %xmm3
	vmovss	%xmm1, -76(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-580(%rbp), %xmm1, %xmm1
	vmulsd	%xmm14, %xmm1, %xmm7
	vaddsd	%xmm1, %xmm6, %xmm6
	vaddsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	%xmm14, %xmm0, %xmm3
	vaddsd	%xmm3, %xmm2, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmovss	988(%rcx), %xmm3
	vsubss	%xmm0, %xmm8, %xmm0
	vsubss	984(%rcx), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm2, %xmm3
	vmulss	%xmm15, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm1, %xmm3
	vmovss	%xmm0, -72(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-576(%rbp), %xmm0, %xmm0
	vmulsd	%xmm14, %xmm0, %xmm5
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm6, %xmm6
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm3, %xmm2
	vmovss	996(%rcx), %xmm6
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-572(%rbp), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm3, %xmm5
	vsubss	%xmm2, %xmm8, %xmm2
	vsubss	992(%rcx), %xmm6, %xmm8
	vmulsd	%xmm4, %xmm1, %xmm6
	vmulss	%xmm15, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm6, %xmm6
	vmovss	%xmm2, -68(%rbp)
	vmulsd	%xmm14, %xmm3, %xmm2
	vaddsd	%xmm2, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm7, %xmm6, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-568(%rbp), %xmm6, %xmm6
	vmulsd	%xmm14, %xmm6, %xmm7
	vsubss	%xmm1, %xmm8, %xmm1
	vmulss	%xmm15, %xmm1, %xmm1
	vsubsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm1, -64(%rbp)
	vmovss	1004(%rcx), %xmm1
	vsubss	1000(%rcx), %xmm1, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm1
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm5, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm3, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-564(%rbp), %xmm5, %xmm5
	vmulsd	%xmm14, %xmm5, %xmm14
	vaddsd	%xmm2, %xmm6, %xmm4
	vsubss	%xmm0, %xmm8, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm3
	vmulsd	.LC33(%rip), %xmm6, %xmm6
	vmulss	%xmm15, %xmm0, %xmm0
	vsubsd	%xmm14, %xmm4, %xmm4
	vaddsd	%xmm14, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm0, -60(%rbp)
	vmovss	1012(%rcx), %xmm0
	vsubsd	%xmm6, %xmm2, %xmm2
	vsubss	1008(%rcx), %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm6, %xmm6
	vsubss	%xmm3, %xmm0, %xmm0
	vmulss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -56(%rbp)
	vmovss	1020(%rcx), %xmm0
	vsubss	1016(%rcx), %xmm0, %xmm7
	vmulsd	.LC36(%rip), %xmm5, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm5, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm6, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -52(%rbp)
	call	memcpy
	movl	$512, %edx
	movq	%r12, %rsi
	leaq	512(%rax), %rdi
	call	memcpy
.L573:
	addq	$1048, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L587:
	.cfi_restore_state
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rcx), %xmm0, %xmm0
	vcvtss2sd	8(%rcx), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm9
	leaq	-1072(%rbp), %rbx
	vmovsd	.LC21(%rip), %xmm10
	leaq	8(%rdi), %rax
	vmovsd	.LC22(%rip), %xmm8
	vmulsd	%xmm9, %xmm0, %xmm0
	vmovapd	.LC23(%rip), %ymm7
	vmulsd	%xmm10, %xmm1, %xmm1
	vmovapd	.LC24(%rip), %ymm5
	leaq	4(%rbx), %rdx
	vmovapd	.LC25(%rip), %ymm6
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rcx), %xmm1, %xmm1
	vmulsd	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%rcx), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rdi), %xmm1
	leaq	484(%rbx), %rdi
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -1072(%rbp)
	.p2align 4,,10
	.p2align 3
.L575:
	vmovups	32(%rax), %ymm4
	addq	$32, %rdx
	addq	$64, %rax
	vmovups	-64(%rax), %ymm2
	vshufps	$136, %ymm4, %ymm2, %ymm3
	vshufps	$221, %ymm4, %ymm2, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vshufps	$68, %ymm0, %ymm3, %ymm1
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vmovups	-48(%rax), %ymm0
	vshufps	$136, -16(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm11
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vmovups	-56(%rax), %ymm0
	vinsertf128	$1, %xmm3, %ymm11, %ymm11
	vshufps	$136, -24(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm12
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm12, %ymm12
	vmovups	-72(%rax), %ymm3
	vshufps	$136, -40(%rax), %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vshufps	$68, %ymm0, %ymm3, %ymm13
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vinsertf128	$1, %xmm0, %ymm13, %ymm0
	vshufps	$68, %ymm3, %ymm2, %ymm4
	vcvtps2pd	%xmm0, %ymm13
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vcvtps2pd	%xmm1, %ymm2
	vinsertf128	$1, %xmm3, %ymm4, %ymm4
	vmulpd	%ymm5, %ymm2, %ymm2
	vmulpd	%ymm7, %ymm13, %ymm3
	vaddpd	%ymm2, %ymm3, %ymm13
	vcvtps2pd	%xmm12, %ymm3
	vmulpd	%ymm5, %ymm3, %ymm3
	vextractf128	$0x1, %ymm0, %xmm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	%ymm7, %ymm0, %ymm0
	vmulpd	%ymm5, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm13, %ymm2
	vcvtps2pd	%xmm11, %ymm3
	vmulpd	%ymm6, %ymm3, %ymm3
	vsubpd	%ymm3, %ymm2, %ymm3
	vaddpd	%ymm1, %ymm0, %ymm2
	vextractf128	$0x1, %ymm12, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm5, %ymm0, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm0, %ymm2, %ymm1
	vextractf128	$0x1, %ymm11, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm6, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm3, %ymm0
	vsubps	%ymm0, %ymm4, %ymm0
	vmovups	%ymm0, -32(%rdx)
	cmpq	%rdi, %rdx
	jne	.L575
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	968(%rcx), %xmm3, %xmm3
	vcvtss2sd	960(%rcx), %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm5
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	976(%rcx), %xmm2, %xmm2
	vxorpd	%xmm6, %xmm6, %xmm6
	vmovsd	.LC27(%rip), %xmm0
	vcvtss2sd	984(%rcx), %xmm6, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm7
	vmulsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm2, %xmm4
	vmulsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm5, %xmm6, %xmm11
	vmulsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm7, %xmm1, %xmm1
	vmulsd	%xmm8, %xmm6, %xmm7
	vmulsd	%xmm0, %xmm6, %xmm6
	vaddsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm11, %xmm2, %xmm2
	vaddsd	%xmm11, %xmm3, %xmm3
	vsubsd	%xmm7, %xmm1, %xmm1
	vmovss	972(%rcx), %xmm7
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vmovss	980(%rcx), %xmm7
	vmovss	%xmm1, -588(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	992(%rcx), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm4
	vmulsd	%xmm0, %xmm1, %xmm0
	vsubsd	%xmm4, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm7, %xmm3
	vmulsd	%xmm5, %xmm1, %xmm7
	vmulsd	.LC28(%rip), %xmm1, %xmm1
	vmovss	%xmm3, -584(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	1000(%rcx), %xmm3, %xmm3
	vmulsd	%xmm8, %xmm3, %xmm11
	vaddsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm6, %xmm7, %xmm7
	vsubsd	%xmm11, %xmm2, %xmm2
	vmovss	988(%rcx), %xmm11
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm11, %xmm2
	vmulsd	%xmm5, %xmm3, %xmm11
	vmovss	%xmm2, -580(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	1008(%rcx), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm6
	vmulsd	%xmm5, %xmm2, %xmm5
	vaddsd	%xmm11, %xmm7, %xmm7
	vaddsd	%xmm11, %xmm0, %xmm0
	vsubsd	%xmm6, %xmm7, %xmm6
	vmovss	996(%rcx), %xmm7
	vaddsd	%xmm5, %xmm0, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	1016(%rcx), %xmm5, %xmm5
	vmulsd	%xmm8, %xmm5, %xmm8
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm8, %xmm0, %xmm0
	vmovss	%xmm6, -576(%rbp)
	vmovss	1004(%rcx), %xmm6
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, -572(%rbp)
	vmulsd	%xmm9, %xmm3, %xmm0
	vmulsd	%xmm9, %xmm5, %xmm9
	vsubsd	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm10, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm0
	vmovss	1012(%rcx), %xmm4
	vaddsd	%xmm9, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, -568(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm0
	vmovsd	.LC30(%rip), %xmm3
	vmulsd	%xmm3, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm0
	vmovss	1020(%rcx), %xmm1
	vaddsd	%xmm5, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -564(%rbp)
	vmovups	(%rcx), %ymm0
	vshufps	$136, 32(%rcx), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	64(%rcx), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 96(%rcx), %ymm0, %ymm0
	vmovaps	%ymm1, -560(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm1, -528(%rbp)
	vmovups	128(%rcx), %ymm0
	vshufps	$136, 160(%rcx), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	192(%rcx), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 224(%rcx), %ymm0, %ymm0
	vmovaps	%ymm1, -496(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	256(%rcx), %ymm0
	vshufps	$136, 288(%rcx), %ymm0, %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm1, -464(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovups	320(%rcx), %ymm0
	vmovaps	%ymm1, -432(%rbp)
	vshufps	$136, 352(%rcx), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	384(%rcx), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 416(%rcx), %ymm0, %ymm0
	vmovaps	%ymm1, -400(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	448(%rcx), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 480(%rcx), %ymm0, %ymm0
	vmovaps	%ymm1, -368(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	512(%rcx), %ymm0
	vshufps	$136, 544(%rcx), %ymm0, %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm1, -336(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovups	576(%rcx), %ymm0
	vmovaps	%ymm1, -304(%rbp)
	vshufps	$136, 608(%rcx), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	640(%rcx), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 672(%rcx), %ymm0, %ymm0
	vmovaps	%ymm1, -272(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	704(%rcx), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 736(%rcx), %ymm0, %ymm0
	vmovaps	%ymm1, -240(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	768(%rcx), %ymm0
	vshufps	$136, 800(%rcx), %ymm0, %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm1, -208(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm1, -176(%rbp)
	vmovups	832(%rcx), %ymm0
	vshufps	$136, 864(%rcx), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	896(%rcx), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 928(%rcx), %ymm0, %ymm0
	vmovaps	%ymm1, -144(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovss	960(%rcx), %xmm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm1, -112(%rbp)
	vmovss	%xmm0, -80(%rbp)
	vmovss	968(%rcx), %xmm0
	vmovss	%xmm0, -76(%rbp)
	vmovss	976(%rcx), %xmm0
	vmovss	%xmm0, -72(%rbp)
	vmovss	984(%rcx), %xmm0
	vmovss	%xmm0, -68(%rbp)
	vmovss	992(%rcx), %xmm0
	vmovss	%xmm0, -64(%rbp)
	vmovss	1000(%rcx), %xmm0
	vmovss	%xmm0, -60(%rbp)
	vmovss	1008(%rcx), %xmm0
	vmovss	%xmm0, -56(%rbp)
	vmovss	1016(%rcx), %xmm0
	vmovss	%xmm0, -52(%rbp)
	cmpl	$2, %esi
	jne	.L588
	vmovapd	.LC31(%rip), %ymm4
	leaq	-560(%rbp), %r12
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L576:
	vmovaps	(%r12,%rax), %ymm0
	vmovaps	(%rbx,%rax), %ymm2
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm2, %ymm1
	vmulpd	%ymm4, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm5, %ymm3
	vcvtps2pd	%xmm0, %ymm1
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm4, %ymm0, %ymm0
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, (%r12,%rax)
	addq	$32, %rax
	cmpq	$512, %rax
	jne	.L576
.L578:
	movq	%r12, %rsi
	movq	%rcx, %rdi
	movl	$512, %edx
	call	memcpy
	movl	$512, %edx
	movq	%rbx, %rsi
	leaq	512(%rax), %rdi
	call	memcpy
	jmp	.L573
.L588:
	leaq	-560(%rbp), %r12
	jmp	.L578
	.cfi_endproc
.LFE1573:
	.size	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi, .-_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,comdat
.LCOLDE43:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi,comdat
.LHOTE43:
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,comdat
.LCOLDB50:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,comdat
.LHOTB50:
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	.type	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi, @function
_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi:
.LFB1606:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	pushq	-8(%r10)
	pushq	%rbp
	leal	-1(%rsi), %eax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x78,0x6
	cmpl	$1, %eax
	jbe	.L595
	vmovups	32(%rdi), %ymm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovups	(%rdi), %ymm2
	vmovsd	.LC33(%rip), %xmm11
	vmovsd	.LC35(%rip), %xmm9
	vshufps	$136, %ymm4, %ymm2, %ymm3
	vshufps	$221, %ymm4, %ymm2, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm1
	vshufps	$68, %ymm1, %ymm3, %ymm0
	vshufps	$238, %ymm1, %ymm3, %ymm1
	vmovsd	.LC36(%rip), %xmm10
	vinsertf128	$1, %xmm1, %ymm0, %ymm1
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm3
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vaddps	%ymm0, %ymm1, %ymm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vmulps	.LC32(%rip), %ymm0, %ymm0
	vmovaps	%ymm0, -80(%rbp)
	vmovss	4(%rdi), %xmm0
	vcvtss2sd	-80(%rbp), %xmm4, %xmm4
	vcvtss2sd	-76(%rbp), %xmm5, %xmm5
	vmulsd	%xmm9, %xmm4, %xmm1
	vcvtss2sd	-72(%rbp), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm5, %xmm5
	vsubss	(%rdi), %xmm0, %xmm3
	vmulsd	%xmm10, %xmm4, %xmm4
	vmovups	24(%rdi), %xmm8
	vmovsd	.LC34(%rip), %xmm12
	vmovss	.LC37(%rip), %xmm7
	vmulsd	%xmm12, %xmm2, %xmm2
	vmovups	8(%rdi), %xmm6
	vmovapd	.LC47(%rip), %xmm13
	vaddsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm4, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm2, %xmm2
	vxorps	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm1, %xmm0
	vmovaps	-80(%rbp), %xmm1
	vcvtps2pd	%xmm1, %xmm15
	vmovhlps	%xmm1, %xmm5, %xmm5
	vxorps	%xmm1, %xmm1, %xmm1
	vsubss	%xmm0, %xmm3, %xmm0
	vmovups	-72(%rbp), %xmm3
	vcvtps2pd	%xmm5, %xmm5
	vmovhlps	%xmm3, %xmm4, %xmm4
	vmulss	%xmm7, %xmm0, %xmm0
	vcvtps2pd	%xmm4, %xmm4
	vmulpd	%xmm13, %xmm4, %xmm4
	vmovss	%xmm0, -48(%rbp)
	vmovups	-76(%rbp), %xmm0
	vcvtps2pd	%xmm0, %xmm2
	vmovhlps	%xmm0, %xmm1, %xmm1
	vcvtps2pd	%xmm3, %xmm0
	vshufps	$221, %xmm8, %xmm6, %xmm3
	vshufps	$136, %xmm8, %xmm6, %xmm6
	vsubps	%xmm6, %xmm3, %xmm8
	vmulpd	%xmm13, %xmm0, %xmm14
	vmovapd	.LC48(%rip), %xmm6
	vcvtps2pd	%xmm1, %xmm1
	vmulpd	%xmm6, %xmm15, %xmm0
	vmulpd	%xmm6, %xmm5, %xmm6
	vaddpd	%xmm2, %xmm0, %xmm0
	vaddpd	%xmm1, %xmm6, %xmm6
	vaddpd	%xmm14, %xmm0, %xmm0
	vaddpd	%xmm4, %xmm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm0, %ymm3
	vmulpd	%xmm13, %xmm15, %xmm0
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%xmm0, %xmm2, %xmm2
	vsubpd	%xmm14, %xmm2, %xmm0
	vmulpd	%xmm13, %xmm5, %xmm2
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-64(%rbp), %xmm5, %xmm5
	vaddpd	%xmm2, %xmm1, %xmm2
	vmovss	44(%rdi), %xmm1
	vsubpd	%xmm4, %xmm2, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-56(%rbp), %xmm4, %xmm4
	vmulsd	%xmm12, %xmm4, %xmm6
	vinsertf128	$0x1, %xmm2, %ymm0, %ymm2
	vcvtpd2psy	%ymm2, %xmm2
	vsubps	%xmm2, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-60(%rbp), %xmm0, %xmm0
	vsubss	40(%rdi), %xmm1, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vsubps	%xmm3, %xmm8, %xmm8
	vmulps	.LC49(%rip), %xmm8, %xmm8
	vmovsd	.LC40(%rip), %xmm3
	vmovups	%xmm8, -44(%rbp)
	vmulsd	%xmm3, %xmm5, %xmm8
	vmulsd	%xmm3, %xmm0, %xmm3
	vaddsd	%xmm0, %xmm8, %xmm8
	vaddsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm1, %xmm1
	vmulsd	%xmm12, %xmm5, %xmm8
	vaddsd	%xmm8, %xmm0, %xmm5
	vmulsd	%xmm12, %xmm0, %xmm0
	vsubsd	%xmm6, %xmm5, %xmm6
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm4, %xmm8
	vcvtsd2ss	%xmm6, %xmm5, %xmm5
	vsubss	%xmm5, %xmm1, %xmm5
	vmovss	52(%rdi), %xmm1
	vsubss	%xmm5, %xmm2, %xmm5
	vaddsd	%xmm4, %xmm3, %xmm2
	vmulss	%xmm7, %xmm5, %xmm5
	vmovss	%xmm5, -28(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-52(%rbp), %xmm5, %xmm5
	vmulsd	%xmm12, %xmm5, %xmm6
	vsubss	48(%rdi), %xmm1, %xmm1
	vmulsd	%xmm10, %xmm5, %xmm10
	movq	-80(%rbp), %rax
	movq	-48(%rbp), %rdx
	vaddsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm6, %xmm8, %xmm6
	movq	%rax, (%rdi)
	movq	-72(%rbp), %rax
	movq	%rdx, 32(%rdi)
	movq	-40(%rbp), %rdx
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	movq	%rax, 8(%rdi)
	movq	-64(%rbp), %rax
	movq	%rdx, 40(%rdi)
	movq	-32(%rbp), %rdx
	vsubss	%xmm2, %xmm1, %xmm1
	movq	%rax, 16(%rdi)
	movq	-56(%rbp), %rax
	vmulss	%xmm7, %xmm1, %xmm1
	movq	%rdx, 48(%rdi)
	movq	%rax, 24(%rdi)
	vmovss	%xmm1, -24(%rbp)
	vmulsd	%xmm11, %xmm4, %xmm1
	vmovss	60(%rdi), %xmm4
	vsubss	56(%rdi), %xmm4, %xmm4
	vsubsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm1, %xmm3
	vmulsd	%xmm9, %xmm5, %xmm1
	vaddsd	%xmm10, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm3, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm1
	vsubss	%xmm1, %xmm4, %xmm1
	vmulss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm1, -20(%rbp)
	movq	-24(%rbp), %rdx
	movq	%rdx, 56(%rdi)
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L595:
	.cfi_restore_state
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rdi), %xmm0, %xmm0
	vcvtss2sd	8(%rdi), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm8
	leaq	32(%rdi), %rax
	vmovsd	.LC21(%rip), %xmm9
	vmulsd	%xmm8, %xmm0, %xmm0
	vmovups	8(%rdi), %xmm10
	vmulsd	%xmm9, %xmm1, %xmm1
	vmovups	(%rdi), %xmm5
	vmovsd	.LC22(%rip), %xmm2
	vmovapd	.LC45(%rip), %xmm4
	vmovapd	.LC44(%rip), %xmm6
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rdi), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	24(%rdi), %xmm0, %xmm0
	vmulsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	4(%rdi), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovups	16(%rdi), %xmm1
	vshufps	$136, 32(%rdi), %xmm1, %xmm13
	vshufps	$136, %xmm1, %xmm5, %xmm5
	vcvtps2pd	%xmm5, %xmm11
	vxorps	%xmm1, %xmm1, %xmm1
	vmovhlps	%xmm5, %xmm1, %xmm1
	vmovss	%xmm0, -80(%rbp)
	vmovups	24(%rdi), %xmm0
	vshufps	$136, 40(%rdi), %xmm0, %xmm12
	vcvtps2pd	%xmm1, %xmm1
	vmulpd	%xmm6, %xmm1, %xmm7
	vshufps	$136, %xmm0, %xmm10, %xmm14
	vshufps	$221, %xmm0, %xmm10, %xmm10
	vcvtps2pd	%xmm14, %xmm3
	vmulpd	%xmm6, %xmm11, %xmm0
	vmulpd	%xmm4, %xmm3, %xmm3
	vaddpd	%xmm3, %xmm0, %xmm11
	vcvtps2pd	%xmm13, %xmm0
	vmulpd	%xmm4, %xmm0, %xmm0
	vaddpd	%xmm0, %xmm11, %xmm3
	vcvtps2pd	%xmm12, %xmm0
	vmovapd	.LC46(%rip), %xmm11
	vmulpd	%xmm11, %xmm0, %xmm0
	vsubpd	%xmm0, %xmm3, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmovhlps	%xmm14, %xmm3, %xmm3
	vcvtps2pd	%xmm3, %xmm6
	vmulpd	%xmm4, %xmm6, %xmm6
	vxorps	%xmm3, %xmm3, %xmm3
	vmovhlps	%xmm12, %xmm3, %xmm3
	vcvtps2pd	%xmm3, %xmm3
	vmulpd	%xmm11, %xmm3, %xmm1
	vaddpd	%xmm6, %xmm7, %xmm6
	vxorps	%xmm7, %xmm7, %xmm7
	vmovhlps	%xmm13, %xmm7, %xmm7
	vcvtps2pd	%xmm7, %xmm7
	vmulpd	%xmm4, %xmm7, %xmm4
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	48(%rdi), %xmm7, %xmm7
	vaddpd	%xmm4, %xmm6, %xmm4
	vsubpd	%xmm1, %xmm4, %xmm1
	vmovsd	.LC26(%rip), %xmm4
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vsubps	%xmm0, %xmm10, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	40(%rdi), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm6
	vmulsd	%xmm4, %xmm7, %xmm4
	vmovups	%xmm0, -76(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	32(%rdi), %xmm0, %xmm0
	vmulsd	.LC27(%rip), %xmm0, %xmm3
	vaddsd	%xmm6, %xmm3, %xmm3
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	56(%rdi), %xmm6, %xmm6
	vaddsd	%xmm4, %xmm3, %xmm3
	vmulsd	%xmm2, %xmm6, %xmm4
	vmulsd	%xmm2, %xmm0, %xmm2
	vmulsd	.LC28(%rip), %xmm0, %xmm0
	vsubsd	%xmm4, %xmm3, %xmm3
	vmovss	44(%rdi), %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vmovss	%xmm3, -60(%rbp)
	vmulsd	%xmm8, %xmm1, %xmm3
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vsubsd	%xmm3, %xmm2, %xmm4
	vmulsd	%xmm9, %xmm7, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm4, %xmm3
	vmulsd	%xmm8, %xmm6, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vmovss	52(%rdi), %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vmovss	%xmm2, -56(%rbp)
	vmovsd	.LC30(%rip), %xmm2
	vmulsd	%xmm2, %xmm7, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm1
	vmulsd	%xmm2, %xmm6, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	60(%rdi), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -52(%rbp)
	vmovss	32(%rdi), %xmm0
	vmovaps	%xmm5, -48(%rbp)
	vmovss	%xmm0, -32(%rbp)
	vmovss	40(%rdi), %xmm0
	vmovss	%xmm0, -28(%rbp)
	vmovss	48(%rdi), %xmm0
	vmovss	%xmm0, -24(%rbp)
	vmovss	56(%rdi), %xmm0
	vmovss	%xmm0, -20(%rbp)
	cmpl	$2, %esi
	jne	.L593
	vmovaps	-48(%rbp), %ymm0
	vmovaps	-80(%rbp), %ymm2
	vmovapd	.LC31(%rip), %ymm5
	vcvtps2pd	%xmm0, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm2, %ymm1
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm5, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm4, %ymm3
	vcvtps2pd	%xmm0, %ymm1
	vmulpd	%ymm5, %ymm2, %ymm0
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -48(%rbp)
.L593:
	movq	-48(%rbp), %rdx
	movq	%rdx, (%rdi)
	movq	-40(%rbp), %rdx
	movq	%rdx, 8(%rdi)
	movq	-32(%rbp), %rdx
	movq	%rdx, 16(%rdi)
	movq	-24(%rbp), %rdx
	movq	%rdx, 24(%rdi)
	movq	-80(%rbp), %rdx
	movq	%rdx, 32(%rdi)
	movq	-72(%rbp), %rdx
	movq	%rdx, 8(%rax)
	movq	-64(%rbp), %rdx
	movq	%rdx, 16(%rax)
	movq	-56(%rbp), %rdx
	movq	%rdx, 24(%rax)
	popq	%r10
	.cfi_def_cfa 10, 0
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1606:
	.size	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi, .-_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,comdat
.LCOLDE50:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi,comdat
.LHOTE50:
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,comdat
.LCOLDB51:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,comdat
.LHOTB51:
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi
	.type	_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi, @function
_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi:
.LFB1607:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movl	$32, %edx
	pushq	-8(%r10)
	pushq	%rbp
	leal	-1(%rsi), %eax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r12
	.cfi_escape 0x10,0xc,0x2,0x76,0x78
	movl	%esi, %r12d
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x70,0x6
	leaq	32(%rdi), %rsi
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x68
	movq	%rdi, %rbx
	subq	$88, %rsp
	cmpl	$1, %eax
	movq	(%rdi), %rax
	movq	%rax, -112(%rbp)
	movq	8(%rdi), %rax
	movq	%rax, -104(%rbp)
	movq	16(%rdi), %rax
	movq	%rax, -96(%rbp)
	movq	24(%rdi), %rax
	leaq	-80(%rbp), %rdi
	movq	%rax, -88(%rbp)
	jbe	.L610
	call	memcpy
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	-112(%rbp), %xmm8, %xmm8
	vcvtss2sd	-108(%rbp), %xmm2, %xmm2
	vxorpd	%xmm7, %xmm7, %xmm7
	vmovsd	.LC36(%rip), %xmm4
	vmulsd	%xmm6, %xmm2, %xmm2
	vcvtss2sd	-104(%rbp), %xmm7, %xmm7
	vmovss	-80(%rbp), %xmm3
	vxorps	%xmm9, %xmm9, %xmm9
	vmovsd	.LC35(%rip), %xmm5
	vmulsd	%xmm4, %xmm8, %xmm0
	vmovsd	.LC34(%rip), %xmm1
	vmulsd	%xmm5, %xmm8, %xmm8
	vmovups	-76(%rbp), %xmm10
	vmulsd	%xmm1, %xmm7, %xmm7
	vsubsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm8, %xmm2, %xmm2
	vxorps	%xmm8, %xmm8, %xmm8
	vaddsd	%xmm7, %xmm0, %xmm0
	vsubsd	%xmm7, %xmm2, %xmm2
	vxorps	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vmovaps	-112(%rbp), %xmm3
	vmovss	%xmm0, (%rbx)
	vmovups	-108(%rbp), %xmm0
	vcvtps2pd	%xmm3, %xmm12
	vmovhlps	%xmm3, %xmm9, %xmm9
	vmovapd	.LC47(%rip), %xmm3
	vmovss	%xmm2, 4(%rbx)
	vcvtps2pd	%xmm0, %xmm11
	vcvtps2pd	%xmm9, %xmm9
	vmovhlps	%xmm0, %xmm8, %xmm8
	vmovups	-104(%rbp), %xmm2
	vcvtps2pd	%xmm8, %xmm8
	vmovhlps	%xmm2, %xmm7, %xmm7
	vcvtps2pd	%xmm2, %xmm0
	vcvtps2pd	%xmm7, %xmm7
	vmulpd	%xmm3, %xmm0, %xmm0
	vmulpd	%xmm3, %xmm7, %xmm7
	vmulpd	%xmm3, %xmm12, %xmm2
	vmulpd	%xmm3, %xmm9, %xmm3
	vaddpd	%xmm11, %xmm2, %xmm2
	vaddpd	%xmm8, %xmm3, %xmm3
	vsubpd	%xmm0, %xmm2, %xmm2
	vsubpd	%xmm7, %xmm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm2, %xmm2
	vsubps	%xmm10, %xmm2, %xmm2
	vmovapd	.LC48(%rip), %xmm3
	vmulpd	%xmm3, %xmm9, %xmm9
	vmulpd	%xmm3, %xmm12, %xmm12
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-88(%rbp), %xmm3, %xmm3
	vmulsd	%xmm6, %xmm3, %xmm6
	vaddpd	%xmm9, %xmm8, %xmm8
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-96(%rbp), %xmm9, %xmm9
	vaddpd	%xmm12, %xmm11, %xmm11
	vaddpd	%xmm8, %xmm7, %xmm7
	vmovss	-60(%rbp), %xmm8
	vaddpd	%xmm11, %xmm0, %xmm0
	vinsertf128	$0x1, %xmm7, %ymm0, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vaddps	%xmm0, %xmm10, %xmm10
	vmulsd	%xmm1, %xmm3, %xmm7
	vunpcklps	%xmm10, %xmm2, %xmm0
	vunpckhps	%xmm10, %xmm2, %xmm10
	vmovups	%xmm10, 24(%rbx)
	vmovsd	.LC40(%rip), %xmm10
	vmulsd	%xmm1, %xmm9, %xmm2
	vmovups	%xmm0, 8(%rbx)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-92(%rbp), %xmm0, %xmm0
	vmulsd	%xmm10, %xmm9, %xmm9
	vaddsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm9, %xmm0, %xmm9
	vsubsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm9, %xmm7, %xmm7
	vmovss	-56(%rbp), %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm8, %xmm2, %xmm2
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vaddss	%xmm7, %xmm8, %xmm7
	vmovss	%xmm2, 40(%rbx)
	vmovss	%xmm7, 44(%rbx)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-84(%rbp), %xmm7, %xmm7
	vmulsd	%xmm1, %xmm7, %xmm8
	vmulsd	%xmm1, %xmm0, %xmm1
	vmulsd	%xmm10, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm7, %xmm5
	vmulsd	%xmm4, %xmm7, %xmm4
	vaddsd	%xmm3, %xmm1, %xmm2
	vsubsd	%xmm6, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm9, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vmovss	%xmm2, 48(%rbx)
	vaddsd	%xmm0, %xmm3, %xmm2
	vaddsd	%xmm6, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm8, %xmm8
	vmovss	-52(%rbp), %xmm2
	vaddsd	%xmm5, %xmm0, %xmm0
	vaddss	%xmm1, %xmm2, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vaddss	%xmm8, %xmm9, %xmm8
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm2, %xmm0, %xmm0
	vmovss	%xmm1, 60(%rbx)
	vmovss	%xmm8, 52(%rbx)
	vmovss	%xmm0, 56(%rbx)
	addq	$88, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L610:
	.cfi_restore_state
	call	memcpy
	cmpl	$2, %r12d
	jne	.L601
	vmovaps	-112(%rbp), %ymm0
	vmovaps	-80(%rbp), %ymm2
	vmovapd	.LC31(%rip), %ymm5
	vcvtps2pd	%xmm0, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm2, %ymm1
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm5, %ymm1, %ymm1
	vsubpd	%ymm1, %ymm4, %ymm3
	vcvtps2pd	%xmm0, %ymm1
	vmulpd	%ymm5, %ymm2, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -112(%rbp)
.L601:
	vmovss	-112(%rbp), %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-112(%rbp), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm5
	vcvtss2sd	-108(%rbp), %xmm10, %xmm10
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-104(%rbp), %xmm7, %xmm7
	vmovsd	.LC21(%rip), %xmm6
	vmovss	%xmm0, (%rbx)
	vmovss	-108(%rbp), %xmm0
	vmovsd	.LC22(%rip), %xmm8
	vmulsd	%xmm6, %xmm10, %xmm2
	vmovsd	.LC26(%rip), %xmm9
	vmovss	%xmm0, 8(%rbx)
	vmovss	-104(%rbp), %xmm0
	vmulsd	%xmm9, %xmm10, %xmm3
	vmovss	%xmm0, 16(%rbx)
	vmovss	-100(%rbp), %xmm0
	vmulsd	%xmm9, %xmm7, %xmm4
	vmovss	%xmm0, 24(%rbx)
	vmovss	-96(%rbp), %xmm0
	vmovss	%xmm0, 32(%rbx)
	vmovss	-92(%rbp), %xmm0
	vmovss	%xmm0, 40(%rbx)
	vmovss	-88(%rbp), %xmm0
	vmovss	%xmm0, 48(%rbx)
	vmovss	-84(%rbp), %xmm0
	vmovss	%xmm0, 56(%rbx)
	vmulsd	%xmm5, %xmm1, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm7, %xmm2
	vsubsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-100(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm11
	vaddsd	%xmm11, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-80(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 4(%rbx)
	vmovsd	.LC27(%rip), %xmm0
	vmulsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm0, %xmm10, %xmm10
	vmulsd	%xmm0, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm1, %xmm3
	vaddsd	%xmm4, %xmm3, %xmm1
	vaddsd	%xmm10, %xmm4, %xmm4
	vsubsd	%xmm11, %xmm1, %xmm1
	vmulsd	%xmm9, %xmm2, %xmm11
	vmulsd	%xmm0, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-76(%rbp), %xmm1, %xmm1
	vaddsd	%xmm11, %xmm4, %xmm4
	vmovss	%xmm1, 12(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-96(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm3
	vmulsd	%xmm9, %xmm1, %xmm10
	vaddsd	%xmm11, %xmm7, %xmm7
	vmulsd	%xmm0, %xmm1, %xmm0
	vsubsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm10, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm2, %xmm2
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-72(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 20(%rbx)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-92(%rbp), %xmm4, %xmm4
	vmulsd	%xmm8, %xmm4, %xmm11
	vmulsd	%xmm9, %xmm4, %xmm10
	vsubsd	%xmm11, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm2, %xmm2
	vaddsd	%xmm10, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vaddss	-68(%rbp), %xmm7, %xmm7
	vmulsd	.LC28(%rip), %xmm1, %xmm1
	vmovss	%xmm7, 28(%rbx)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-88(%rbp), %xmm7, %xmm7
	vmulsd	%xmm8, %xmm7, %xmm11
	vmulsd	%xmm9, %xmm7, %xmm9
	vmulsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm11, %xmm2, %xmm2
	vaddsd	%xmm9, %xmm0, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-64(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 36(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-84(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm8
	vsubsd	%xmm8, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-60(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 44(%rbx)
	vmulsd	%xmm5, %xmm4, %xmm0
	vmulsd	%xmm5, %xmm2, %xmm5
	vmulsd	.LC29(%rip), %xmm4, %xmm4
	vsubsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm3, %xmm5
	vmovsd	.LC30(%rip), %xmm3
	vmulsd	%xmm3, %xmm7, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-56(%rbp), %xmm5, %xmm5
	vmovss	%xmm5, 52(%rbx)
	vsubsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-52(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 60(%rbx)
	addq	$88, %rsp
	popq	%rbx
	popq	%r10
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1607:
	.size	_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi, .-_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,comdat
.LCOLDE51:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi,comdat
.LHOTE51:
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,comdat
.LCOLDB53:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,comdat
.LHOTB53:
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	.type	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi, @function
_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi:
.LFB1673:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	leal	-1(%rsi), %eax
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r10
	.cfi_offset 10, -24
	cmpl	$1, %eax
	jbe	.L617
	vmovups	(%rdi), %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm12, %xmm12, %xmm12
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovups	16(%rdi), %xmm2
	vmovsd	.LC33(%rip), %xmm11
	vmovsd	.LC35(%rip), %xmm9
	vshufps	$136, %xmm2, %xmm0, %xmm1
	vshufps	$221, %xmm2, %xmm0, %xmm0
	vaddps	%xmm0, %xmm1, %xmm0
	vmulps	.LC49(%rip), %xmm0, %xmm0
	vmovsd	.LC36(%rip), %xmm10
	vmovss	4(%rdi), %xmm1
	vsubss	(%rdi), %xmm1, %xmm3
	vmovss	.LC37(%rip), %xmm7
	vmovaps	%xmm0, -48(%rbp)
	vcvtss2sd	-48(%rbp), %xmm4, %xmm4
	vcvtss2sd	-44(%rbp), %xmm12, %xmm12
	vmulsd	%xmm10, %xmm4, %xmm1
	vcvtss2sd	-40(%rbp), %xmm8, %xmm8
	vmulsd	%xmm11, %xmm12, %xmm6
	movq	-48(%rbp), %rax
	vmovsd	.LC34(%rip), %xmm0
	vmulsd	%xmm9, %xmm4, %xmm2
	vmulsd	%xmm0, %xmm8, %xmm5
	movq	%rax, (%rdi)
	movq	-40(%rbp), %rax
	vsubsd	%xmm6, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm5, %xmm1
	vsubsd	%xmm5, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm1, %xmm2, %xmm1
	vmovsd	.LC40(%rip), %xmm2
	vmulsd	%xmm2, %xmm4, %xmm6
	vsubss	%xmm1, %xmm3, %xmm1
	vmulsd	%xmm2, %xmm12, %xmm2
	vmulss	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm12, %xmm6
	vaddsd	%xmm6, %xmm5, %xmm6
	vmovss	%xmm1, -32(%rbp)
	vmovss	12(%rdi), %xmm1
	vsubss	8(%rdi), %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	movq	%rax, 8(%rdi)
	vcvtsd2ss	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm0, %xmm4, %xmm6
	vaddsd	%xmm6, %xmm12, %xmm4
	vsubsd	%xmm5, %xmm4, %xmm5
	vxorps	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm5, %xmm4, %xmm4
	vsubss	%xmm4, %xmm1, %xmm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-36(%rbp), %xmm5, %xmm5
	vmulsd	%xmm0, %xmm5, %xmm6
	vmovss	20(%rdi), %xmm1
	vmulsd	%xmm0, %xmm12, %xmm0
	vsubss	16(%rdi), %xmm1, %xmm1
	vsubss	%xmm4, %xmm3, %xmm4
	vmulsd	%xmm10, %xmm5, %xmm10
	vaddsd	%xmm2, %xmm8, %xmm3
	vmulss	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm3, %xmm3
	vmovss	%xmm4, -28(%rbp)
	vaddsd	%xmm0, %xmm8, %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	movq	-32(%rbp), %rax
	vsubsd	%xmm6, %xmm4, %xmm4
	movq	%rax, 16(%rdi)
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vmovss	28(%rdi), %xmm4
	vsubss	24(%rdi), %xmm4, %xmm4
	vsubss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm1, -24(%rbp)
	vmulsd	%xmm11, %xmm8, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm3
	vmulsd	%xmm9, %xmm5, %xmm1
	vaddsd	%xmm10, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm3, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm1
	vsubss	%xmm1, %xmm4, %xmm1
	vmulss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm1, -20(%rbp)
	movq	-24(%rbp), %rax
	movq	%rax, 24(%rdi)
.L611:
	popq	%r10
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L617:
	.cfi_restore_state
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rdi), %xmm0, %xmm0
	vcvtss2sd	8(%rdi), %xmm1, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	16(%rdi), %xmm7, %xmm7
	vmovsd	.LC20(%rip), %xmm8
	vxorpd	%xmm6, %xmm6, %xmm6
	vmovsd	.LC21(%rip), %xmm9
	vcvtss2sd	24(%rdi), %xmm6, %xmm6
	leaq	16(%rdi), %rax
	vmovsd	.LC22(%rip), %xmm2
	vmulsd	%xmm8, %xmm0, %xmm3
	vmulsd	%xmm9, %xmm1, %xmm5
	vmulsd	%xmm2, %xmm6, %xmm10
	vmulsd	%xmm2, %xmm0, %xmm2
	vaddsd	%xmm5, %xmm3, %xmm5
	vmulsd	%xmm8, %xmm7, %xmm3
	vsubsd	%xmm3, %xmm5, %xmm5
	vmovss	4(%rdi), %xmm3
	vaddsd	%xmm10, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm3, %xmm5
	vmovsd	.LC26(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm4
	vmovss	%xmm5, -48(%rbp)
	vmulsd	.LC27(%rip), %xmm0, %xmm5
	vmulsd	.LC28(%rip), %xmm0, %xmm0
	vaddsd	%xmm4, %xmm5, %xmm5
	vmulsd	%xmm3, %xmm7, %xmm4
	vmovss	12(%rdi), %xmm3
	vaddsd	%xmm4, %xmm5, %xmm4
	vsubsd	%xmm10, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm4
	vmulsd	%xmm8, %xmm1, %xmm3
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vmovss	%xmm4, -44(%rbp)
	vsubsd	%xmm3, %xmm2, %xmm4
	vmulsd	%xmm9, %xmm7, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm4, %xmm3
	vmulsd	%xmm8, %xmm6, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vmovss	20(%rdi), %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vmovss	%xmm2, -40(%rbp)
	vmovsd	.LC30(%rip), %xmm2
	vmulsd	%xmm2, %xmm7, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm1
	vmulsd	%xmm2, %xmm6, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	28(%rdi), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -36(%rbp)
	vmovss	(%rdi), %xmm0
	vmovss	%xmm0, -32(%rbp)
	vmovss	8(%rdi), %xmm0
	vmovss	%xmm0, -28(%rbp)
	vmovss	16(%rdi), %xmm0
	vmovss	%xmm0, -24(%rbp)
	vmovss	24(%rdi), %xmm0
	vmovss	%xmm0, -20(%rbp)
	cmpl	$2, %esi
	jne	.L615
	vmovaps	-32(%rbp), %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmovaps	-48(%rbp), %xmm5
	vmovapd	.LC52(%rip), %xmm4
	vcvtps2pd	%xmm2, %xmm0
	vmovhlps	%xmm2, %xmm1, %xmm1
	vxorps	%xmm2, %xmm2, %xmm2
	vmovhlps	%xmm5, %xmm2, %xmm2
	vcvtps2pd	%xmm5, %xmm3
	vcvtps2pd	%xmm1, %xmm1
	vmulpd	%xmm4, %xmm3, %xmm3
	vcvtps2pd	%xmm2, %xmm2
	vmulpd	%xmm4, %xmm2, %xmm2
	vaddpd	%xmm3, %xmm0, %xmm0
	vaddpd	%xmm2, %xmm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vmovaps	%xmm0, -32(%rbp)
.L615:
	movq	-32(%rbp), %rdx
	movq	%rdx, (%rdi)
	movq	-24(%rbp), %rdx
	movq	%rdx, 8(%rdi)
	movq	-48(%rbp), %rdx
	movq	%rdx, 16(%rdi)
	movq	-40(%rbp), %rdx
	movq	%rdx, 8(%rax)
	jmp	.L611
	.cfi_endproc
.LFE1673:
	.size	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi, .-_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,comdat
.LCOLDE53:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi,comdat
.LHOTE53:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,comdat
	.align 2
.LCOLDB58:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,comdat
.LHOTB58:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi
	.type	_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi, @function
_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi:
.LFB1394:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movq	%rdi, %rax
	pushq	-8(%r10)
	pushq	%rbp
	addq	$24, %rax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r15
	.cfi_escape 0x10,0xf,0x2,0x76,0x78
	movl	%ecx, %r15d
	pushq	%r14
	pushq	%r13
	.cfi_escape 0x10,0xe,0x2,0x76,0x70
	.cfi_escape 0x10,0xd,0x2,0x76,0x68
	leaq	67108888(%rdi), %r13
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x58,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x60
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x50
	movq	%rax, %rbx
	subq	$4194592, %rsp
	movq	%rdi, -4194368(%rbp)
	vmovss	%xmm0, -4194384(%rbp)
	movl	%esi, -4194380(%rbp)
	movl	%edx, -4194628(%rbp)
	movq	%rax, -4194360(%rbp)
	movq	%rax, -4194624(%rbp)
.L623:
	leaq	262144(%rbx), %r12
	movq	%rbx, %r14
	.p2align 4,,10
	.p2align 3
.L619:
	movq	%r14, %rdi
	movl	%r15d, %esi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	cmpq	%r12, %r14
	jne	.L619
	leaq	1024(%rbx), %r14
	xorl	%edi, %edi
	movq	%rbx, %rsi
	addl	$1, %edi
	movq	%r14, %r8
	cmpl	$256, %edi
	je	.L817
.L819:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L621:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$256, %eax
	jne	.L621
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$256, %edi
	jne	.L819
.L817:
	movq	%rbx, %rdi
	jmp	.L620
.L820:
	addq	$1024, %r14
.L620:
	movl	%r15d, %esi
	call	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	movq	%r14, %rdi
	cmpq	%r14, %r12
	jne	.L820
	movq	%r12, %rbx
	cmpq	%r12, %r13
	jne	.L623
	movq	-4194368(%rbp), %rax
	xorl	%r10d, %r10d
	addq	$262168, %rax
	movq	%rax, -4194376(%rbp)
	movq	%rax, %r11
.L629:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$256, %r8d
	je	.L625
.L821:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L626:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$256, %edx
	jne	.L626
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$256, %r8d
	jne	.L821
.L625:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$256, %r10d
	jne	.L629
	movq	-4194360(%rbp), %r12
.L630:
	leaq	262144(%r12), %rbx
	.p2align 4,,10
	.p2align 3
.L631:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	cmpq	%rbx, %r12
	jne	.L631
	cmpq	%r12, %r13
	jne	.L630
	movq	-4194368(%rbp), %rax
	cmpl	$2, %r15d
	movl	%r15d, -4194632(%rbp)
	vmovaps	.LC32(%rip), %ymm14
	leal	-1(%r15), %ebx
	sete	%r14b
	vmovapd	.LC38(%rip), %ymm15
	addq	$33554456, %rax
	movq	%rax, -4194616(%rbp)
	movq	-4194360(%rbp), %rax
	movq	%rax, %r12
.L649:
	movq	%r12, %r13
	movl	$128, %r15d
	.p2align 4,,10
	.p2align 3
.L639:
	cmpl	$1, %ebx
	jbe	.L822
	vmovups	32(%r13), %ymm4
	leaq	-4194352(%rbp), %rdi
	xorl	%eax, %eax
	movabsq	$4602678819172646912, %rdx
	vmovups	0(%r13), %ymm1
	leaq	-2097200(%rbp), %rsi
	vmovq	%rdx, %xmm5
	movabsq	$4608871268660281344, %rcx
	vmovsd	.LC35(%rip), %xmm13
	vmovq	%rcx, %xmm7
	addq	$4, %rsi
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	leaq	8(%rdi), %r8
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	96(%r13), %ymm4
	addq	$4, %rdi
	vmovups	64(%r13), %ymm1
	vmovsd	.LC34(%rip), %xmm8
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194352(%rbp)
	vmovss	.LC37(%rip), %xmm9
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	160(%r13), %ymm4
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovups	128(%r13), %ymm1
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	224(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194320(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	192(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	288(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194288(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	256(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	352(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194256(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	320(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	416(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194224(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	384(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	480(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194192(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	448(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194160(%rbp)
	vcvtss2sd	-4194352(%rbp), %xmm4, %xmm4
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmulsd	%xmm13, %xmm4, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194344(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm2
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194128(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194348(%rbp), %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm5
	vmovss	4(%r13), %xmm0
	vsubss	0(%r13), %xmm0, %xmm3
	vmulsd	%xmm7, %xmm4, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm0, %xmm0
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm0, %xmm3, %xmm3
	vmulss	%xmm9, %xmm3, %xmm0
	vmovss	%xmm0, -2097200(%rbp)
	.p2align 4,,10
	.p2align 3
.L638:
	vmovups	8(%r13,%rax,2), %ymm10
	vmovups	40(%r13,%rax,2), %ymm3
	vmovups	(%rdi,%rax), %ymm0
	vmovaps	-4194352(%rbp,%rax), %ymm5
	vshufps	$221, %ymm3, %ymm10, %ymm11
	vshufps	$136, %ymm3, %ymm10, %ymm3
	vperm2f128	$3, %ymm11, %ymm11, %ymm4
	vcvtps2pd	%xmm0, %ymm1
	vshufps	$68, %ymm4, %ymm11, %ymm12
	vshufps	$238, %ymm4, %ymm11, %ymm4
	vperm2f128	$3, %ymm3, %ymm3, %ymm11
	vcvtps2pd	%xmm5, %ymm7
	vinsertf128	$1, %xmm4, %ymm12, %ymm12
	vshufps	$68, %ymm11, %ymm3, %ymm4
	vshufps	$238, %ymm11, %ymm3, %ymm11
	vmulpd	.LC39(%rip), %ymm7, %ymm3
	vaddpd	%ymm1, %ymm3, %ymm3
	vmulpd	%ymm15, %ymm7, %ymm7
	vmovups	(%r8,%rax), %ymm2
	vextractf128	$0x1, %ymm5, %xmm5
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm5, %ymm5
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm2, %ymm6
	vaddpd	%ymm7, %ymm1, %ymm1
	vmulpd	%ymm15, %ymm6, %ymm6
	vextractf128	$0x1, %ymm2, %xmm2
	vaddpd	%ymm6, %ymm3, %ymm10
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm15, %ymm2, %ymm2
	vmulpd	.LC39(%rip), %ymm5, %ymm3
	vmulpd	%ymm15, %ymm5, %ymm5
	vaddpd	%ymm0, %ymm3, %ymm3
	vinsertf128	$1, %xmm11, %ymm4, %ymm11
	vaddpd	%ymm5, %ymm0, %ymm0
	vsubpd	%ymm6, %ymm1, %ymm1
	vcvtpd2psy	%ymm10, %xmm10
	vaddpd	%ymm2, %ymm3, %ymm3
	vsubpd	%ymm2, %ymm0, %ymm0
	vsubps	%ymm11, %ymm12, %ymm4
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm10, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vsubps	%ymm0, %ymm4, %ymm4
	vmulps	%ymm14, %ymm4, %ymm4
	vmovups	%ymm4, (%rsi,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L638
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194128(%rbp), %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC40(%rip), %xmm5
	vcvtss2sd	-4194120(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovss	460(%r13), %xmm1
	vsubss	456(%r13), %xmm1, %xmm7
	vmulsd	%xmm5, %xmm0, %xmm1
	vcvtss2sd	-4194124(%rbp), %xmm3, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	leaq	-4194352(%rbp), %rsi
	movq	%r13, %rdi
	vmovapd	%ymm15, -4194448(%rbp)
	vmovaps	%ymm14, -4194416(%rbp)
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmulsd	%xmm8, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm3, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmulsd	%xmm5, %xmm3, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm8, %xmm3, %xmm3
	vcvtss2sd	-4194116(%rbp), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	%xmm8, %xmm1, %xmm7
	vaddsd	%xmm6, %xmm2, %xmm6
	vaddsd	%xmm3, %xmm2, %xmm3
	vmulss	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm7, %xmm6, %xmm6
	vsubsd	%xmm7, %xmm3, %xmm3
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	468(%r13), %xmm0
	vsubss	464(%r13), %xmm0, %xmm10
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmovss	476(%r13), %xmm3
	vsubss	%xmm0, %xmm10, %xmm0
	vsubss	472(%r13), %xmm3, %xmm10
	vmulsd	%xmm5, %xmm2, %xmm3
	vmulss	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm1, %xmm3
	vmovss	%xmm0, -2096968(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194112(%rbp), %xmm0, %xmm0
	vmulsd	%xmm8, %xmm0, %xmm6
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm6, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm4, %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194108(%rbp), %xmm4, %xmm4
	vmulsd	%xmm8, %xmm4, %xmm3
	vaddsd	%xmm6, %xmm4, %xmm6
	vsubss	%xmm2, %xmm10, %xmm2
	vmulss	%xmm9, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm2, -2096964(%rbp)
	vmovss	484(%r13), %xmm2
	vsubss	480(%r13), %xmm2, %xmm10
	vmulsd	%xmm5, %xmm1, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm7, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194104(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm7
	vsubss	%xmm1, %xmm10, %xmm1
	vmulss	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm7, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm1, -2096960(%rbp)
	vmovss	492(%r13), %xmm1
	vsubss	488(%r13), %xmm1, %xmm10
	vmulsd	%xmm5, %xmm0, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm6, %xmm1, %xmm0
	vmulsd	%xmm5, %xmm4, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-4194100(%rbp), %xmm6, %xmm6
	vmulsd	%xmm8, %xmm6, %xmm8
	vaddsd	%xmm3, %xmm2, %xmm5
	vsubss	%xmm0, %xmm10, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm4
	vmulss	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm8, %xmm5, %xmm8
	vxorps	%xmm5, %xmm5, %xmm5
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	500(%r13), %xmm0
	vsubss	496(%r13), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm5, %xmm4, %xmm4
	vmovq	%rdx, %xmm5
	movl	$256, %edx
	vmulsd	%xmm5, %xmm2, %xmm2
	vmovq	%rcx, %xmm5
	vsubss	%xmm4, %xmm0, %xmm0
	vmulss	%xmm9, %xmm0, %xmm0
	vsubsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm2, %xmm2
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	508(%r13), %xmm0
	vsubss	504(%r13), %xmm0, %xmm7
	vmulsd	%xmm5, %xmm6, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm13, %xmm6, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulss	%xmm9, %xmm0, %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-2097200(%rbp), %rsi
	call	memcpy
	vmovapd	-4194448(%rbp), %ymm15
	vmovaps	-4194416(%rbp), %ymm14
.L636:
	addq	$1024, %r13
	subl	$1, %r15d
	jne	.L639
	leaq	1024(%r12), %r8
	xorl	%edi, %edi
	movq	%r12, %rsi
	addl	$1, %edi
	cmpl	$128, %edi
	je	.L744
.L823:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L641:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$128, %eax
	jne	.L641
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$128, %edi
	jne	.L823
.L744:
	vmovaps	.LC32(%rip), %ymm12
	movq	%r12, %r13
	movl	$128, %r15d
	vmovapd	.LC38(%rip), %ymm13
	vmovapd	.LC39(%rip), %ymm10
	.p2align 4,,10
	.p2align 3
.L640:
	cmpl	$1, %ebx
	jbe	.L824
	vmovups	32(%r13), %ymm4
	xorl	%eax, %eax
	movabsq	$4603804719079489536, %rcx
	movabsq	$4602678819172646912, %rdx
	vmovups	0(%r13), %ymm1
	leaq	-4194352(%rbp), %rsi
	vmovq	%rdx, %xmm5
	movabsq	$4608871268660281344, %rdi
	vmovsd	.LC34(%rip), %xmm7
	leaq	8(%rsi), %r9
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	leaq	4(%rsi), %r8
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	96(%r13), %ymm4
	vmovsd	%xmm7, -4194448(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovups	64(%r13), %ymm1
	addq	$4, %rsi
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194352(%rbp)
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vmovups	160(%r13), %ymm4
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	128(%r13), %ymm1
	vmulps	%ymm12, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vmovaps	%ymm0, -4194320(%rbp)
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vmovups	224(%r13), %ymm4
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	192(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	288(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194288(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	256(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	352(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194256(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	320(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	416(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194224(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	384(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	480(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194192(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	448(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194160(%rbp)
	vcvtss2sd	-4194352(%rbp), %xmm4, %xmm4
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194344(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovq	%rcx, %xmm7
	vmulsd	%xmm7, %xmm4, %xmm1
	vmovq	%rdi, %xmm7
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194128(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194348(%rbp), %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm5
	vmovss	4(%r13), %xmm0
	vsubss	0(%r13), %xmm0, %xmm3
	vmulsd	%xmm7, %xmm4, %xmm0
	vmovss	.LC37(%rip), %xmm7
	vmovss	%xmm7, -4194416(%rbp)
	vaddsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm0, %xmm0
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097200(%rbp)
	.p2align 4,,10
	.p2align 3
.L648:
	vmovups	(%r8,%rax), %ymm3
	vmovups	8(%r13,%rax,2), %ymm9
	vmovups	(%r9,%rax), %ymm2
	vcvtps2pd	%xmm3, %ymm1
	vextractf128	$0x1, %ymm3, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmovups	40(%r13,%rax,2), %ymm6
	vmovaps	-4194352(%rbp,%rax), %ymm4
	vcvtps2pd	%xmm2, %ymm7
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vshufps	$221, %ymm6, %ymm9, %ymm11
	vmulpd	%ymm13, %ymm2, %ymm5
	vshufps	$136, %ymm6, %ymm9, %ymm6
	vperm2f128	$3, %ymm11, %ymm11, %ymm2
	vcvtps2pd	%xmm4, %ymm8
	vshufps	$68, %ymm2, %ymm11, %ymm3
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vshufps	$238, %ymm2, %ymm11, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm11
	vmulpd	%ymm13, %ymm7, %ymm7
	vperm2f128	$3, %ymm6, %ymm6, %ymm2
	vshufps	$68, %ymm2, %ymm6, %ymm3
	vshufps	$238, %ymm2, %ymm6, %ymm2
	vmulpd	%ymm10, %ymm4, %ymm6
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddpd	%ymm0, %ymm6, %ymm6
	vmulpd	%ymm13, %ymm4, %ymm4
	vsubps	%ymm2, %ymm11, %ymm3
	vmulpd	%ymm10, %ymm8, %ymm2
	vmulpd	%ymm13, %ymm8, %ymm8
	vaddpd	%ymm1, %ymm2, %ymm2
	vaddpd	%ymm4, %ymm0, %ymm0
	vaddpd	%ymm8, %ymm1, %ymm1
	vaddpd	%ymm7, %ymm2, %ymm2
	vaddpd	%ymm5, %ymm6, %ymm6
	vsubpd	%ymm7, %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmulps	%ymm12, %ymm0, %ymm0
	vmovups	%ymm0, (%rsi,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L648
	vmovaps	%ymm14, -4194576(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194128(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	-4194448(%rbp), %xmm14
	vcvtss2sd	-4194120(%rbp), %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194124(%rbp), %xmm0, %xmm0
	vmovsd	.LC40(%rip), %xmm4
	vmovapd	%ymm15, -4194608(%rbp)
	leaq	-4194352(%rbp), %rsi
	vmulsd	%xmm14, %xmm3, %xmm7
	vmovss	-4194416(%rbp), %xmm15
	vmovapd	%ymm10, -4194544(%rbp)
	vmulsd	%xmm4, %xmm3, %xmm6
	vmovapd	%ymm13, -4194512(%rbp)
	vmovss	460(%r13), %xmm1
	vmulsd	%xmm14, %xmm2, %xmm5
	vmovaps	%ymm12, -4194480(%rbp)
	vsubss	456(%r13), %xmm1, %xmm1
	vmulsd	%xmm14, %xmm0, %xmm8
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm5, %xmm7, %xmm7
	vaddsd	%xmm5, %xmm6, %xmm6
	vaddsd	%xmm8, %xmm2, %xmm8
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm3
	vmovss	468(%r13), %xmm6
	vsubss	464(%r13), %xmm6, %xmm9
	vmulsd	%xmm4, %xmm0, %xmm6
	vsubss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm15, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm2, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovss	%xmm1, -2096972(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194116(%rbp), %xmm1, %xmm1
	vmulsd	%xmm14, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm2
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm3, %xmm8, %xmm8
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm6, %xmm7
	vaddsd	%xmm1, %xmm5, %xmm8
	vsubss	%xmm7, %xmm9, %xmm0
	vmovss	476(%r13), %xmm7
	vsubss	472(%r13), %xmm7, %xmm7
	vmulss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -2096968(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194112(%rbp), %xmm0, %xmm0
	vmulsd	%xmm14, %xmm0, %xmm6
	vaddsd	%xmm3, %xmm0, %xmm9
	vaddsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm2, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm8
	vxorps	%xmm1, %xmm1, %xmm1
	vsubss	%xmm5, %xmm7, %xmm2
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194108(%rbp), %xmm5, %xmm5
	vmovss	484(%r13), %xmm7
	vsubss	480(%r13), %xmm7, %xmm7
	vaddsd	%xmm0, %xmm8, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm0
	vmulss	%xmm15, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm5, %xmm4
	vmovss	%xmm2, -2096964(%rbp)
	vmulsd	%xmm14, %xmm5, %xmm2
	vaddsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm8, %xmm8
	vsubsd	%xmm2, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm1, %xmm1
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm1, %xmm3
	vsubss	%xmm3, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-4194104(%rbp), %xmm7, %xmm7
	vmulsd	%xmm14, %xmm7, %xmm8
	vmovss	492(%r13), %xmm3
	vsubss	488(%r13), %xmm3, %xmm3
	vmulss	%xmm15, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm0, %xmm0
	vmovss	%xmm1, -2096960(%rbp)
	vaddsd	%xmm6, %xmm5, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm7, %xmm6
	vaddsd	%xmm7, %xmm4, %xmm5
	vsubsd	%xmm8, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194100(%rbp), %xmm3, %xmm3
	vmulsd	%xmm14, %xmm3, %xmm1
	vmulss	%xmm15, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm5, %xmm5
	vsubsd	%xmm1, %xmm6, %xmm1
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	500(%r13), %xmm0
	vsubss	496(%r13), %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovq	%rdx, %xmm5
	movl	$256, %edx
	vmulsd	%xmm5, %xmm7, %xmm7
	vsubss	%xmm1, %xmm0, %xmm0
	vmovq	%rdi, %xmm1
	movq	%r13, %rdi
	vmulsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm7, %xmm2, %xmm2
	vmulss	%xmm15, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm7, %xmm4
	vmovq	%rcx, %xmm7
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm3, %xmm2
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	508(%r13), %xmm0
	vsubss	504(%r13), %xmm0, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm4, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-2097200(%rbp), %rsi
	call	memcpy
	vmovapd	-4194608(%rbp), %ymm15
	vmovaps	-4194576(%rbp), %ymm14
	vmovapd	-4194544(%rbp), %ymm10
	vmovapd	-4194512(%rbp), %ymm13
	vmovaps	-4194480(%rbp), %ymm12
.L646:
	addq	$1024, %r13
	subl	$1, %r15d
	jne	.L640
	addq	$262144, %r12
	cmpq	-4194616(%rbp), %r12
	jne	.L649
	movl	-4194632(%rbp), %r15d
	xorl	%r10d, %r10d
	movq	-4194376(%rbp), %r11
.L650:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$128, %r8d
	je	.L651
.L825:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L652:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$128, %edx
	jne	.L652
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$128, %r8d
	jne	.L825
.L651:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$128, %r10d
	jne	.L650
	movq	-4194360(%rbp), %rax
	vmovaps	.LC32(%rip), %ymm14
	vmovapd	.LC38(%rip), %ymm13
	vmovapd	.LC39(%rip), %ymm15
	vmovapd	.LC23(%rip), %ymm12
	movq	%rax, -4194576(%rbp)
.L655:
	movq	-4194576(%rbp), %r13
	movl	$128, %r12d
	.p2align 4,,10
	.p2align 3
.L662:
	cmpl	$1, %ebx
	jbe	.L826
	vmovups	32(%r13), %ymm4
	vxorpd	%xmm5, %xmm5, %xmm5
	leaq	8(%r13), %rdx
	xorl	%eax, %eax
	vmovups	0(%r13), %ymm1
	leaq	-4194352(%rbp), %r11
	movabsq	$4602678819172646912, %rsi
	movabsq	$4603804719079489536, %rcx
	vmovsd	.LC34(%rip), %xmm11
	leaq	8(%r11), %r10
	vmovq	%rsi, %xmm7
	movabsq	$4608871268660281344, %rdi
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	96(%r13), %ymm4
	vmovups	64(%r13), %ymm1
	leaq	4(%r11), %r9
	leaq	-2097200(%rbp), %r11
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vmulps	%ymm14, %ymm0, %ymm0
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vmovaps	%ymm0, -4194352(%rbp)
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vcvtss2sd	-4194352(%rbp), %xmm5, %xmm5
	leaq	4(%r11), %r8
	vmovups	160(%r13), %ymm4
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	128(%r13), %ymm1
	vmulps	%ymm14, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vmovaps	%ymm0, -4194320(%rbp)
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vmovups	224(%r13), %ymm4
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	192(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	288(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194288(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	256(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	352(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194256(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	320(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	416(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194224(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	384(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	480(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194192(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	448(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194160(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194128(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194348(%rbp), %xmm0, %xmm0
	vmulsd	%xmm7, %xmm0, %xmm4
	vmovq	%rcx, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194344(%rbp), %xmm0, %xmm0
	vmulsd	%xmm7, %xmm5, %xmm1
	vmovq	%rdi, %xmm7
	vmulsd	%xmm11, %xmm0, %xmm3
	vmovss	4(%r13), %xmm0
	vsubss	0(%r13), %xmm0, %xmm2
	vmulsd	%xmm7, %xmm5, %xmm0
	vmovss	.LC37(%rip), %xmm7
	vaddsd	%xmm4, %xmm1, %xmm1
	vmovss	%xmm7, -4194416(%rbp)
	vsubsd	%xmm4, %xmm0, %xmm0
	vsubsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm3, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097200(%rbp)
	.p2align 4,,10
	.p2align 3
.L661:
	vmovups	(%r9,%rax), %ymm3
	vmovups	(%rdx,%rax,2), %ymm9
	vmovups	32(%rdx,%rax,2), %ymm6
	vcvtps2pd	%xmm3, %ymm1
	vextractf128	$0x1, %ymm3, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmovups	(%r10,%rax), %ymm2
	vmovaps	-4194352(%rbp,%rax), %ymm4
	vshufps	$221, %ymm6, %ymm9, %ymm10
	vshufps	$136, %ymm6, %ymm9, %ymm6
	vcvtps2pd	%xmm2, %ymm7
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm13, %ymm2, %ymm5
	vcvtps2pd	%xmm4, %ymm8
	vperm2f128	$3, %ymm10, %ymm10, %ymm2
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vshufps	$68, %ymm2, %ymm10, %ymm3
	vshufps	$238, %ymm2, %ymm10, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm10
	vperm2f128	$3, %ymm6, %ymm6, %ymm2
	vshufps	$68, %ymm2, %ymm6, %ymm3
	vshufps	$238, %ymm2, %ymm6, %ymm2
	vmulpd	%ymm15, %ymm4, %ymm6
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddpd	%ymm0, %ymm6, %ymm6
	vmulpd	%ymm13, %ymm4, %ymm4
	vmulpd	%ymm13, %ymm7, %ymm7
	vsubps	%ymm2, %ymm10, %ymm3
	vmulpd	%ymm15, %ymm8, %ymm2
	vmulpd	%ymm13, %ymm8, %ymm8
	vaddpd	%ymm1, %ymm2, %ymm2
	vaddpd	%ymm4, %ymm0, %ymm0
	vaddpd	%ymm8, %ymm1, %ymm1
	vaddpd	%ymm7, %ymm2, %ymm2
	vaddpd	%ymm5, %ymm6, %ymm6
	vsubpd	%ymm7, %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmulps	%ymm14, %ymm0, %ymm0
	vmovups	%ymm0, (%r8,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L661
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194128(%rbp), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm6
	vmovsd	.LC40(%rip), %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194120(%rbp), %xmm2, %xmm2
	vmovss	460(%r13), %xmm1
	vmulsd	%xmm11, %xmm2, %xmm5
	vsubss	456(%r13), %xmm1, %xmm7
	vmulsd	%xmm4, %xmm3, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194124(%rbp), %xmm0, %xmm0
	vmovaps	%ymm14, -4194448(%rbp)
	vmovss	-4194416(%rbp), %xmm14
	vmulsd	%xmm11, %xmm0, %xmm8
	movl	$256, %edx
	vmovapd	%ymm12, -4194544(%rbp)
	vaddsd	%xmm6, %xmm0, %xmm3
	vmovapd	%ymm15, -4194512(%rbp)
	vmovapd	%ymm13, -4194480(%rbp)
	vaddsd	%xmm0, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm3, %xmm6
	vxorps	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm2, %xmm8
	vaddsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm6, %xmm3, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm3
	vmovss	468(%r13), %xmm1
	vsubss	464(%r13), %xmm1, %xmm1
	vsubss	%xmm3, %xmm7, %xmm3
	vmulsd	%xmm4, %xmm0, %xmm7
	vmulss	%xmm14, %xmm3, %xmm3
	vaddsd	%xmm7, %xmm2, %xmm7
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovss	%xmm3, -2096972(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194116(%rbp), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm6
	vaddsd	%xmm3, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm7, %xmm7
	vsubsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm7, %xmm0
	vmovss	476(%r13), %xmm8
	vsubss	472(%r13), %xmm8, %xmm8
	vsubss	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm14, %xmm0, %xmm0
	vmovss	%xmm0, -2096968(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194112(%rbp), %xmm0, %xmm0
	vmulsd	%xmm11, %xmm0, %xmm7
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm0, %xmm6
	vaddsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194108(%rbp), %xmm5, %xmm5
	vmulsd	%xmm11, %xmm5, %xmm2
	vsubss	%xmm1, %xmm8, %xmm1
	vmovss	484(%r13), %xmm8
	vsubss	480(%r13), %xmm8, %xmm8
	vmulss	%xmm14, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vsubsd	%xmm2, %xmm6, %xmm6
	vmovss	%xmm1, -2096964(%rbp)
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vmovss	492(%r13), %xmm3
	vsubss	%xmm6, %xmm1, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-4194104(%rbp), %xmm6, %xmm6
	vsubss	488(%r13), %xmm3, %xmm3
	vsubss	%xmm1, %xmm8, %xmm1
	vmulsd	%xmm11, %xmm6, %xmm8
	vmulss	%xmm14, %xmm1, %xmm1
	vmovss	%xmm1, -2096960(%rbp)
	vmulsd	%xmm4, %xmm0, %xmm1
	vaddsd	%xmm7, %xmm5, %xmm0
	vmulsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm6, %xmm5
	vaddsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm0, %xmm8
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm6, %xmm4, %xmm1
	vsubss	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194100(%rbp), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm11
	vmulss	%xmm14, %xmm0, %xmm0
	vaddsd	%xmm11, %xmm1, %xmm1
	vsubsd	%xmm11, %xmm5, %xmm11
	vxorps	%xmm5, %xmm5, %xmm5
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	500(%r13), %xmm0
	vsubss	496(%r13), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm11, %xmm5, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vmovq	%rsi, %xmm5
	vmulsd	%xmm5, %xmm6, %xmm6
	leaq	-4194352(%rbp), %rsi
	vsubss	%xmm1, %xmm0, %xmm0
	vmovq	%rdi, %xmm1
	movq	%r13, %rdi
	vmulsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm6, %xmm2, %xmm2
	vmulss	%xmm14, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm6, %xmm6
	vmovq	%rcx, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm4, %xmm3, %xmm2
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	508(%r13), %xmm0
	vsubss	504(%r13), %xmm0, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm6, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm14, %xmm0, %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-2097200(%rbp), %rsi
	call	memcpy
	vmovapd	-4194544(%rbp), %ymm12
	vmovapd	-4194512(%rbp), %ymm15
	vmovapd	-4194480(%rbp), %ymm13
	vmovaps	-4194448(%rbp), %ymm14
.L659:
	addq	$1024, %r13
	subl	$1, %r12d
	jne	.L662
	addq	$262144, -4194576(%rbp)
	movq	-4194576(%rbp), %rax
	cmpq	%rax, -4194616(%rbp)
	jne	.L655
	movq	-4194368(%rbp), %rax
	movq	-4194360(%rbp), %r9
	vmovapd	.LC38(%rip), %ymm15
	leaq	16777240(%rax), %r12
.L676:
	movq	%r9, %rax
	movl	$64, %edx
	vmovapd	.LC31(%rip), %ymm11
	jmp	.L668
.L664:
	vmovups	32(%rax), %ymm4
	addq	$1024, %rax
	vmovups	-1024(%rax), %ymm0
	vmovups	-928(%rax), %ymm5
	vshufps	$136, %ymm4, %ymm0, %ymm3
	vshufps	$221, %ymm4, %ymm0, %ymm0
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm1
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-960(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm1, %ymm2
	vmovups	-864(%rax), %ymm6
	vmulps	.LC32(%rip), %ymm2, %ymm2
	vmovaps	%ymm2, -4194352(%rbp)
	vcvtps2pd	%xmm2, %ymm10
	vshufps	$136, %ymm5, %ymm0, %ymm4
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm4, %ymm4, %ymm3
	vshufps	$68, %ymm3, %ymm4, %ymm1
	vshufps	$238, %ymm3, %ymm4, %ymm3
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm4
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vmovups	-896(%rax), %ymm0
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vaddps	%ymm3, %ymm1, %ymm1
	vmovups	-800(%rax), %ymm7
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulps	.LC32(%rip), %ymm1, %ymm1
	vshufps	$136, %ymm6, %ymm0, %ymm5
	vshufps	$221, %ymm6, %ymm0, %ymm0
	vperm2f128	$3, %ymm5, %ymm5, %ymm4
	vshufps	$68, %ymm4, %ymm5, %ymm3
	vshufps	$238, %ymm4, %ymm5, %ymm4
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vshufps	$68, %ymm4, %ymm0, %ymm5
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vinsertf128	$1, %xmm4, %ymm5, %ymm0
	vmovups	-832(%rax), %ymm4
	vaddps	%ymm0, %ymm3, %ymm0
	vmovaps	%ymm1, -4194320(%rbp)
	vmovups	-984(%rax), %ymm13
	vmulps	.LC32(%rip), %ymm0, %ymm0
	vshufps	$136, %ymm7, %ymm4, %ymm6
	vshufps	$221, %ymm7, %ymm4, %ymm4
	vperm2f128	$3, %ymm6, %ymm6, %ymm5
	vxorpd	%xmm7, %xmm7, %xmm7
	vshufps	$68, %ymm5, %ymm6, %ymm3
	vcvtss2sd	-4194352(%rbp), %xmm7, %xmm7
	vshufps	$238, %ymm5, %ymm6, %ymm5
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vshufps	$68, %ymm5, %ymm4, %ymm6
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vmulsd	.LC35(%rip), %xmm7, %xmm4
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vaddps	%ymm5, %ymm3, %ymm3
	vmulps	.LC32(%rip), %ymm3, %ymm3
	vmovaps	%ymm3, -4194256(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194348(%rbp), %xmm3, %xmm3
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	.LC33(%rip), %xmm3, %xmm8
	vmovss	-1020(%rax), %xmm3
	vcvtss2sd	-4194344(%rbp), %xmm5, %xmm5
	vmovaps	%ymm0, -4194288(%rbp)
	vsubss	-1024(%rax), %xmm3, %xmm6
	vmulsd	.LC36(%rip), %xmm7, %xmm3
	vmulsd	.LC34(%rip), %xmm5, %xmm5
	vmovss	.LC37(%rip), %xmm7
	vaddsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm8, %xmm3, %xmm3
	vmovups	-1016(%rax), %ymm8
	vsubsd	%xmm5, %xmm4, %xmm4
	vshufps	$221, %ymm13, %ymm8, %ymm12
	vshufps	$136, %ymm13, %ymm8, %ymm13
	vperm2f128	$3, %ymm13, %ymm13, %ymm8
	vaddsd	%xmm3, %xmm5, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vperm2f128	$3, %ymm12, %ymm12, %ymm4
	vshufps	$68, %ymm4, %ymm12, %ymm14
	vshufps	$238, %ymm4, %ymm12, %ymm4
	vshufps	$68, %ymm8, %ymm13, %ymm12
	vinsertf128	$1, %xmm4, %ymm14, %ymm4
	vshufps	$238, %ymm8, %ymm13, %ymm8
	vinsertf128	$1, %xmm8, %ymm12, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vmulpd	.LC39(%rip), %ymm10, %ymm12
	vmulpd	%ymm15, %ymm10, %ymm10
	vmulpd	.LC39(%rip), %ymm2, %ymm4
	vsubss	%xmm3, %xmm6, %xmm3
	vmulpd	%ymm15, %ymm2, %ymm2
	vmovups	-4194344(%rbp), %ymm6
	vcvtps2pd	%xmm6, %ymm9
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm15, %ymm9, %ymm9
	vcvtps2pd	%xmm6, %ymm6
	vmulss	%xmm7, %xmm3, %xmm3
	vmulpd	%ymm15, %ymm6, %ymm6
	vmovss	%xmm3, -2097200(%rbp)
	vmovups	-4194348(%rbp), %ymm3
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm5, %ymm12, %ymm12
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm10, %ymm5, %ymm5
	vaddpd	%ymm3, %ymm4, %ymm4
	vaddpd	%ymm2, %ymm3, %ymm3
	vaddpd	%ymm9, %ymm12, %ymm12
	vaddpd	%ymm6, %ymm4, %ymm4
	vsubpd	%ymm9, %ymm5, %ymm9
	vsubpd	%ymm6, %ymm3, %ymm3
	vcvtpd2psy	%ymm12, %xmm12
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm12, %ymm4
	vcvtpd2psy	%ymm9, %xmm2
	vcvtps2pd	%xmm1, %ymm9
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm6
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vsubps	%ymm2, %ymm4, %ymm2
	vsubps	%ymm2, %ymm8, %ymm2
	vmulps	.LC32(%rip), %ymm2, %ymm2
	vmovups	-920(%rax), %ymm10
	vmovups	%ymm2, -2097196(%rbp)
	vmovups	-952(%rax), %ymm4
	vmovups	-4194316(%rbp), %ymm5
	vshufps	$221, %ymm10, %ymm4, %ymm12
	vshufps	$136, %ymm10, %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm10
	vmovups	-4194312(%rbp), %ymm3
	vcvtps2pd	%xmm5, %ymm2
	vextractf128	$0x1, %ymm5, %xmm1
	vperm2f128	$3, %ymm12, %ymm12, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vshufps	$68, %ymm5, %ymm12, %ymm13
	vshufps	$238, %ymm5, %ymm12, %ymm5
	vshufps	$68, %ymm10, %ymm4, %ymm12
	vinsertf128	$1, %xmm5, %ymm13, %ymm5
	vshufps	$238, %ymm10, %ymm4, %ymm10
	vinsertf128	$1, %xmm10, %ymm12, %ymm10
	vsubps	%ymm10, %ymm5, %ymm5
	vmulpd	.LC39(%rip), %ymm6, %ymm4
	vmulpd	%ymm15, %ymm6, %ymm6
	vaddpd	%ymm1, %ymm4, %ymm4
	vcvtps2pd	%xmm3, %ymm8
	vmulpd	%ymm15, %ymm8, %ymm8
	vmulpd	.LC39(%rip), %ymm9, %ymm10
	vaddpd	%ymm6, %ymm1, %ymm1
	vmulpd	%ymm15, %ymm9, %ymm9
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm2, %ymm10, %ymm10
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm15, %ymm3, %ymm3
	vmovups	-4194284(%rbp), %ymm6
	vaddpd	%ymm9, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm9
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm3, %ymm4, %ymm4
	vaddpd	%ymm8, %ymm10, %ymm10
	vsubpd	%ymm8, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm1, %ymm3
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm4, %ymm10, %ymm4
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm3, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vsubps	%ymm1, %ymm4, %ymm1
	vmovups	-888(%rax), %ymm3
	vcvtps2pd	%xmm6, %ymm4
	vmovups	-4194280(%rbp), %ymm2
	vsubps	%ymm1, %ymm5, %ymm1
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm6, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulps	.LC32(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm2, %ymm8
	vmovups	%ymm1, -2097164(%rbp)
	vmovups	-856(%rax), %ymm1
	vmulpd	%ymm15, %ymm8, %ymm8
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm15, %ymm2, %ymm2
	vshufps	$221, %ymm1, %ymm3, %ymm10
	vshufps	$136, %ymm1, %ymm3, %ymm1
	vperm2f128	$3, %ymm10, %ymm10, %ymm6
	vperm2f128	$3, %ymm1, %ymm1, %ymm3
	vshufps	$68, %ymm6, %ymm10, %ymm12
	vshufps	$238, %ymm6, %ymm10, %ymm6
	vshufps	$68, %ymm3, %ymm1, %ymm10
	vinsertf128	$1, %xmm6, %ymm12, %ymm6
	vshufps	$238, %ymm3, %ymm1, %ymm3
	vinsertf128	$1, %xmm3, %ymm10, %ymm3
	vsubps	%ymm3, %ymm6, %ymm6
	vmulpd	.LC39(%rip), %ymm9, %ymm1
	vmulpd	.LC39(%rip), %ymm5, %ymm3
	vaddpd	%ymm4, %ymm1, %ymm1
	vmulpd	%ymm15, %ymm5, %ymm5
	vaddpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm5, %ymm0, %ymm0
	vmovsd	.LC40(%rip), %xmm5
	vaddpd	%ymm8, %ymm1, %ymm1
	vaddpd	%ymm2, %ymm3, %ymm3
	vsubpd	%ymm2, %ymm0, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm1, %ymm3
	vmulpd	%ymm15, %ymm9, %ymm1
	vaddpd	%ymm1, %ymm4, %ymm1
	vsubpd	%ymm8, %ymm1, %ymm1
	vcvtpd2psy	%ymm1, %xmm0
	vcvtpd2psy	%ymm2, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vsubps	%ymm0, %ymm3, %ymm0
	vcvtss2sd	-4194256(%rbp), %xmm1, %xmm1
	vmovss	-820(%rax), %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubss	-824(%rax), %xmm3, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm3
	vcvtss2sd	-4194248(%rbp), %xmm2, %xmm2
	vmulsd	.LC34(%rip), %xmm2, %xmm4
	vsubps	%ymm0, %ymm6, %ymm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmulps	.LC32(%rip), %ymm0, %ymm0
	vmovups	%ymm0, -2097132(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194252(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm6, %xmm6
	vmulsd	.LC34(%rip), %xmm1, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm1
	vsubsd	%xmm4, %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm1, %xmm6, %xmm1
	vmovss	-812(%rax), %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	vsubss	-816(%rax), %xmm3, %xmm9
	vmulsd	%xmm5, %xmm0, %xmm3
	vsubss	%xmm1, %xmm8, %xmm1
	vmulss	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vmovss	%xmm1, -2097100(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194244(%rbp), %xmm1, %xmm1
	vmulsd	.LC34(%rip), %xmm1, %xmm8
	vaddsd	%xmm4, %xmm1, %xmm4
	vaddsd	%xmm8, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm6, %xmm6
	vmulsd	.LC34(%rip), %xmm0, %xmm3
	vaddsd	%xmm2, %xmm3, %xmm0
	vsubsd	%xmm8, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vsubss	%xmm0, %xmm9, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194240(%rbp), %xmm0, %xmm0
	vmovss	-804(%rax), %xmm3
	vmulsd	.LC34(%rip), %xmm0, %xmm6
	vaddsd	%xmm8, %xmm0, %xmm8
	vsubss	-808(%rax), %xmm3, %xmm9
	vmulsd	%xmm5, %xmm2, %xmm3
	movq	-4194352(%rbp), %rcx
	vsubsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm1, %xmm3, %xmm3
	movq	%rcx, -1024(%rax)
	movq	-4194344(%rbp), %rcx
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm3, %xmm3
	movq	%rcx, -1016(%rax)
	movq	-4194336(%rbp), %rcx
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194236(%rbp), %xmm4, %xmm4
	vmulsd	.LC34(%rip), %xmm4, %xmm3
	vaddsd	%xmm6, %xmm4, %xmm6
	movq	%rcx, -1008(%rax)
	movq	-4194328(%rbp), %rcx
	vsubss	%xmm2, %xmm9, %xmm2
	vmulss	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vmovss	%xmm2, -2097092(%rbp)
	vmovss	-796(%rax), %xmm2
	vsubss	-800(%rax), %xmm2, %xmm9
	vmulsd	%xmm5, %xmm1, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm8, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194232(%rbp), %xmm2, %xmm2
	vmulsd	.LC34(%rip), %xmm2, %xmm8
	vsubss	%xmm1, %xmm9, %xmm1
	vmulss	%xmm7, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm1, -2097088(%rbp)
	vmovss	-788(%rax), %xmm1
	vsubss	-792(%rax), %xmm1, %xmm9
	vmulsd	%xmm5, %xmm0, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm1, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-4194228(%rbp), %xmm8, %xmm8
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm6, %xmm1, %xmm0
	vmulsd	%xmm5, %xmm4, %xmm1
	vmulsd	.LC34(%rip), %xmm8, %xmm6
	vaddsd	%xmm3, %xmm2, %xmm5
	vsubss	%xmm0, %xmm9, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vmulss	%xmm7, %xmm0, %xmm0
	vsubsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm6, %xmm4, %xmm4
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	-780(%rax), %xmm0
	vsubsd	%xmm2, %xmm3, %xmm3
	vsubss	-784(%rax), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm2
	vsubss	%xmm4, %xmm0, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	-772(%rax), %xmm0
	vsubss	-776(%rax), %xmm0, %xmm6
	vmulsd	.LC36(%rip), %xmm8, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm8, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	movq	%rcx, -1000(%rax)
	movq	-4194320(%rbp), %rcx
	movq	-2097200(%rbp), %rsi
	movq	%rcx, -992(%rax)
	movq	-4194312(%rbp), %rcx
	movq	%rsi, -896(%rax)
	movq	-2097192(%rbp), %rsi
	movq	%rcx, -984(%rax)
	movq	-4194304(%rbp), %rcx
	movq	%rsi, -888(%rax)
	movq	-2097184(%rbp), %rsi
	movq	%rcx, -976(%rax)
	movq	-4194296(%rbp), %rcx
	movq	%rsi, -880(%rax)
	movq	-2097176(%rbp), %rsi
	movq	%rcx, -968(%rax)
	movq	-4194288(%rbp), %rcx
	movq	%rsi, -872(%rax)
	movq	-2097168(%rbp), %rsi
	movq	%rcx, -960(%rax)
	movq	-4194280(%rbp), %rcx
	movq	%rcx, -952(%rax)
	movq	-4194272(%rbp), %rcx
	movq	%rcx, -944(%rax)
	movq	-4194264(%rbp), %rcx
	movq	%rcx, -936(%rax)
	movq	-4194256(%rbp), %rcx
	movq	%rcx, -928(%rax)
	movq	-4194248(%rbp), %rcx
	movq	%rcx, -920(%rax)
	movq	-4194240(%rbp), %rcx
	movq	%rcx, -912(%rax)
	movq	-4194232(%rbp), %rcx
	movq	%rcx, -904(%rax)
	movq	%rsi, -864(%rax)
	movq	-2097160(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-2097152(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-2097144(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-2097136(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-2097128(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-2097120(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-2097112(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-2097104(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-2097096(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-2097088(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-2097080(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %edx
	je	.L827
.L668:
	cmpl	$1, %ebx
	ja	.L664
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rax), %xmm0, %xmm0
	vcvtss2sd	8(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm0, %xmm0
	vmulsd	.LC21(%rip), %xmm1, %xmm1
	vmovups	40(%rax), %ymm5
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
	vmovups	8(%rax), %ymm0
	vshufps	$136, %ymm5, %ymm0, %ymm2
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm3
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vmovups	24(%rax), %ymm2
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 56(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm4
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovups	16(%rax), %ymm4
	vshufps	$136, 48(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm6
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vmovups	(%rax), %ymm4
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vshufps	$136, 32(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm12
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vinsertf128	$1, %xmm6, %ymm12, %ymm12
	vshufps	$68, %ymm4, %ymm0, %ymm5
	vcvtps2pd	%xmm12, %ymm7
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vmulpd	.LC23(%rip), %ymm7, %ymm0
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm12, %xmm8
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm8, %ymm8
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vmulpd	.LC23(%rip), %ymm8, %ymm5
	vaddpd	%ymm3, %ymm5, %ymm3
	vmovups	104(%rax), %ymm5
	vcvtpd2psy	%ymm0, %xmm0
	vaddpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm1, %ymm2, %ymm1
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm1
	vsubps	%ymm1, %ymm4, %ymm1
	vmovups	%ymm1, -4194348(%rbp)
	vmovups	72(%rax), %ymm1
	vshufps	$136, %ymm5, %ymm1, %ymm2
	vshufps	$221, %ymm5, %ymm1, %ymm1
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm3
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovups	88(%rax), %ymm3
	vshufps	$136, 120(%rax), %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm4
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vmovups	80(%rax), %ymm4
	vshufps	$136, 112(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm3
	vshufps	$68, %ymm3, %ymm4, %ymm6
	vshufps	$238, %ymm3, %ymm4, %ymm3
	vmovups	64(%rax), %ymm4
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	vshufps	$136, 96(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm10
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm1, %ymm1, %ymm4
	vinsertf128	$1, %xmm6, %ymm10, %ymm10
	vshufps	$68, %ymm4, %ymm1, %ymm5
	vcvtps2pd	%xmm10, %ymm6
	vshufps	$238, %ymm4, %ymm1, %ymm4
	vcvtps2pd	%xmm0, %ymm1
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vmulpd	.LC23(%rip), %ymm6, %ymm5
	vaddpd	%ymm1, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmovups	168(%rax), %ymm13
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtps2pd	%xmm2, %ymm5
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vsubpd	%ymm5, %ymm1, %ymm1
	vextractf128	$0x1, %ymm10, %xmm5
	vcvtps2pd	%xmm5, %ymm14
	vmulpd	.LC23(%rip), %ymm14, %ymm5
	vaddpd	%ymm0, %ymm5, %ymm0
	vcvtpd2psy	%ymm1, %xmm1
	vaddpd	%ymm3, %ymm0, %ymm3
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm0, %ymm3, %ymm0
	vmovups	136(%rax), %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm4, %ymm0
	vshufps	$136, %ymm13, %ymm3, %ymm1
	vshufps	$221, %ymm13, %ymm3, %ymm3
	vmovups	%ymm0, -4194316(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	152(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	vshufps	$136, 184(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm4
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmovups	144(%rax), %ymm4
	vshufps	$136, 176(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm1
	vshufps	$68, %ymm1, %ymm4, %ymm5
	vshufps	$238, %ymm1, %ymm4, %ymm1
	vmovups	128(%rax), %ymm4
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vshufps	$136, 160(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vshufps	$68, %ymm5, %ymm4, %ymm9
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vperm2f128	$3, %ymm3, %ymm3, %ymm4
	vinsertf128	$1, %xmm5, %ymm9, %ymm9
	vshufps	$68, %ymm4, %ymm3, %ymm13
	vcvtps2pd	%xmm9, %ymm5
	vshufps	$238, %ymm4, %ymm3, %ymm4
	vmovapd	%ymm5, -4194512(%rbp)
	vcvtps2pd	%xmm2, %ymm3
	vinsertf128	$1, %xmm4, %ymm13, %ymm13
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmulpd	.LC23(%rip), %ymm5, %ymm5
	vaddpd	%ymm3, %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm3, %ymm5, %ymm3
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm5, %ymm3, %ymm3
	vextractf128	$0x1, %ymm9, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	.LC23(%rip), %ymm5, %ymm4
	vaddpd	%ymm4, %ymm2, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm1, %ymm2, %ymm2
	vmovsd	.LC26(%rip), %xmm1
	vsubpd	%ymm0, %ymm2, %ymm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vsubps	%ymm2, %ymm13, %ymm13
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	200(%rax), %xmm3, %xmm3
	vmovups	%ymm13, -4194284(%rbp)
	vmovapd	%xmm3, %xmm13
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	208(%rax), %xmm3, %xmm3
	vmulsd	%xmm3, %xmm1, %xmm2
	vmovsd	%xmm3, -4194480(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	216(%rax), %xmm3, %xmm3
	vmovsd	%xmm13, -4194416(%rbp)
	vcvtss2sd	192(%rax), %xmm0, %xmm0
	vmulsd	%xmm13, %xmm1, %xmm13
	vmovsd	.LC27(%rip), %xmm4
	vmulsd	.LC27(%rip), %xmm0, %xmm0
	vaddsd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm3, %xmm13
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm13, %xmm0, %xmm0
	vmovss	204(%rax), %xmm13
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm13, %xmm0
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	224(%rax), %xmm13, %xmm13
	vmovss	%xmm0, -4194252(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm0
	vmovsd	%xmm0, -4194448(%rbp)
	vmulsd	-4194416(%rbp), %xmm4, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm13, %xmm2
	vaddsd	-4194448(%rbp), %xmm0, %xmm0
	vsubsd	%xmm2, %xmm0, %xmm0
	vmovsd	%xmm2, -4194544(%rbp)
	vmovss	212(%rax), %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm13, %xmm2
	vmovss	%xmm0, -4194248(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	232(%rax), %xmm0, %xmm0
	vmovsd	%xmm0, -4194416(%rbp)
	vmulsd	-4194480(%rbp), %xmm4, %xmm0
	vaddsd	-4194448(%rbp), %xmm0, %xmm0
	vmovsd	.LC22(%rip), %xmm4
	vmulsd	-4194416(%rbp), %xmm4, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm0
	vmovss	220(%rax), %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, -4194244(%rbp)
	vmulsd	.LC27(%rip), %xmm3, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	240(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm4
	vaddsd	%xmm0, %xmm2, %xmm0
	vmulsd	-4194416(%rbp), %xmm1, %xmm2
	vmulsd	%xmm1, %xmm3, %xmm1
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm0
	vmovss	228(%rax), %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, -4194240(%rbp)
	vmulsd	.LC27(%rip), %xmm13, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	248(%rax), %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm2, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	236(%rax), %xmm1
	vmovaps	%ymm12, -2097200(%rbp)
	vmovsd	.LC20(%rip), %xmm4
	vmovaps	%ymm10, -2097168(%rbp)
	vmulsd	.LC28(%rip), %xmm13, %xmm13
	vmovaps	%ymm9, -2097136(%rbp)
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	-4194544(%rbp), %xmm1
	vmovss	%xmm0, -4194236(%rbp)
	vmulsd	-4194416(%rbp), %xmm4, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC21(%rip), %xmm3, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm2, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	244(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vmovss	%xmm0, -4194232(%rbp)
	vmovsd	-4194416(%rbp), %xmm0
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm13, %xmm13
	vmulsd	%xmm1, %xmm3, %xmm0
	vmulsd	%xmm1, %xmm2, %xmm1
	vsubsd	%xmm0, %xmm13, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	252(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194228(%rbp)
	vmovss	192(%rax), %xmm0
	vmovss	%xmm0, -2097104(%rbp)
	vmovss	200(%rax), %xmm0
	vmovss	%xmm0, -2097100(%rbp)
	vmovss	208(%rax), %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vmovss	216(%rax), %xmm0
	vmovss	%xmm0, -2097092(%rbp)
	vmovss	224(%rax), %xmm0
	vmovss	%xmm0, -2097088(%rbp)
	vmovss	232(%rax), %xmm0
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	240(%rax), %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	248(%rax), %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	testb	%r14b, %r14b
	je	.L667
	vmovaps	-4194352(%rbp), %ymm1
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm7, %ymm0
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm8, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194320(%rbp), %ymm1
	vmovaps	%ymm0, -2097200(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm6, %ymm0
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm14, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194288(%rbp), %ymm1
	vmovaps	%ymm0, -2097168(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	-4194512(%rbp), %ymm0, %ymm0
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194256(%rbp), %ymm1
	vmovaps	%ymm0, -2097136(%rbp)
	vmovaps	-2097104(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm3, %ymm3
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2097104(%rbp)
.L667:
	movq	-2097200(%rbp), %rcx
	addq	$1024, %rax
	movq	-4194352(%rbp), %rsi
	movq	%rcx, -1024(%rax)
	movq	-2097192(%rbp), %rcx
	movq	%rcx, -1016(%rax)
	movq	-2097184(%rbp), %rcx
	movq	%rcx, -1008(%rax)
	movq	-2097176(%rbp), %rcx
	movq	%rcx, -1000(%rax)
	movq	-2097168(%rbp), %rcx
	movq	%rcx, -992(%rax)
	movq	-2097160(%rbp), %rcx
	movq	%rcx, -984(%rax)
	movq	-2097152(%rbp), %rcx
	movq	%rcx, -976(%rax)
	movq	-2097144(%rbp), %rcx
	movq	%rcx, -968(%rax)
	movq	-2097136(%rbp), %rcx
	movq	%rcx, -960(%rax)
	movq	-2097128(%rbp), %rcx
	movq	%rcx, -952(%rax)
	movq	-2097120(%rbp), %rcx
	movq	%rcx, -944(%rax)
	movq	-2097112(%rbp), %rcx
	movq	%rcx, -936(%rax)
	movq	-2097104(%rbp), %rcx
	movq	%rcx, -928(%rax)
	movq	-2097096(%rbp), %rcx
	movq	%rcx, -920(%rax)
	movq	-2097088(%rbp), %rcx
	movq	%rcx, -912(%rax)
	movq	-2097080(%rbp), %rcx
	movq	%rcx, -904(%rax)
	movq	%rsi, -896(%rax)
	movq	-4194344(%rbp), %rsi
	movq	%rsi, -888(%rax)
	movq	-4194336(%rbp), %rsi
	movq	%rsi, -880(%rax)
	movq	-4194328(%rbp), %rsi
	movq	%rsi, -872(%rax)
	movq	-4194320(%rbp), %rsi
	movq	%rsi, -864(%rax)
	movq	-4194312(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-4194304(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-4194296(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-4194288(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-4194280(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-4194272(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-4194264(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-4194256(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-4194248(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-4194240(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-4194232(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %edx
	jne	.L668
.L827:
	leaq	1024(%r9), %r8
	xorl	%edi, %edi
	movq	%r9, %rsi
	addl	$1, %edi
	cmpl	$64, %edi
	je	.L745
.L828:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L670:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$64, %eax
	jne	.L670
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$64, %edi
	jne	.L828
.L745:
	movq	%r9, %rax
	movl	$64, %ecx
	vmovaps	.LC32(%rip), %ymm11
	vmovapd	.LC38(%rip), %ymm10
	jmp	.L669
	.p2align 4,,10
	.p2align 3
.L672:
	vmovups	32(%rax), %ymm3
	addq	$1024, %rax
	vmovups	-1024(%rax), %ymm0
	vmovups	-928(%rax), %ymm5
	vshufps	$136, %ymm3, %ymm0, %ymm2
	vshufps	$221, %ymm3, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm4
	vshufps	$68, %ymm4, %ymm2, %ymm1
	vshufps	$238, %ymm4, %ymm2, %ymm4
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vshufps	$68, %ymm2, %ymm0, %ymm4
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-960(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm4, %ymm4
	vaddps	%ymm4, %ymm1, %ymm4
	vmovups	-864(%rax), %ymm6
	vshufps	$136, %ymm5, %ymm0, %ymm3
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm1
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-896(%rax), %ymm0
	vmulps	%ymm11, %ymm4, %ymm4
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovups	-800(%rax), %ymm7
	vaddps	%ymm2, %ymm1, %ymm1
	vcvtps2pd	%xmm4, %ymm9
	vmovaps	%ymm4, -4194352(%rbp)
	vshufps	$136, %ymm6, %ymm0, %ymm5
	vshufps	$221, %ymm6, %ymm0, %ymm0
	vperm2f128	$3, %ymm5, %ymm5, %ymm3
	vshufps	$68, %ymm3, %ymm5, %ymm2
	vshufps	$238, %ymm3, %ymm5, %ymm3
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm5
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm5, %ymm0
	vaddps	%ymm0, %ymm2, %ymm0
	vmulps	%ymm11, %ymm1, %ymm1
	vmovups	-832(%rax), %ymm3
	vmovaps	%ymm1, -4194320(%rbp)
	vmovups	-984(%rax), %ymm13
	vshufps	$136, %ymm7, %ymm3, %ymm6
	vshufps	$221, %ymm7, %ymm3, %ymm3
	vperm2f128	$3, %ymm6, %ymm6, %ymm5
	vshufps	$68, %ymm5, %ymm6, %ymm2
	vshufps	$238, %ymm5, %ymm6, %ymm5
	vinsertf128	$1, %xmm5, %ymm2, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm5
	vshufps	$68, %ymm5, %ymm3, %ymm6
	vshufps	$238, %ymm5, %ymm3, %ymm5
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vaddps	%ymm5, %ymm2, %ymm2
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-4194352(%rbp), %xmm7, %xmm7
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194344(%rbp), %xmm5, %xmm5
	vmulps	%ymm11, %ymm0, %ymm0
	vmulsd	.LC35(%rip), %xmm7, %xmm3
	vmovaps	%ymm0, -4194288(%rbp)
	vmulsd	.LC34(%rip), %xmm5, %xmm5
	vmulps	%ymm11, %ymm2, %ymm2
	vmovaps	%ymm2, -4194256(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194348(%rbp), %xmm2, %xmm2
	vmulsd	.LC33(%rip), %xmm2, %xmm8
	vmovss	-1020(%rax), %xmm2
	vsubss	-1024(%rax), %xmm2, %xmm6
	vmulsd	.LC36(%rip), %xmm7, %xmm2
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm8, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm5, %xmm2
	vmovups	-4194344(%rbp), %ymm5
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtps2pd	%xmm5, %ymm8
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm10, %ymm8, %ymm8
	vcvtps2pd	%xmm5, %ymm5
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vmovups	-4194348(%rbp), %ymm3
	vmulpd	%ymm10, %ymm5, %ymm5
	vsubss	%xmm2, %xmm6, %xmm2
	vmovups	-1016(%rax), %ymm6
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vshufps	$221, %ymm13, %ymm6, %ymm12
	vshufps	$136, %ymm13, %ymm6, %ymm6
	vperm2f128	$3, %ymm12, %ymm12, %ymm7
	vperm2f128	$3, %ymm6, %ymm6, %ymm13
	vshufps	$68, %ymm7, %ymm12, %ymm14
	vshufps	$238, %ymm7, %ymm12, %ymm7
	vshufps	$68, %ymm13, %ymm6, %ymm12
	vinsertf128	$1, %xmm7, %ymm14, %ymm7
	vshufps	$238, %ymm13, %ymm6, %ymm13
	vinsertf128	$1, %xmm13, %ymm12, %ymm12
	vsubps	%ymm12, %ymm7, %ymm7
	vmulpd	.LC39(%rip), %ymm9, %ymm6
	vmovss	%xmm2, -2097200(%rbp)
	vmulpd	%ymm10, %ymm9, %ymm9
	vextractf128	$0x1, %ymm4, %xmm2
	vcvtps2pd	%xmm3, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm6, %ymm6
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	.LC39(%rip), %ymm2, %ymm12
	vaddpd	%ymm9, %ymm4, %ymm4
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm10, %ymm2, %ymm2
	vaddpd	%ymm3, %ymm12, %ymm12
	vaddpd	%ymm2, %ymm3, %ymm3
	vaddpd	%ymm8, %ymm6, %ymm6
	vaddpd	%ymm5, %ymm12, %ymm12
	vsubpd	%ymm8, %ymm4, %ymm8
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtpd2psy	%ymm6, %xmm6
	vmovups	-4194312(%rbp), %ymm5
	vcvtpd2psy	%ymm12, %xmm12
	vinsertf128	$0x1, %xmm12, %ymm6, %ymm6
	vmovups	-920(%rax), %ymm12
	vcvtpd2psy	%ymm8, %xmm2
	vcvtps2pd	%xmm1, %ymm8
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vsubps	%ymm2, %ymm6, %ymm2
	vmovups	-952(%rax), %ymm6
	vshufps	$221, %ymm12, %ymm6, %ymm9
	vshufps	$136, %ymm12, %ymm6, %ymm12
	vperm2f128	$3, %ymm12, %ymm12, %ymm6
	vsubps	%ymm2, %ymm7, %ymm7
	vmulps	%ymm11, %ymm7, %ymm2
	vmovups	%ymm2, -2097196(%rbp)
	vcvtps2pd	%xmm5, %ymm7
	vmovups	-4194316(%rbp), %ymm2
	vmulpd	%ymm10, %ymm7, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	%ymm10, %ymm5, %ymm5
	vcvtps2pd	%xmm2, %ymm3
	vextractf128	$0x1, %ymm2, %xmm1
	vperm2f128	$3, %ymm9, %ymm9, %ymm2
	vshufps	$68, %ymm2, %ymm9, %ymm13
	vshufps	$238, %ymm2, %ymm9, %ymm2
	vshufps	$68, %ymm6, %ymm12, %ymm9
	vshufps	$238, %ymm6, %ymm12, %ymm6
	vinsertf128	$1, %xmm6, %ymm9, %ymm6
	vmulpd	.LC39(%rip), %ymm8, %ymm9
	vmulpd	%ymm10, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm9, %ymm9
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm8, %ymm3, %ymm3
	vinsertf128	$1, %xmm2, %ymm13, %ymm2
	vmovups	-856(%rax), %ymm8
	vsubps	%ymm6, %ymm2, %ymm6
	vmulpd	.LC39(%rip), %ymm4, %ymm2
	vaddpd	%ymm7, %ymm9, %ymm9
	vaddpd	%ymm1, %ymm2, %ymm2
	vsubpd	%ymm7, %ymm3, %ymm7
	vmulpd	%ymm10, %ymm4, %ymm3
	vmovups	-4194284(%rbp), %ymm4
	vaddpd	%ymm3, %ymm1, %ymm3
	vcvtpd2psy	%ymm9, %xmm9
	vaddpd	%ymm5, %ymm2, %ymm2
	vcvtpd2psy	%ymm7, %xmm1
	vcvtps2pd	%xmm0, %ymm7
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm4, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm9, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm1
	vmovups	-888(%rax), %ymm3
	vmovups	-4194280(%rbp), %ymm2
	vshufps	$221, %ymm8, %ymm3, %ymm9
	vshufps	$136, %ymm8, %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm8
	vsubps	%ymm1, %ymm6, %ymm1
	vcvtps2pd	%xmm2, %ymm6
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm10, %ymm6, %ymm6
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm10, %ymm2, %ymm2
	vmulps	%ymm11, %ymm1, %ymm1
	vmovups	%ymm1, -2097164(%rbp)
	vcvtps2pd	%xmm4, %ymm1
	vperm2f128	$3, %ymm9, %ymm9, %ymm4
	vshufps	$68, %ymm4, %ymm9, %ymm12
	vshufps	$238, %ymm4, %ymm9, %ymm4
	vshufps	$68, %ymm8, %ymm3, %ymm9
	vinsertf128	$1, %xmm4, %ymm12, %ymm4
	vshufps	$238, %ymm8, %ymm3, %ymm8
	vinsertf128	$1, %xmm8, %ymm9, %ymm8
	vsubps	%ymm8, %ymm4, %ymm4
	vmulpd	.LC39(%rip), %ymm5, %ymm3
	vmulpd	.LC39(%rip), %ymm7, %ymm8
	vaddpd	%ymm0, %ymm3, %ymm3
	vmulpd	%ymm10, %ymm7, %ymm7
	vmulpd	%ymm10, %ymm5, %ymm5
	vaddpd	%ymm1, %ymm8, %ymm8
	vaddpd	%ymm5, %ymm0, %ymm0
	vxorps	%xmm5, %xmm5, %xmm5
	vaddpd	%ymm7, %ymm1, %ymm1
	vaddpd	%ymm6, %ymm8, %ymm8
	vaddpd	%ymm2, %ymm3, %ymm3
	vsubpd	%ymm6, %ymm1, %ymm1
	vsubpd	%ymm2, %ymm0, %ymm2
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm8, %ymm3
	vcvtpd2psy	%ymm1, %xmm0
	vcvtpd2psy	%ymm2, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovss	-820(%rax), %xmm3
	vcvtss2sd	-4194256(%rbp), %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194248(%rbp), %xmm2, %xmm2
	vsubss	-824(%rax), %xmm3, %xmm7
	vmulsd	.LC34(%rip), %xmm2, %xmm6
	vsubps	%ymm0, %ymm4, %ymm0
	vmovsd	.LC40(%rip), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm3
	vmulps	%ymm11, %ymm0, %ymm0
	vmovups	%ymm0, -2097132(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194252(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	.LC34(%rip), %xmm1, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm1
	vsubsd	%xmm6, %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovss	-812(%rax), %xmm3
	vxorps	%xmm5, %xmm5, %xmm5
	vsubss	-816(%rax), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm3
	vsubss	%xmm1, %xmm7, %xmm1
	vmulss	.LC37(%rip), %xmm1, %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vmovss	%xmm1, -2097100(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194244(%rbp), %xmm1, %xmm1
	vmulsd	.LC34(%rip), %xmm1, %xmm7
	vaddsd	%xmm6, %xmm1, %xmm6
	vaddsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	.LC34(%rip), %xmm0, %xmm3
	vaddsd	%xmm3, %xmm2, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmovss	-804(%rax), %xmm3
	vsubss	%xmm0, %xmm8, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vsubss	-808(%rax), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm2, %xmm3
	vmovss	%xmm0, -2097096(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194240(%rbp), %xmm0, %xmm0
	vmulsd	.LC34(%rip), %xmm0, %xmm5
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm1, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm6, %xmm6
	vaddsd	%xmm5, %xmm3, %xmm3
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm6, %xmm3, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vsubss	%xmm2, %xmm8, %xmm2
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vmovss	%xmm2, -2097092(%rbp)
	vmovss	-796(%rax), %xmm6
	vcvtss2sd	-4194236(%rbp), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm3, %xmm5
	vsubss	-800(%rax), %xmm6, %xmm8
	vmulsd	%xmm4, %xmm1, %xmm6
	vmulsd	.LC34(%rip), %xmm3, %xmm2
	movq	-4194352(%rbp), %rdx
	vaddsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm7, %xmm7
	movq	%rdx, -1024(%rax)
	movq	-4194344(%rbp), %rdx
	vaddsd	%xmm2, %xmm6, %xmm6
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	movq	%rdx, -1016(%rax)
	movq	-4194336(%rbp), %rdx
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-4194232(%rbp), %xmm6, %xmm6
	vmulsd	.LC34(%rip), %xmm6, %xmm7
	vsubss	%xmm1, %xmm8, %xmm1
	movq	%rdx, -1008(%rax)
	vmulss	.LC37(%rip), %xmm1, %xmm1
	movq	-4194328(%rbp), %rdx
	vsubsd	%xmm7, %xmm5, %xmm5
	vmovss	%xmm1, -2097088(%rbp)
	vmovss	-788(%rax), %xmm1
	vsubss	-792(%rax), %xmm1, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm1
	movq	%rdx, -1000(%rax)
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm5, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm3, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194228(%rbp), %xmm5, %xmm5
	vaddsd	%xmm2, %xmm6, %xmm4
	vmulsd	.LC34(%rip), %xmm5, %xmm7
	vsubss	%xmm0, %xmm8, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm3
	vmulsd	.LC33(%rip), %xmm6, %xmm6
	vsubsd	%xmm7, %xmm4, %xmm4
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	-780(%rax), %xmm0
	vaddsd	%xmm7, %xmm3, %xmm3
	vsubss	-784(%rax), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm2, %xmm2
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm6, %xmm6
	vsubss	%xmm3, %xmm0, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	-772(%rax), %xmm0
	vsubss	-776(%rax), %xmm0, %xmm7
	vmulsd	.LC36(%rip), %xmm5, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm5, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm6, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	movq	-4194320(%rbp), %rdx
	movq	-2097200(%rbp), %rsi
	movq	%rdx, -992(%rax)
	movq	-4194312(%rbp), %rdx
	movq	%rsi, -896(%rax)
	movq	-2097192(%rbp), %rsi
	movq	%rdx, -984(%rax)
	movq	-4194304(%rbp), %rdx
	movq	%rsi, -888(%rax)
	movq	-2097184(%rbp), %rsi
	movq	%rdx, -976(%rax)
	movq	-4194296(%rbp), %rdx
	movq	%rsi, -880(%rax)
	movq	-2097176(%rbp), %rsi
	movq	%rdx, -968(%rax)
	movq	-4194288(%rbp), %rdx
	movq	%rsi, -872(%rax)
	movq	-2097168(%rbp), %rsi
	movq	%rdx, -960(%rax)
	movq	-4194280(%rbp), %rdx
	movq	%rdx, -952(%rax)
	movq	-4194272(%rbp), %rdx
	movq	%rdx, -944(%rax)
	movq	-4194264(%rbp), %rdx
	movq	%rdx, -936(%rax)
	movq	-4194256(%rbp), %rdx
	movq	%rdx, -928(%rax)
	movq	-4194248(%rbp), %rdx
	movq	%rdx, -920(%rax)
	movq	-4194240(%rbp), %rdx
	movq	%rdx, -912(%rax)
	movq	-4194232(%rbp), %rdx
	movq	%rdx, -904(%rax)
	movq	%rsi, -864(%rax)
	movq	-2097160(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-2097152(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-2097144(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-2097136(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-2097128(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-2097120(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-2097112(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-2097104(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-2097096(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-2097088(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-2097080(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %ecx
	je	.L829
.L669:
	cmpl	$1, %ebx
	ja	.L672
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rax), %xmm0, %xmm0
	vcvtss2sd	8(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm0, %xmm0
	vmulsd	.LC21(%rip), %xmm1, %xmm1
	vmovups	40(%rax), %ymm5
	vmovups	104(%rax), %ymm8
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
	vmovups	8(%rax), %ymm0
	vshufps	$136, %ymm5, %ymm0, %ymm2
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm3
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vmovups	24(%rax), %ymm2
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 56(%rax), %ymm2, %ymm2
	vcvtps2pd	%xmm3, %ymm5
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm4
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovups	16(%rax), %ymm4
	vshufps	$136, 48(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm6
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vmovups	(%rax), %ymm4
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vshufps	$136, 32(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm12
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vinsertf128	$1, %xmm6, %ymm12, %ymm12
	vshufps	$68, %ymm4, %ymm0, %ymm6
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vinsertf128	$1, %xmm4, %ymm6, %ymm6
	vcvtps2pd	%xmm12, %ymm4
	vmulpd	.LC23(%rip), %ymm4, %ymm0
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vextractf128	$0x1, %ymm12, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	.LC23(%rip), %ymm5, %ymm7
	vaddpd	%ymm3, %ymm7, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vaddpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm1, %ymm2, %ymm1
	vmovups	72(%rax), %ymm2
	vshufps	$136, %ymm8, %ymm2, %ymm3
	vshufps	$221, %ymm8, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm1
	vsubps	%ymm1, %ymm6, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vmovups	%ymm1, -4194348(%rbp)
	vshufps	$68, %ymm0, %ymm3, %ymm1
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vmovups	88(%rax), %ymm0
	vshufps	$136, 120(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm6
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm6, %ymm6
	vmovups	80(%rax), %ymm3
	vshufps	$136, 112(%rax), %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vshufps	$68, %ymm0, %ymm3, %ymm7
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vmovups	64(%rax), %ymm3
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vshufps	$136, 96(%rax), %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm7
	vshufps	$68, %ymm7, %ymm3, %ymm9
	vshufps	$238, %ymm7, %ymm3, %ymm7
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vinsertf128	$1, %xmm7, %ymm9, %ymm9
	vshufps	$68, %ymm3, %ymm2, %ymm7
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vcvtps2pd	%xmm1, %ymm2
	vinsertf128	$1, %xmm3, %ymm7, %ymm7
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vcvtps2pd	%xmm9, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC23(%rip), %ymm3, %ymm8
	vaddpd	%ymm2, %ymm8, %ymm8
	vcvtps2pd	%xmm0, %ymm2
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vmovups	168(%rax), %ymm13
	vaddpd	%ymm2, %ymm8, %ymm2
	vcvtps2pd	%xmm6, %ymm8
	vmovapd	%ymm3, -4194512(%rbp)
	vmulpd	.LC25(%rip), %ymm8, %ymm8
	vsubpd	%ymm8, %ymm2, %ymm2
	vextractf128	$0x1, %ymm9, %xmm8
	vcvtps2pd	%xmm8, %ymm14
	vmulpd	.LC23(%rip), %ymm14, %ymm8
	vaddpd	%ymm1, %ymm8, %ymm1
	vmovapd	%ymm14, -4194544(%rbp)
	vcvtpd2psy	%ymm2, %xmm2
	vaddpd	%ymm0, %ymm1, %ymm1
	vextractf128	$0x1, %ymm6, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm0, %ymm7, %ymm0
	vmovups	136(%rax), %ymm7
	vshufps	$136, %ymm13, %ymm7, %ymm1
	vshufps	$221, %ymm13, %ymm7, %ymm7
	vmovups	%ymm0, -4194316(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	152(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	vshufps	$136, 184(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm6
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm6, %ymm0
	vmovups	144(%rax), %ymm6
	vshufps	$136, 176(%rax), %ymm6, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm1
	vshufps	$68, %ymm1, %ymm6, %ymm8
	vshufps	$238, %ymm1, %ymm6, %ymm1
	vinsertf128	$1, %xmm1, %ymm8, %ymm1
	vmovups	128(%rax), %ymm8
	vshufps	$136, 160(%rax), %ymm8, %ymm8
	vperm2f128	$3, %ymm8, %ymm8, %ymm6
	vshufps	$68, %ymm6, %ymm8, %ymm14
	vshufps	$238, %ymm6, %ymm8, %ymm6
	vperm2f128	$3, %ymm7, %ymm7, %ymm8
	vinsertf128	$1, %xmm6, %ymm14, %ymm6
	vshufps	$68, %ymm8, %ymm7, %ymm13
	vcvtps2pd	%xmm6, %ymm14
	vshufps	$238, %ymm8, %ymm7, %ymm8
	vcvtps2pd	%xmm2, %ymm7
	vinsertf128	$1, %xmm8, %ymm13, %ymm8
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm7, %ymm7
	vmulpd	.LC23(%rip), %ymm14, %ymm13
	vaddpd	%ymm7, %ymm13, %ymm13
	vcvtps2pd	%xmm1, %ymm7
	vmulpd	.LC24(%rip), %ymm7, %ymm7
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm7, %ymm13, %ymm7
	vcvtps2pd	%xmm0, %ymm13
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	.LC25(%rip), %ymm13, %ymm13
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm13, %ymm7, %ymm7
	vextractf128	$0x1, %ymm6, %xmm13
	vcvtps2pd	%xmm13, %ymm13
	vmulpd	.LC23(%rip), %ymm13, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm7, %xmm7
	vaddpd	%ymm1, %ymm2, %ymm2
	vmovsd	.LC26(%rip), %xmm1
	vsubpd	%ymm0, %ymm2, %ymm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm7, %ymm2
	vsubps	%ymm2, %ymm8, %ymm2
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	200(%rax), %xmm7, %xmm7
	vmovapd	%xmm7, %xmm3
	vxorpd	%xmm7, %xmm7, %xmm7
	vmovups	%ymm2, -4194284(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	208(%rax), %xmm2, %xmm2
	vmovsd	%xmm2, -4194480(%rbp)
	vcvtss2sd	216(%rax), %xmm7, %xmm7
	vcvtss2sd	192(%rax), %xmm0, %xmm0
	vmovapd	%xmm7, %xmm8
	vmulsd	%xmm3, %xmm1, %xmm7
	vmovsd	%xmm3, -4194416(%rbp)
	vmovsd	.LC22(%rip), %xmm3
	vmulsd	%xmm2, %xmm1, %xmm2
	vmovsd	%xmm8, -4194448(%rbp)
	vmulsd	.LC27(%rip), %xmm0, %xmm0
	vaddsd	%xmm7, %xmm0, %xmm0
	vmulsd	%xmm8, %xmm3, %xmm7
	vmovsd	.LC27(%rip), %xmm3
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm0
	vmovss	204(%rax), %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	%xmm8, %xmm1, %xmm7
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	224(%rax), %xmm8, %xmm8
	vmovss	%xmm0, -4194252(%rbp)
	vmulsd	-4194416(%rbp), %xmm3, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm8, %xmm2
	vaddsd	%xmm7, %xmm0, %xmm0
	vmovsd	%xmm2, -4194576(%rbp)
	vsubsd	%xmm2, %xmm0, %xmm0
	vmovss	212(%rax), %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm8, %xmm2
	vmovss	%xmm0, -4194248(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	232(%rax), %xmm0, %xmm0
	vmovsd	%xmm0, -4194416(%rbp)
	vmulsd	-4194480(%rbp), %xmm3, %xmm0
	vmovsd	.LC22(%rip), %xmm3
	vaddsd	%xmm0, %xmm7, %xmm0
	vmulsd	-4194416(%rbp), %xmm3, %xmm7
	vmovsd	.LC27(%rip), %xmm3
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm0
	vmovss	220(%rax), %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	-4194416(%rbp), %xmm1, %xmm7
	vmovss	%xmm0, -4194244(%rbp)
	vmulsd	-4194448(%rbp), %xmm3, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	240(%rax), %xmm2, %xmm2
	vmulsd	%xmm1, %xmm2, %xmm1
	vmulsd	.LC22(%rip), %xmm2, %xmm3
	vaddsd	%xmm7, %xmm0, %xmm0
	vsubsd	%xmm3, %xmm0, %xmm0
	vmovss	228(%rax), %xmm3
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, -4194240(%rbp)
	vmulsd	.LC27(%rip), %xmm8, %xmm0
	vaddsd	%xmm7, %xmm0, %xmm0
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	248(%rax), %xmm7, %xmm7
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm7, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	236(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194236(%rbp)
	vmovsd	-4194576(%rbp), %xmm1
	vmovsd	.LC20(%rip), %xmm3
	vmovaps	%ymm12, -2097200(%rbp)
	vmulsd	-4194416(%rbp), %xmm3, %xmm0
	vmovaps	%ymm9, -2097168(%rbp)
	vmulsd	.LC28(%rip), %xmm8, %xmm8
	vmovaps	%ymm6, -2097136(%rbp)
	vsubsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm3, %xmm7, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	244(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vmovss	%xmm0, -4194232(%rbp)
	vmovsd	-4194416(%rbp), %xmm0
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm8, %xmm8
	vmulsd	%xmm1, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm7, %xmm1
	vsubsd	%xmm0, %xmm8, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	252(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194228(%rbp)
	vmovss	192(%rax), %xmm0
	vmovss	%xmm0, -2097104(%rbp)
	vmovss	200(%rax), %xmm0
	vmovss	%xmm0, -2097100(%rbp)
	vmovss	208(%rax), %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vmovss	216(%rax), %xmm0
	vmovss	%xmm0, -2097092(%rbp)
	vmovss	224(%rax), %xmm0
	vmovss	%xmm0, -2097088(%rbp)
	vmovss	232(%rax), %xmm0
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	240(%rax), %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	248(%rax), %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	testb	%r14b, %r14b
	je	.L675
	vmovaps	-4194352(%rbp), %ymm1
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm4, %ymm0
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194320(%rbp), %ymm1
	vmovaps	%ymm0, -2097200(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	-4194512(%rbp), %ymm0, %ymm0
	vaddpd	-4194544(%rbp), %ymm1, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194288(%rbp), %ymm1
	vmovaps	%ymm0, -2097168(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm14, %ymm0
	vaddpd	%ymm1, %ymm13, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194256(%rbp), %ymm1
	vmovaps	%ymm0, -2097136(%rbp)
	vmovaps	-2097104(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2097104(%rbp)
.L675:
	movq	-2097200(%rbp), %rdx
	addq	$1024, %rax
	movq	-4194352(%rbp), %rsi
	movq	%rdx, -1024(%rax)
	movq	-2097192(%rbp), %rdx
	movq	%rdx, -1016(%rax)
	movq	-2097184(%rbp), %rdx
	movq	%rdx, -1008(%rax)
	movq	-2097176(%rbp), %rdx
	movq	%rdx, -1000(%rax)
	movq	-2097168(%rbp), %rdx
	movq	%rdx, -992(%rax)
	movq	-2097160(%rbp), %rdx
	movq	%rdx, -984(%rax)
	movq	-2097152(%rbp), %rdx
	movq	%rdx, -976(%rax)
	movq	-2097144(%rbp), %rdx
	movq	%rdx, -968(%rax)
	movq	-2097136(%rbp), %rdx
	movq	%rdx, -960(%rax)
	movq	-2097128(%rbp), %rdx
	movq	%rdx, -952(%rax)
	movq	-2097120(%rbp), %rdx
	movq	%rdx, -944(%rax)
	movq	-2097112(%rbp), %rdx
	movq	%rdx, -936(%rax)
	movq	-2097104(%rbp), %rdx
	movq	%rdx, -928(%rax)
	movq	-2097096(%rbp), %rdx
	movq	%rdx, -920(%rax)
	movq	-2097088(%rbp), %rdx
	movq	%rdx, -912(%rax)
	movq	-2097080(%rbp), %rdx
	movq	%rdx, -904(%rax)
	movq	%rsi, -896(%rax)
	movq	-4194344(%rbp), %rsi
	movq	%rsi, -888(%rax)
	movq	-4194336(%rbp), %rsi
	movq	%rsi, -880(%rax)
	movq	-4194328(%rbp), %rsi
	movq	%rsi, -872(%rax)
	movq	-4194320(%rbp), %rsi
	movq	%rsi, -864(%rax)
	movq	-4194312(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-4194304(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-4194296(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-4194288(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-4194280(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-4194272(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-4194264(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-4194256(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-4194248(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-4194240(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-4194232(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %ecx
	jne	.L669
.L829:
	addq	$262144, %r9
	cmpq	%r12, %r9
	jne	.L676
	movq	-4194376(%rbp), %r11
	xorl	%r10d, %r10d
.L677:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$64, %r8d
	je	.L678
.L830:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L679:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$64, %edx
	jne	.L679
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$64, %r8d
	jne	.L830
.L678:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$64, %r10d
	jne	.L677
	movq	-4194360(%rbp), %rdx
	vmovaps	.LC32(%rip), %ymm15
	vmovapd	.LC38(%rip), %ymm14
	vmovapd	.LC39(%rip), %ymm13
.L682:
	movq	%rdx, %rax
	movl	$64, %ecx
	jmp	.L687
.L683:
	vmovups	32(%rax), %ymm3
	addq	$1024, %rax
	vmovups	-1024(%rax), %ymm0
	vmovups	-928(%rax), %ymm5
	vshufps	$136, %ymm3, %ymm0, %ymm2
	vshufps	$221, %ymm3, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm4
	vshufps	$68, %ymm4, %ymm2, %ymm1
	vshufps	$238, %ymm4, %ymm2, %ymm4
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vshufps	$68, %ymm2, %ymm0, %ymm4
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-960(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm4, %ymm4
	vaddps	%ymm4, %ymm1, %ymm4
	vmovups	-864(%rax), %ymm6
	vshufps	$136, %ymm5, %ymm0, %ymm3
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm1
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-896(%rax), %ymm0
	vmulps	%ymm15, %ymm4, %ymm4
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovups	-800(%rax), %ymm7
	vaddps	%ymm2, %ymm1, %ymm1
	vcvtps2pd	%xmm4, %ymm9
	vmovaps	%ymm4, -4194352(%rbp)
	vshufps	$136, %ymm6, %ymm0, %ymm5
	vshufps	$221, %ymm6, %ymm0, %ymm0
	vperm2f128	$3, %ymm5, %ymm5, %ymm3
	vshufps	$68, %ymm3, %ymm5, %ymm2
	vshufps	$238, %ymm3, %ymm5, %ymm3
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm5
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm5, %ymm0
	vaddps	%ymm0, %ymm2, %ymm0
	vmulps	%ymm15, %ymm1, %ymm1
	vmovups	-832(%rax), %ymm3
	vmovaps	%ymm1, -4194320(%rbp)
	vmovups	-984(%rax), %ymm11
	vshufps	$136, %ymm7, %ymm3, %ymm6
	vshufps	$221, %ymm7, %ymm3, %ymm3
	vperm2f128	$3, %ymm6, %ymm6, %ymm5
	vshufps	$68, %ymm5, %ymm6, %ymm2
	vshufps	$238, %ymm5, %ymm6, %ymm5
	vinsertf128	$1, %xmm5, %ymm2, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm5
	vshufps	$68, %ymm5, %ymm3, %ymm6
	vshufps	$238, %ymm5, %ymm3, %ymm5
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vaddps	%ymm5, %ymm2, %ymm2
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-4194352(%rbp), %xmm7, %xmm7
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194344(%rbp), %xmm5, %xmm5
	vmulps	%ymm15, %ymm0, %ymm0
	vmulsd	.LC35(%rip), %xmm7, %xmm3
	vmovaps	%ymm0, -4194288(%rbp)
	vmulsd	.LC34(%rip), %xmm5, %xmm5
	vmulps	%ymm15, %ymm2, %ymm2
	vmovaps	%ymm2, -4194256(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194348(%rbp), %xmm2, %xmm2
	vmulsd	.LC33(%rip), %xmm2, %xmm8
	vmovss	-1020(%rax), %xmm2
	vsubss	-1024(%rax), %xmm2, %xmm6
	vmulsd	.LC36(%rip), %xmm7, %xmm2
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm8, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm5, %xmm2
	vmovups	-4194344(%rbp), %ymm5
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtps2pd	%xmm5, %ymm8
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm14, %ymm8, %ymm8
	vcvtps2pd	%xmm5, %ymm5
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vmovups	-4194348(%rbp), %ymm3
	vmulpd	%ymm14, %ymm5, %ymm5
	vsubss	%xmm2, %xmm6, %xmm2
	vmovups	-1016(%rax), %ymm6
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vshufps	$221, %ymm11, %ymm6, %ymm10
	vshufps	$136, %ymm11, %ymm6, %ymm6
	vperm2f128	$3, %ymm10, %ymm10, %ymm7
	vperm2f128	$3, %ymm6, %ymm6, %ymm11
	vshufps	$68, %ymm7, %ymm10, %ymm12
	vshufps	$238, %ymm7, %ymm10, %ymm7
	vshufps	$68, %ymm11, %ymm6, %ymm10
	vinsertf128	$1, %xmm7, %ymm12, %ymm7
	vshufps	$238, %ymm11, %ymm6, %ymm11
	vmulpd	%ymm13, %ymm9, %ymm6
	vinsertf128	$1, %xmm11, %ymm10, %ymm10
	vmulpd	%ymm14, %ymm9, %ymm9
	vmovss	%xmm2, -2097200(%rbp)
	vsubps	%ymm10, %ymm7, %ymm7
	vextractf128	$0x1, %ymm4, %xmm2
	vcvtps2pd	%xmm3, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm6, %ymm6
	vmulpd	%ymm13, %ymm2, %ymm10
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm9, %ymm4, %ymm4
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm14, %ymm2, %ymm2
	vaddpd	%ymm3, %ymm10, %ymm10
	vaddpd	%ymm2, %ymm3, %ymm3
	vaddpd	%ymm8, %ymm6, %ymm6
	vaddpd	%ymm5, %ymm10, %ymm10
	vsubpd	%ymm8, %ymm4, %ymm8
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtpd2psy	%ymm6, %xmm6
	vmovups	-4194312(%rbp), %ymm5
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm10, %ymm6, %ymm6
	vmovups	-920(%rax), %ymm10
	vcvtpd2psy	%ymm8, %xmm2
	vcvtps2pd	%xmm1, %ymm8
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vsubps	%ymm2, %ymm6, %ymm2
	vmovups	-952(%rax), %ymm6
	vshufps	$221, %ymm10, %ymm6, %ymm9
	vshufps	$136, %ymm10, %ymm6, %ymm10
	vperm2f128	$3, %ymm10, %ymm10, %ymm6
	vsubps	%ymm2, %ymm7, %ymm7
	vmulps	%ymm15, %ymm7, %ymm2
	vmovups	%ymm2, -2097196(%rbp)
	vcvtps2pd	%xmm5, %ymm7
	vmovups	-4194316(%rbp), %ymm2
	vmulpd	%ymm14, %ymm7, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	%ymm14, %ymm5, %ymm5
	vcvtps2pd	%xmm2, %ymm3
	vextractf128	$0x1, %ymm2, %xmm1
	vperm2f128	$3, %ymm9, %ymm9, %ymm2
	vshufps	$68, %ymm2, %ymm9, %ymm11
	vshufps	$238, %ymm2, %ymm9, %ymm2
	vshufps	$68, %ymm6, %ymm10, %ymm9
	vshufps	$238, %ymm6, %ymm10, %ymm6
	vinsertf128	$1, %xmm6, %ymm9, %ymm6
	vmulpd	%ymm13, %ymm8, %ymm9
	vmulpd	%ymm14, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm9, %ymm9
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm8, %ymm3, %ymm3
	vinsertf128	$1, %xmm2, %ymm11, %ymm2
	vsubps	%ymm6, %ymm2, %ymm6
	vmulpd	%ymm13, %ymm4, %ymm2
	vaddpd	%ymm7, %ymm9, %ymm9
	vaddpd	%ymm1, %ymm2, %ymm2
	vsubpd	%ymm7, %ymm3, %ymm7
	vmulpd	%ymm14, %ymm4, %ymm3
	vaddpd	%ymm3, %ymm1, %ymm3
	vcvtpd2psy	%ymm9, %xmm9
	vaddpd	%ymm5, %ymm2, %ymm2
	vcvtpd2psy	%ymm7, %xmm1
	vcvtps2pd	%xmm0, %ymm7
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtps2pd	%xmm0, %ymm5
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm9, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm1
	vsubps	%ymm1, %ymm6, %ymm1
	vmulps	%ymm15, %ymm1, %ymm1
	vmovups	%ymm1, -2097164(%rbp)
	vmovups	-888(%rax), %ymm3
	vmovups	-856(%rax), %ymm8
	vmovups	-4194284(%rbp), %ymm4
	vshufps	$221, %ymm8, %ymm3, %ymm9
	vshufps	$136, %ymm8, %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm8
	vmovups	-4194280(%rbp), %ymm2
	vcvtps2pd	%xmm4, %ymm1
	vextractf128	$0x1, %ymm4, %xmm0
	vperm2f128	$3, %ymm9, %ymm9, %ymm4
	vcvtps2pd	%xmm0, %ymm0
	vshufps	$68, %ymm4, %ymm9, %ymm10
	vshufps	$238, %ymm4, %ymm9, %ymm4
	vshufps	$68, %ymm8, %ymm3, %ymm9
	vinsertf128	$1, %xmm4, %ymm10, %ymm4
	vshufps	$238, %ymm8, %ymm3, %ymm8
	vmulpd	%ymm13, %ymm5, %ymm3
	vinsertf128	$1, %xmm8, %ymm9, %ymm8
	vaddpd	%ymm0, %ymm3, %ymm3
	vsubps	%ymm8, %ymm4, %ymm4
	vmulpd	%ymm14, %ymm5, %ymm5
	vmulpd	%ymm13, %ymm7, %ymm8
	vmulpd	%ymm14, %ymm7, %ymm7
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm6
	vmulpd	%ymm14, %ymm6, %ymm6
	vxorps	%xmm5, %xmm5, %xmm5
	vaddpd	%ymm1, %ymm8, %ymm8
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm14, %ymm2, %ymm2
	vaddpd	%ymm7, %ymm1, %ymm1
	vaddpd	%ymm2, %ymm3, %ymm3
	vaddpd	%ymm6, %ymm8, %ymm8
	vsubpd	%ymm6, %ymm1, %ymm1
	vsubpd	%ymm2, %ymm0, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm8, %xmm8
	vinsertf128	$0x1, %xmm3, %ymm8, %ymm3
	vcvtpd2psy	%ymm1, %xmm0
	vcvtpd2psy	%ymm2, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovss	-820(%rax), %xmm3
	vcvtss2sd	-4194256(%rbp), %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194248(%rbp), %xmm2, %xmm2
	vsubss	-824(%rax), %xmm3, %xmm7
	vmulsd	.LC34(%rip), %xmm2, %xmm6
	vsubps	%ymm0, %ymm4, %ymm0
	vmovsd	.LC40(%rip), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm3
	vmulps	%ymm15, %ymm0, %ymm0
	vmovups	%ymm0, -2097132(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194252(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	.LC34(%rip), %xmm1, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm1
	vsubsd	%xmm6, %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovss	-812(%rax), %xmm3
	vxorps	%xmm5, %xmm5, %xmm5
	vsubss	-816(%rax), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm3
	vsubss	%xmm1, %xmm7, %xmm1
	vmulss	.LC37(%rip), %xmm1, %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vmovss	%xmm1, -2097100(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194244(%rbp), %xmm1, %xmm1
	vmulsd	.LC34(%rip), %xmm1, %xmm7
	vaddsd	%xmm6, %xmm1, %xmm6
	vaddsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	.LC34(%rip), %xmm0, %xmm3
	vaddsd	%xmm2, %xmm3, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmovss	-804(%rax), %xmm3
	vsubss	%xmm0, %xmm8, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vsubss	-808(%rax), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm2, %xmm3
	vmovss	%xmm0, -2097096(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194240(%rbp), %xmm0, %xmm0
	vmulsd	.LC34(%rip), %xmm0, %xmm5
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm1, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm6, %xmm6
	vaddsd	%xmm5, %xmm3, %xmm3
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm6, %xmm3, %xmm2
	vmovss	-796(%rax), %xmm6
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194236(%rbp), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm3, %xmm5
	vsubss	%xmm2, %xmm8, %xmm2
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vsubss	-800(%rax), %xmm6, %xmm8
	vmulsd	%xmm4, %xmm1, %xmm6
	vmovss	%xmm2, -2097092(%rbp)
	vmulsd	.LC34(%rip), %xmm3, %xmm2
	vaddsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm6, %xmm6
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vsubss	%xmm1, %xmm8, %xmm1
	vmulss	.LC37(%rip), %xmm1, %xmm1
	vmovss	%xmm1, -2097088(%rbp)
	vmovss	-788(%rax), %xmm1
	vcvtss2sd	-4194232(%rbp), %xmm6, %xmm6
	vsubss	-792(%rax), %xmm1, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm1
	vmulsd	.LC34(%rip), %xmm6, %xmm7
	movq	-4194352(%rbp), %rsi
	vaddsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm7, %xmm5, %xmm5
	movq	%rsi, -1024(%rax)
	movq	-4194344(%rbp), %rsi
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	movq	%rsi, -1016(%rax)
	movq	-4194336(%rbp), %rsi
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm5, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm3, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194228(%rbp), %xmm5, %xmm5
	vaddsd	%xmm2, %xmm6, %xmm4
	vmulsd	.LC34(%rip), %xmm5, %xmm7
	vsubss	%xmm0, %xmm8, %xmm0
	movq	%rsi, -1008(%rax)
	vmulss	.LC37(%rip), %xmm0, %xmm0
	movq	-4194328(%rbp), %rsi
	vaddsd	%xmm6, %xmm1, %xmm3
	vmulsd	.LC33(%rip), %xmm6, %xmm6
	vsubsd	%xmm7, %xmm4, %xmm4
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	-780(%rax), %xmm0
	vaddsd	%xmm7, %xmm3, %xmm3
	movq	%rsi, -1000(%rax)
	movq	-4194320(%rbp), %rsi
	vsubss	-784(%rax), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm2, %xmm2
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm6, %xmm6
	movq	%rsi, -992(%rax)
	movq	-4194312(%rbp), %rsi
	vsubss	%xmm3, %xmm0, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	movq	%rsi, -984(%rax)
	movq	-4194304(%rbp), %rsi
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	-772(%rax), %xmm0
	vsubss	-776(%rax), %xmm0, %xmm7
	vmulsd	.LC36(%rip), %xmm5, %xmm0
	movq	%rsi, -976(%rax)
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm5, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm6, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	movq	-4194296(%rbp), %rsi
	movq	-2097200(%rbp), %rdi
	movq	%rsi, -968(%rax)
	movq	-4194288(%rbp), %rsi
	movq	%rdi, -896(%rax)
	movq	-2097192(%rbp), %rdi
	movq	%rsi, -960(%rax)
	movq	-4194280(%rbp), %rsi
	movq	%rdi, -888(%rax)
	movq	-2097184(%rbp), %rdi
	movq	%rsi, -952(%rax)
	movq	-4194272(%rbp), %rsi
	movq	%rdi, -880(%rax)
	movq	-2097176(%rbp), %rdi
	movq	%rsi, -944(%rax)
	movq	-4194264(%rbp), %rsi
	movq	%rdi, -872(%rax)
	movq	-2097168(%rbp), %rdi
	movq	%rsi, -936(%rax)
	movq	-4194256(%rbp), %rsi
	movq	%rdi, -864(%rax)
	movq	-2097160(%rbp), %rdi
	movq	%rsi, -928(%rax)
	movq	-4194248(%rbp), %rsi
	movq	%rdi, -856(%rax)
	movq	-2097152(%rbp), %rdi
	movq	%rsi, -920(%rax)
	movq	-4194240(%rbp), %rsi
	movq	%rdi, -848(%rax)
	movq	-2097144(%rbp), %rdi
	movq	%rsi, -912(%rax)
	movq	-4194232(%rbp), %rsi
	movq	%rsi, -904(%rax)
	movq	%rdi, -840(%rax)
	movq	-2097136(%rbp), %rdi
	movq	%rdi, -832(%rax)
	movq	-2097128(%rbp), %rdi
	movq	%rdi, -824(%rax)
	movq	-2097120(%rbp), %rdi
	movq	%rdi, -816(%rax)
	movq	-2097112(%rbp), %rdi
	movq	%rdi, -808(%rax)
	movq	-2097104(%rbp), %rdi
	movq	%rdi, -800(%rax)
	movq	-2097096(%rbp), %rdi
	movq	%rdi, -792(%rax)
	movq	-2097088(%rbp), %rdi
	movq	%rdi, -784(%rax)
	movq	-2097080(%rbp), %rdi
	movq	%rdi, -776(%rax)
	subl	$1, %ecx
	je	.L831
.L687:
	cmpl	$1, %ebx
	ja	.L683
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rax), %xmm0, %xmm0
	vcvtss2sd	8(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm0, %xmm0
	vmulsd	.LC21(%rip), %xmm1, %xmm1
	vmovups	40(%rax), %ymm5
	vmovups	104(%rax), %ymm8
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
	vmovups	8(%rax), %ymm0
	vshufps	$136, %ymm5, %ymm0, %ymm2
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm3
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vmovups	24(%rax), %ymm2
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 56(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm4
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovups	16(%rax), %ymm4
	vshufps	$136, 48(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm6
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vmovups	(%rax), %ymm4
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vshufps	$136, 32(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm10
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vinsertf128	$1, %xmm6, %ymm10, %ymm10
	vshufps	$68, %ymm4, %ymm0, %ymm5
	vcvtps2pd	%xmm10, %ymm6
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vmulpd	.LC23(%rip), %ymm6, %ymm0
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm10, %xmm7
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm7, %ymm7
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vmulpd	.LC23(%rip), %ymm7, %ymm5
	vaddpd	%ymm3, %ymm5, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vaddpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm1, %ymm2, %ymm1
	vmovups	72(%rax), %ymm2
	vshufps	$136, %ymm8, %ymm2, %ymm3
	vshufps	$221, %ymm8, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm1
	vsubps	%ymm1, %ymm4, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vmovups	%ymm1, -4194348(%rbp)
	vshufps	$68, %ymm0, %ymm3, %ymm1
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vmovups	88(%rax), %ymm0
	vshufps	$136, 120(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm4
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovups	80(%rax), %ymm4
	vshufps	$136, 112(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm0
	vshufps	$68, %ymm0, %ymm4, %ymm5
	vshufps	$238, %ymm0, %ymm4, %ymm0
	vmovups	64(%rax), %ymm4
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	vshufps	$136, 96(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vshufps	$68, %ymm5, %ymm4, %ymm9
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vperm2f128	$3, %ymm2, %ymm2, %ymm4
	vinsertf128	$1, %xmm5, %ymm9, %ymm9
	vshufps	$68, %ymm4, %ymm2, %ymm5
	vshufps	$238, %ymm4, %ymm2, %ymm4
	vcvtps2pd	%xmm1, %ymm2
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vcvtps2pd	%xmm9, %ymm5
	vmovapd	%ymm5, -4194512(%rbp)
	vmulpd	.LC23(%rip), %ymm5, %ymm5
	vaddpd	%ymm2, %ymm5, %ymm5
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vaddpd	%ymm2, %ymm5, %ymm2
	vcvtps2pd	%xmm3, %ymm5
	vmovups	168(%rax), %ymm11
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vsubpd	%ymm5, %ymm2, %ymm2
	vextractf128	$0x1, %ymm9, %xmm5
	vcvtps2pd	%xmm5, %ymm12
	vmulpd	.LC23(%rip), %ymm12, %ymm5
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtpd2psy	%ymm2, %xmm2
	vaddpd	%ymm0, %ymm1, %ymm1
	vextractf128	$0x1, %ymm3, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vmovups	136(%rax), %ymm3
	vsubpd	%ymm0, %ymm1, %ymm0
	vshufps	$136, %ymm11, %ymm3, %ymm1
	vshufps	$221, %ymm11, %ymm3, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm0, %ymm4, %ymm0
	vmovups	%ymm0, -4194316(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	152(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	vshufps	$136, 184(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm4
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmovups	144(%rax), %ymm4
	vshufps	$136, 176(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm1
	vshufps	$68, %ymm1, %ymm4, %ymm5
	vshufps	$238, %ymm1, %ymm4, %ymm1
	vmovups	128(%rax), %ymm4
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vshufps	$136, 160(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vshufps	$68, %ymm5, %ymm4, %ymm8
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vperm2f128	$3, %ymm3, %ymm3, %ymm4
	vinsertf128	$1, %xmm5, %ymm8, %ymm8
	vshufps	$68, %ymm4, %ymm3, %ymm11
	vcvtps2pd	%xmm8, %ymm5
	vshufps	$238, %ymm4, %ymm3, %ymm4
	vmovapd	%ymm5, -4194544(%rbp)
	vcvtps2pd	%xmm2, %ymm3
	vinsertf128	$1, %xmm4, %ymm11, %ymm11
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmulpd	.LC23(%rip), %ymm5, %ymm5
	vaddpd	%ymm3, %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm3, %ymm5, %ymm3
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm5, %ymm3, %ymm3
	vextractf128	$0x1, %ymm8, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	.LC23(%rip), %ymm5, %ymm4
	vaddpd	%ymm4, %ymm2, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm1, %ymm2, %ymm2
	vmovsd	.LC26(%rip), %xmm1
	vsubpd	%ymm0, %ymm2, %ymm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vsubps	%ymm2, %ymm11, %ymm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	200(%rax), %xmm3, %xmm3
	vmovapd	%xmm3, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovups	%ymm2, -4194284(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	208(%rax), %xmm2, %xmm2
	vmovsd	%xmm2, -4194480(%rbp)
	vcvtss2sd	216(%rax), %xmm3, %xmm3
	vcvtss2sd	192(%rax), %xmm0, %xmm0
	vmovapd	%xmm3, %xmm11
	vmulsd	%xmm4, %xmm1, %xmm3
	vmovsd	%xmm4, -4194416(%rbp)
	vmovsd	.LC22(%rip), %xmm4
	vmulsd	%xmm2, %xmm1, %xmm2
	vmovsd	%xmm11, -4194448(%rbp)
	vmulsd	.LC27(%rip), %xmm0, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vmulsd	%xmm11, %xmm4, %xmm3
	vmovsd	.LC27(%rip), %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm3, %xmm0, %xmm0
	vmovss	204(%rax), %xmm3
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmulsd	%xmm11, %xmm1, %xmm3
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	224(%rax), %xmm11, %xmm11
	vmovss	%xmm0, -4194252(%rbp)
	vmulsd	-4194416(%rbp), %xmm4, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vmulsd	.LC22(%rip), %xmm11, %xmm2
	vaddsd	%xmm3, %xmm0, %xmm0
	vmovsd	%xmm2, -4194576(%rbp)
	vsubsd	%xmm2, %xmm0, %xmm0
	vmovss	212(%rax), %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm11, %xmm2
	vmovss	%xmm0, -4194248(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	232(%rax), %xmm0, %xmm0
	vmovsd	%xmm0, -4194416(%rbp)
	vmulsd	-4194480(%rbp), %xmm4, %xmm0
	vmovsd	.LC22(%rip), %xmm4
	vaddsd	%xmm0, %xmm3, %xmm0
	vmulsd	-4194416(%rbp), %xmm4, %xmm3
	vmovsd	.LC27(%rip), %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm3, %xmm0, %xmm0
	vmovss	220(%rax), %xmm3
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmulsd	-4194416(%rbp), %xmm1, %xmm3
	vmovss	%xmm0, -4194244(%rbp)
	vmulsd	-4194448(%rbp), %xmm4, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	240(%rax), %xmm2, %xmm2
	vmulsd	%xmm1, %xmm2, %xmm1
	vmulsd	.LC22(%rip), %xmm2, %xmm4
	vaddsd	%xmm3, %xmm0, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm0
	vmovss	228(%rax), %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, -4194240(%rbp)
	vmulsd	.LC27(%rip), %xmm11, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	248(%rax), %xmm3, %xmm3
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm3, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	236(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194236(%rbp)
	vmovsd	-4194576(%rbp), %xmm1
	vmovsd	.LC20(%rip), %xmm4
	vmovaps	%ymm10, -2097200(%rbp)
	vmulsd	-4194416(%rbp), %xmm4, %xmm0
	vmovaps	%ymm9, -2097168(%rbp)
	vmulsd	.LC28(%rip), %xmm11, %xmm11
	vmovaps	%ymm8, -2097136(%rbp)
	vsubsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm3, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	244(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vmovss	%xmm0, -4194232(%rbp)
	vmovsd	-4194416(%rbp), %xmm0
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm11, %xmm11
	vmulsd	%xmm1, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm0, %xmm11, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	252(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194228(%rbp)
	vmovss	192(%rax), %xmm0
	vmovss	%xmm0, -2097104(%rbp)
	vmovss	200(%rax), %xmm0
	vmovss	%xmm0, -2097100(%rbp)
	vmovss	208(%rax), %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vmovss	216(%rax), %xmm0
	vmovss	%xmm0, -2097092(%rbp)
	vmovss	224(%rax), %xmm0
	vmovss	%xmm0, -2097088(%rbp)
	vmovss	232(%rax), %xmm0
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	240(%rax), %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	248(%rax), %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	testb	%r14b, %r14b
	je	.L686
	vmovaps	-4194352(%rbp), %ymm1
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm6, %ymm0
	vaddpd	%ymm1, %ymm7, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194320(%rbp), %ymm1
	vmovaps	%ymm0, -2097200(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	-4194512(%rbp), %ymm0, %ymm0
	vaddpd	%ymm1, %ymm12, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194288(%rbp), %ymm1
	vmovaps	%ymm0, -2097168(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	-4194544(%rbp), %ymm0, %ymm0
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194256(%rbp), %ymm1
	vmovaps	%ymm0, -2097136(%rbp)
	vmovaps	-2097104(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2097104(%rbp)
.L686:
	movq	-2097200(%rbp), %rsi
	addq	$1024, %rax
	movq	-4194352(%rbp), %rdi
	movq	%rsi, -1024(%rax)
	movq	-2097192(%rbp), %rsi
	movq	%rsi, -1016(%rax)
	movq	-2097184(%rbp), %rsi
	movq	%rsi, -1008(%rax)
	movq	-2097176(%rbp), %rsi
	movq	%rsi, -1000(%rax)
	movq	-2097168(%rbp), %rsi
	movq	%rsi, -992(%rax)
	movq	-2097160(%rbp), %rsi
	movq	%rsi, -984(%rax)
	movq	-2097152(%rbp), %rsi
	movq	%rsi, -976(%rax)
	movq	-2097144(%rbp), %rsi
	movq	%rsi, -968(%rax)
	movq	-2097136(%rbp), %rsi
	movq	%rsi, -960(%rax)
	movq	-2097128(%rbp), %rsi
	movq	%rsi, -952(%rax)
	movq	-2097120(%rbp), %rsi
	movq	%rsi, -944(%rax)
	movq	-2097112(%rbp), %rsi
	movq	%rsi, -936(%rax)
	movq	-2097104(%rbp), %rsi
	movq	%rsi, -928(%rax)
	movq	-2097096(%rbp), %rsi
	movq	%rsi, -920(%rax)
	movq	-2097088(%rbp), %rsi
	movq	%rsi, -912(%rax)
	movq	-2097080(%rbp), %rsi
	movq	%rsi, -904(%rax)
	movq	%rdi, -896(%rax)
	movq	-4194344(%rbp), %rdi
	movq	%rdi, -888(%rax)
	movq	-4194336(%rbp), %rdi
	movq	%rdi, -880(%rax)
	movq	-4194328(%rbp), %rdi
	movq	%rdi, -872(%rax)
	movq	-4194320(%rbp), %rdi
	movq	%rdi, -864(%rax)
	movq	-4194312(%rbp), %rdi
	movq	%rdi, -856(%rax)
	movq	-4194304(%rbp), %rdi
	movq	%rdi, -848(%rax)
	movq	-4194296(%rbp), %rdi
	movq	%rdi, -840(%rax)
	movq	-4194288(%rbp), %rdi
	movq	%rdi, -832(%rax)
	movq	-4194280(%rbp), %rdi
	movq	%rdi, -824(%rax)
	movq	-4194272(%rbp), %rdi
	movq	%rdi, -816(%rax)
	movq	-4194264(%rbp), %rdi
	movq	%rdi, -808(%rax)
	movq	-4194256(%rbp), %rdi
	movq	%rdi, -800(%rax)
	movq	-4194248(%rbp), %rdi
	movq	%rdi, -792(%rax)
	movq	-4194240(%rbp), %rdi
	movq	%rdi, -784(%rax)
	movq	-4194232(%rbp), %rdi
	movq	%rdi, -776(%rax)
	subl	$1, %ecx
	jne	.L687
.L831:
	addq	$262144, %rdx
	cmpq	%rdx, %r12
	jne	.L682
	movq	-4194368(%rbp), %rax
	movl	%ecx, -4194512(%rbp)
	movq	-4194360(%rbp), %r13
	leaq	8388632(%rax), %r12
.L693:
	leaq	32768(%r13), %rbx
	movq	%r13, %r14
.L689:
	movq	%r14, %rdi
	movl	%r15d, %esi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	cmpq	%r14, %rbx
	jne	.L689
	leaq	1024(%r13), %r8
	xorl	%edi, %edi
	movq	%r13, %rsi
	addl	$1, %edi
	cmpl	$32, %edi
	je	.L746
.L832:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L691:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$32, %eax
	jne	.L691
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$32, %edi
	jne	.L832
.L746:
	movq	%r13, %r14
.L690:
	movq	%r14, %rdi
	movl	%r15d, %esi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	cmpq	%r14, %rbx
	jne	.L690
	addq	$262144, %r13
	cmpq	%r12, %r13
	jne	.L693
	movq	-4194376(%rbp), %r11
	xorl	%r8d, %r8d
.L694:
	xorl	%r9d, %r9d
	movslq	%r8d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r10
	addl	$1, %r9d
	salq	$8, %rdi
	cmpl	$32, %r9d
	je	.L695
.L833:
	movq	%r10, %rcx
	movl	%r9d, %edx
	.p2align 4,,10
	.p2align 3
.L696:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$32, %edx
	jne	.L696
	addl	$1, %r9d
	addq	$262148, %r10
	addq	$262144, %rsi
	cmpl	$32, %r9d
	jne	.L833
.L695:
	addl	$1, %r8d
	addq	$1024, %r11
	cmpl	$32, %r8d
	jne	.L694
	movq	-4194360(%rbp), %r13
.L699:
	leaq	32768(%r13), %r14
	movq	%r13, %rbx
.L700:
	movq	%rbx, %rdi
	movl	%r15d, %esi
	addq	$1024, %rbx
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	cmpq	%r14, %rbx
	jne	.L700
	addq	$262144, %r13
	cmpq	%r13, %r12
	jne	.L699
	movq	-4194368(%rbp), %rax
	movq	-4194360(%rbp), %rbx
	addq	$4194328, %rax
	movq	%rax, -4194448(%rbp)
.L705:
	leaq	16384(%rbx), %rax
	movq	%rbx, %r12
	movq	%rax, -4194416(%rbp)
.L702:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	cmpq	%r12, -4194416(%rbp)
	jne	.L702
	movq	%rbx, %rax
	movq	%rbx, %rdx
	xorl	%edi, %edi
	movl	$15, -4194480(%rbp)
	movl	$9, %r13d
	movl	$8, %r12d
	movl	$7, %r11d
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %esi
	movl	$2, %ecx
	jmp	.L743
	.p2align 4,,10
	.p2align 3
.L834:
	vmovss	4(%rax), %xmm0
	vmovss	1024(%rax), %xmm1
	vmovss	%xmm0, 1024(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %ecx
	ja	.L704
	movslq	%ecx, %r14
	vmovss	2048(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 2048(%rax)
	cmpl	$15, %esi
	ja	.L704
	movslq	%esi, %r14
	vmovss	3072(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 3072(%rax)
	cmpl	$15, %r8d
	ja	.L704
	movslq	%r8d, %r14
	vmovss	4096(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 4096(%rax)
	cmpl	$15, %r9d
	ja	.L704
	movslq	%r9d, %r14
	vmovss	5120(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 5120(%rax)
	cmpl	$15, %r10d
	ja	.L704
	movslq	%r10d, %r14
	vmovss	6144(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 6144(%rax)
	cmpl	$15, %r11d
	ja	.L704
	movslq	%r11d, %r14
	vmovss	7168(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 7168(%rax)
	cmpl	$15, %r12d
	ja	.L704
	movslq	%r12d, %r14
	vmovss	8192(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 8192(%rax)
	cmpl	$15, %r13d
	ja	.L704
	movslq	%r13d, %r14
	vmovss	9216(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	9(%rdi), %r14d
	vmovss	%xmm0, 9216(%rax)
	cmpl	$15, %r14d
	ja	.L704
	movslq	%r14d, %r14
	vmovss	10240(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	10(%rdi), %r14d
	vmovss	%xmm0, 10240(%rax)
	cmpl	$15, %r14d
	ja	.L704
	movslq	%r14d, %r14
	vmovss	11264(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	11(%rdi), %r14d
	vmovss	%xmm0, 11264(%rax)
	cmpl	$15, %r14d
	ja	.L704
	movslq	%r14d, %r14
	vmovss	12288(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	12(%rdi), %r14d
	vmovss	%xmm0, 12288(%rax)
	cmpl	$15, %r14d
	ja	.L704
	movslq	%r14d, %r14
	vmovss	13312(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	13(%rdi), %r14d
	vmovss	%xmm0, 13312(%rax)
	cmpl	$15, %r14d
	ja	.L704
	movslq	%r14d, %r14
	cmpl	$15, -4194480(%rbp)
	vmovss	14336(%rax), %xmm1
	leaq	(%rdx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 14336(%rax)
	jne	.L704
	vmovss	60(%rdx), %xmm0
	vmovss	15360(%rax), %xmm1
	vmovss	%xmm1, 60(%rdx)
	vmovss	%xmm0, 15360(%rax)
.L704:
	addl	$1, -4194480(%rbp)
	addq	$1024, %rdx
	addl	$1, %ecx
	addl	$1, %esi
	addq	$1028, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %r12d
	addl	$1, %r13d
.L743:
	addl	$1, %edi
	cmpl	$16, %edi
	jne	.L834
	movq	%rbx, %r12
.L703:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	cmpq	%r12, -4194416(%rbp)
	jne	.L703
	addq	$262144, %rbx
	cmpq	-4194448(%rbp), %rbx
	jne	.L705
	movq	-4194360(%rbp), %rax
	movl	$0, -4194416(%rbp)
	movq	%rax, -4194480(%rbp)
.L706:
	movslq	-4194416(%rbp), %rdi
	movl	$8, %r13d
	xorl	%esi, %esi
	movl	$7, %r12d
	movq	-4194480(%rbp), %rax
	movl	$6, %ebx
	movl	$5, %r11d
	movl	$4, %r10d
	movq	-4194360(%rbp), %rdx
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$15, %r14d
	movq	%rdi, %rcx
	salq	$10, %rdi
	salq	$8, %rcx
	movq	%rdi, -4194544(%rbp)
	jmp	.L709
	.p2align 4,,10
	.p2align 3
.L835:
	vmovss	4(%rax), %xmm0
	vmovss	262144(%rax), %xmm1
	vmovss	%xmm0, 262144(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %r8d
	ja	.L708
	movslq	%r8d, %rdi
	vmovss	524288(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 524288(%rax)
	cmpl	$15, %r9d
	ja	.L708
	movslq	%r9d, %rdi
	vmovss	786432(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 786432(%rax)
	cmpl	$15, %r10d
	ja	.L708
	movslq	%r10d, %rdi
	vmovss	1048576(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1048576(%rax)
	cmpl	$15, %r11d
	ja	.L708
	movslq	%r11d, %rdi
	vmovss	1310720(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1310720(%rax)
	cmpl	$15, %ebx
	ja	.L708
	movslq	%ebx, %rdi
	vmovss	1572864(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1572864(%rax)
	cmpl	$15, %r12d
	ja	.L708
	movslq	%r12d, %rdi
	vmovss	1835008(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1835008(%rax)
	cmpl	$15, %r13d
	ja	.L708
	movslq	%r13d, %rdi
	vmovss	2097152(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	8(%rsi), %edi
	vmovss	%xmm0, 2097152(%rax)
	cmpl	$15, %edi
	ja	.L708
	movslq	%edi, %rdi
	vmovss	2359296(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	9(%rsi), %edi
	vmovss	%xmm0, 2359296(%rax)
	cmpl	$15, %edi
	ja	.L708
	movslq	%edi, %rdi
	vmovss	2621440(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	10(%rsi), %edi
	vmovss	%xmm0, 2621440(%rax)
	cmpl	$15, %edi
	ja	.L708
	movslq	%edi, %rdi
	vmovss	2883584(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	11(%rsi), %edi
	vmovss	%xmm0, 2883584(%rax)
	cmpl	$15, %edi
	ja	.L708
	movslq	%edi, %rdi
	vmovss	3145728(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	12(%rsi), %edi
	vmovss	%xmm0, 3145728(%rax)
	cmpl	$15, %edi
	ja	.L708
	movslq	%edi, %rdi
	vmovss	3407872(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	13(%rsi), %edi
	vmovss	%xmm0, 3407872(%rax)
	cmpl	$15, %edi
	ja	.L708
	movslq	%edi, %rdi
	vmovss	3670016(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 3670016(%rax)
	cmpl	$15, %r14d
	jne	.L708
	movq	-4194544(%rbp), %rdi
	vmovss	3932160(%rax), %xmm1
	addq	%rdx, %rdi
	vmovss	60(%rdi), %xmm0
	vmovss	%xmm1, 60(%rdi)
	vmovss	%xmm0, 3932160(%rax)
.L708:
	addl	$1, %r14d
	addq	$262144, %rdx
	addq	$262148, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
	addl	$1, %r13d
.L709:
	addl	$1, %esi
	cmpl	$16, %esi
	jne	.L835
	addl	$1, -4194416(%rbp)
	movl	-4194416(%rbp), %eax
	addq	$1024, -4194480(%rbp)
	cmpl	$16, %eax
	jne	.L706
	movq	-4194360(%rbp), %r12
.L710:
	leaq	16384(%r12), %r13
	movq	%r12, %rbx
.L711:
	movq	%rbx, %rdi
	movl	%r15d, %esi
	addq	$1024, %rbx
	call	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	cmpq	%r13, %rbx
	jne	.L711
	addq	$262144, %r12
	cmpq	-4194448(%rbp), %r12
	jne	.L710
	movq	-4194368(%rbp), %rax
	movq	-4194360(%rbp), %r12
	addq	$2097176, %rax
	movq	%rax, -4194416(%rbp)
.L718:
	leaq	8192(%r12), %rbx
	movq	%r12, %r13
.L713:
	movq	%r13, %rdi
	movl	%r15d, %esi
	addq	$1024, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r13, %rbx
	jne	.L713
	leaq	7168(%r12), %rax
	movq	%r12, %rdx
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %edi
	movl	$2, %esi
	movl	$7, %ecx
.L715:
	cmpl	$14, %ecx
	je	.L717
	vmovss	-7164(%rax), %xmm0
	vmovss	-6144(%rax), %xmm1
	vmovss	%xmm0, -6144(%rax)
	vmovss	%xmm1, -7164(%rax)
	cmpl	$7, %esi
	ja	.L717
	movslq	%esi, %r11
	vmovss	-5120(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -5120(%rax)
	cmpl	$7, %edi
	ja	.L717
	movslq	%edi, %r11
	vmovss	-4096(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -4096(%rax)
	cmpl	$7, %r8d
	ja	.L717
	movslq	%r8d, %r11
	vmovss	-3072(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -3072(%rax)
	cmpl	$7, %r9d
	ja	.L717
	movslq	%r9d, %r11
	vmovss	-2048(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -2048(%rax)
	cmpl	$7, %r10d
	ja	.L717
	movslq	%r10d, %r11
	vmovss	-1024(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -1024(%rax)
	cmpl	$7, %ecx
	jne	.L717
	vmovss	28(%rdx), %xmm0
	vmovss	(%rax), %xmm1
	vmovss	%xmm1, 28(%rdx)
	vmovss	%xmm0, (%rax)
.L717:
	addl	$1, %ecx
	addq	$1024, %rdx
	addq	$1028, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	cmpl	$15, %ecx
	jne	.L715
	movq	%r12, %r13
.L716:
	movq	%r13, %rdi
	movl	%r15d, %esi
	addq	$1024, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r13, %rbx
	jne	.L716
	addq	$262144, %r12
	cmpq	-4194416(%rbp), %r12
	jne	.L718
	movq	-4194368(%rbp), %rax
	xorl	%r14d, %r14d
	leaq	28(%rax), %r13
.L722:
	movq	-4194360(%rbp), %rdx
	movslq	%r14d, %rdi
	movq	%r13, %rax
	movl	$6, %ebx
	movq	%rdi, %rcx
	movl	$5, %r11d
	salq	$10, %rdi
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$7, %esi
	salq	$8, %rcx
.L721:
	cmpl	$14, %esi
	je	.L720
	vmovss	(%rax), %xmm0
	vmovss	262140(%rax), %xmm1
	vmovss	%xmm0, 262140(%rax)
	vmovss	%xmm1, (%rax)
	cmpl	$7, %r8d
	ja	.L720
	movslq	%r8d, %r12
	vmovss	524284(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 524284(%rax)
	cmpl	$7, %r9d
	ja	.L720
	movslq	%r9d, %r12
	vmovss	786428(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 786428(%rax)
	cmpl	$7, %r10d
	ja	.L720
	movslq	%r10d, %r12
	vmovss	1048572(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1048572(%rax)
	cmpl	$7, %r11d
	ja	.L720
	movslq	%r11d, %r12
	vmovss	1310716(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1310716(%rax)
	cmpl	$7, %ebx
	ja	.L720
	movslq	%ebx, %r12
	vmovss	1572860(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1572860(%rax)
	cmpl	$7, %esi
	jne	.L720
	leaq	(%rdx,%rdi), %r12
	vmovss	1835004(%rax), %xmm1
	vmovss	28(%r12), %xmm0
	vmovss	%xmm1, 28(%r12)
	vmovss	%xmm0, 1835004(%rax)
.L720:
	addl	$1, %esi
	addq	$262144, %rdx
	addq	$262148, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	cmpl	$15, %esi
	jne	.L721
	addl	$1, %r14d
	addq	$1024, %r13
	cmpl	$8, %r14d
	jne	.L722
	movq	-4194624(%rbp), %rbx
.L723:
	leaq	8192(%rbx), %r13
	movq	%rbx, %r12
.L724:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r13, %r12
	jne	.L724
	addq	$262144, %rbx
	cmpq	-4194416(%rbp), %rbx
	jne	.L723
	movl	$2097152, %edx
	xorl	%esi, %esi
	leaq	-4194352(%rbp), %rdi
	movl	$7, %r13d
	call	memset
	movq	-4194368(%rbp), %r15
	leaq	-4194352(%rbp), %rsi
	movq	-4194360(%rbp), %rcx
	vmovss	-4194384(%rbp), %xmm0
	leaq	69206048(%r15), %r12
	leaq	16(%r15), %rdi
	movq	%r12, %rdx
	call	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf
	movl	$2097152, %edx
	leaq	-2097200(%rbp), %rdi
	movslq	%eax, %rbx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	leaq	67108896(%r15), %rdi
	xorl	%edx, %edx
	movl	$6, %r11d
	movl	$5, %r10d
	movl	$1, %eax
	movl	$4, %r9d
	movq	$2, -4194360(%rbp)
	movl	$3, %r8d
	movl	$1, %r15d
.L726:
	movl	%r15d, %ecx
	movq	%rax, %rsi
	movq	%rax, %r14
	salq	%cl, %rsi
	movq	%r15, %rcx
	shrq	$6, %rcx
	testq	%rsi, -2097200(%rbp,%rcx,8)
	movl	-4194360(%rbp), %ecx
	setne	%sil
	salq	%cl, %r14
	movq	-4194360(%rbp), %rcx
	movzbl	%sil, %esi
	addl	%esi, %esi
	shrq	$6, %rcx
	testq	%r14, -2097200(%rbp,%rcx,8)
	movq	%rax, %r14
	setne	%cl
	movzbl	%cl, %ecx
	sall	$2, %ecx
	orl	%ecx, %esi
	movl	%edx, %ecx
	salq	%cl, %r14
	movq	%rdx, %rcx
	shrq	$6, %rcx
	testq	%r14, -2097200(%rbp,%rcx,8)
	movq	%rax, %r14
	setne	%cl
	movzbl	%cl, %ecx
	orl	%ecx, %esi
	movl	%r8d, %ecx
	salq	%cl, %r14
	movq	%r8, %rcx
	shrq	$6, %rcx
	testq	%r14, -2097200(%rbp,%rcx,8)
	movq	%rax, %r14
	setne	%cl
	movzbl	%cl, %ecx
	sall	$3, %ecx
	orl	%ecx, %esi
	movl	%r9d, %ecx
	salq	%cl, %r14
	movq	%r9, %rcx
	shrq	$6, %rcx
	testq	%r14, -2097200(%rbp,%rcx,8)
	movq	%rax, %r14
	setne	%cl
	movzbl	%cl, %ecx
	sall	$4, %ecx
	orl	%ecx, %esi
	movl	%r10d, %ecx
	salq	%cl, %r14
	movq	%r10, %rcx
	shrq	$6, %rcx
	testq	%r14, -2097200(%rbp,%rcx,8)
	movq	%rax, %r14
	setne	%cl
	movzbl	%cl, %ecx
	sall	$5, %ecx
	orl	%ecx, %esi
	movl	%r11d, %ecx
	salq	%cl, %r14
	movq	%r11, %rcx
	shrq	$6, %rcx
	testq	%r14, -2097200(%rbp,%rcx,8)
	movq	%rax, %r14
	setne	%cl
	movzbl	%cl, %ecx
	sall	$6, %ecx
	orl	%ecx, %esi
	movl	%r13d, %ecx
	salq	%cl, %r14
	movq	%r13, %rcx
	shrq	$6, %rcx
	testq	%r14, -2097200(%rbp,%rcx,8)
	setne	%cl
	addq	$8, %rdx
	addq	$8, %r15
	movzbl	%cl, %ecx
	addq	$8, %r8
	addq	$8, %r9
	sall	$7, %ecx
	addq	$8, %r10
	addq	$8, %r11
	orl	%ecx, %esi
	addq	$8, %r13
	addq	$1, %rdi
	addq	$8, -4194360(%rbp)
	movb	%sil, -1(%rdi)
	cmpq	$16777216, %rdx
	jne	.L726
	cmpb	$0, -4194628(%rbp)
	je	.L727
	testl	%ebx, %ebx
	jle	.L728
	leal	-32(%rbx), %ecx
	leal	-1(%rbx), %eax
	shrl	$5, %ecx
	addl	$1, %ecx
	movl	%ecx, %edx
	sall	$5, %edx
	cmpl	$30, %eax
	jbe	.L748
	movl	-4194512(%rbp), %esi
	movq	%r12, %rax
	vmovdqa	.LC54(%rip), %ymm7
	vmovdqa	.LC55(%rip), %ymm6
	vmovdqa	.LC56(%rip), %ymm5
	vmovdqa	.LC57(%rip), %ymm4
.L730:
	vmovdqu	(%rax), %ymm1
	addl	$1, %esi
	subq	$-128, %rax
	vmovdqu	-96(%rax), %ymm9
	vmovdqu	-64(%rax), %ymm0
	vpshufb	%ymm7, %ymm1, %ymm2
	vpshufb	%ymm5, %ymm1, %ymm1
	vmovdqu	-32(%rax), %ymm8
	vpshufb	%ymm6, %ymm9, %ymm3
	vpshufb	%ymm4, %ymm9, %ymm9
	vpor	%ymm3, %ymm2, %ymm3
	vpor	%ymm9, %ymm1, %ymm1
	vpermq	$216, %ymm3, %ymm3
	vpermq	$216, %ymm1, %ymm1
	vpshufb	%ymm7, %ymm0, %ymm2
	vpshufb	%ymm6, %ymm8, %ymm9
	vpshufb	%ymm5, %ymm0, %ymm0
	vpor	%ymm9, %ymm2, %ymm2
	vpshufb	%ymm4, %ymm8, %ymm8
	vpermq	$216, %ymm2, %ymm2
	vpor	%ymm8, %ymm0, %ymm0
	vpshufb	%ymm6, %ymm2, %ymm9
	vpermq	$216, %ymm0, %ymm0
	vpshufb	%ymm7, %ymm3, %ymm8
	vpshufb	%ymm4, %ymm2, %ymm2
	vpor	%ymm9, %ymm8, %ymm8
	vpshufb	%ymm5, %ymm3, %ymm3
	vpermq	$216, %ymm8, %ymm8
	vpor	%ymm2, %ymm3, %ymm2
	vpshufb	%ymm6, %ymm0, %ymm9
	vpermq	$216, %ymm2, %ymm2
	vpshufb	%ymm7, %ymm1, %ymm3
	vpshufb	%ymm4, %ymm0, %ymm0
	vpshufb	%ymm5, %ymm1, %ymm1
	vpor	%ymm9, %ymm3, %ymm3
	vpor	%ymm0, %ymm1, %ymm0
	vpermq	$216, %ymm3, %ymm3
	vpermq	$216, %ymm0, %ymm0
	vpunpcklbw	%ymm3, %ymm0, %ymm1
	vpunpckhbw	%ymm3, %ymm0, %ymm0
	vperm2i128	$32, %ymm0, %ymm1, %ymm3
	vperm2i128	$49, %ymm0, %ymm1, %ymm0
	vpunpcklbw	%ymm8, %ymm2, %ymm1
	vpunpckhbw	%ymm8, %ymm2, %ymm8
	vperm2i128	$32, %ymm8, %ymm1, %ymm2
	vperm2i128	$49, %ymm8, %ymm1, %ymm1
	vpunpcklbw	%ymm2, %ymm3, %ymm8
	vpunpckhbw	%ymm2, %ymm3, %ymm2
	vperm2i128	$32, %ymm2, %ymm8, %ymm3
	vperm2i128	$49, %ymm2, %ymm8, %ymm2
	vmovdqu	%ymm3, -128(%rax)
	vmovdqu	%ymm2, -96(%rax)
	vpunpcklbw	%ymm1, %ymm0, %ymm2
	vpunpckhbw	%ymm1, %ymm0, %ymm0
	vperm2i128	$32, %ymm0, %ymm2, %ymm1
	vperm2i128	$49, %ymm0, %ymm2, %ymm0
	vmovdqu	%ymm1, -64(%rax)
	vmovdqu	%ymm0, -32(%rax)
	cmpl	%ecx, %esi
	jb	.L730
	cmpl	%ebx, %edx
	je	.L733
.L729:
	leal	0(,%rdx,4), %eax
	cltq
	addq	%r12, %rax
.L732:
	movzbl	1(%rax), %edi
	addl	$1, %edx
	addq	$4, %rax
	movzbl	-2(%rax), %esi
	movzbl	-1(%rax), %ecx
	movzbl	-4(%rax), %r8d
	movb	%dil, -2(%rax)
	movb	%sil, -3(%rax)
	movb	%cl, -4(%rax)
	movb	%r8b, -1(%rax)
	cmpl	%edx, %ebx
	jg	.L732
.L733:
	cmpb	$0, -4194380(%rbp)
	jne	.L735
.L734:
	leaq	2097152(,%rbx,4), %rax
	jmp	.L816
	.p2align 4,,10
	.p2align 3
.L822:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	0(%r13), %xmm0, %xmm0
	vmovsd	.LC20(%rip), %xmm11
	vmovsd	.LC21(%rip), %xmm12
	vcvtss2sd	8(%r13), %xmm1, %xmm1
	leaq	-4194352(%rbp), %rax
	vmovsd	.LC22(%rip), %xmm10
	vmulsd	%xmm11, %xmm0, %xmm0
	vmovapd	.LC24(%rip), %ymm6
	vmulsd	%xmm12, %xmm1, %xmm1
	vmovapd	.LC25(%rip), %ymm7
	leaq	-4194352(%rbp), %rdi
	leaq	4(%rax), %rdx
	leaq	228(%rdi), %rcx
	leaq	8(%r13), %rax
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%r13), %xmm1, %xmm1
	vmulsd	%xmm10, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
.L634:
	vmovups	32(%rax), %ymm8
	addq	$32, %rdx
	addq	$64, %rax
	vmovups	-64(%rax), %ymm1
	vshufps	$136, %ymm8, %ymm1, %ymm2
	vshufps	$221, %ymm8, %ymm1, %ymm1
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm5
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm5, %ymm5
	vmovups	-48(%rax), %ymm0
	vshufps	$136, -16(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-56(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vshufps	$136, -24(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm4
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovups	-72(%rax), %ymm4
	vshufps	$136, -40(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm0
	vshufps	$68, %ymm0, %ymm4, %ymm9
	vshufps	$238, %ymm0, %ymm4, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm4
	vinsertf128	$1, %xmm0, %ymm9, %ymm0
	vshufps	$68, %ymm4, %ymm1, %ymm8
	vshufps	$238, %ymm4, %ymm1, %ymm4
	vcvtps2pd	%xmm0, %ymm1
	vinsertf128	$1, %xmm4, %ymm8, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm5, %ymm8
	vmulpd	.LC23(%rip), %ymm1, %ymm1
	vmulpd	%ymm6, %ymm8, %ymm8
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm8, %ymm1, %ymm1
	vmulpd	.LC23(%rip), %ymm0, %ymm0
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	%ymm6, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm8
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm6, %ymm8, %ymm8
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm8, %ymm1, %ymm1
	vmulpd	%ymm6, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm8
	vmulpd	%ymm7, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm0, %ymm0
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm7, %ymm2, %ymm2
	vsubpd	%ymm8, %ymm1, %ymm1
	vsubpd	%ymm2, %ymm0, %ymm0
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm4, %ymm0
	vmovups	%ymm0, -32(%rdx)
	cmpq	%rcx, %rdx
	jne	.L634
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	456(%r13), %xmm5, %xmm5
	vcvtss2sd	448(%r13), %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	464(%r13), %xmm7, %xmm7
	vxorpd	%xmm4, %xmm4, %xmm4
	vmovsd	.LC27(%rip), %xmm0
	vcvtss2sd	472(%r13), %xmm4, %xmm4
	vmovss	468(%r13), %xmm8
	vmulsd	%xmm6, %xmm5, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm7, %xmm3
	vmulsd	%xmm0, %xmm5, %xmm5
	vmulsd	%xmm0, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm10, %xmm4, %xmm2
	vaddsd	%xmm5, %xmm3, %xmm5
	vaddsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm2, %xmm1, %xmm1
	vmovss	460(%r13), %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	480(%r13), %xmm2, %xmm2
	vmulsd	%xmm10, %xmm2, %xmm3
	vmovss	%xmm1, -4194124(%rbp)
	vmulsd	%xmm6, %xmm4, %xmm1
	vmulsd	%xmm0, %xmm4, %xmm4
	vaddsd	%xmm1, %xmm5, %xmm5
	vaddsd	%xmm7, %xmm1, %xmm1
	vsubsd	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm8, %xmm5
	vmulsd	%xmm6, %xmm2, %xmm8
	vmovss	%xmm5, -4194120(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	488(%r13), %xmm5, %xmm5
	vmulsd	%xmm10, %xmm5, %xmm7
	vmulsd	%xmm6, %xmm5, %xmm9
	vaddsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm7, %xmm1, %xmm1
	vmovss	476(%r13), %xmm7
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vaddsd	%xmm4, %xmm8, %xmm7
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	496(%r13), %xmm4, %xmm4
	vmovss	%xmm1, -4194116(%rbp)
	vmulsd	%xmm10, %xmm4, %xmm1
	vaddsd	%xmm9, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm7, %xmm7
	vmovss	484(%r13), %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm1, -4194112(%rbp)
	vmulsd	%xmm0, %xmm2, %xmm1
	vmulsd	%xmm6, %xmm4, %xmm0
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	504(%r13), %xmm6, %xmm6
	vmulsd	%xmm10, %xmm6, %xmm10
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	492(%r13), %xmm1
	vsubsd	%xmm10, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm11, %xmm5, %xmm1
	vmovss	%xmm0, -4194108(%rbp)
	vmulsd	%xmm12, %xmm4, %xmm0
	vsubsd	%xmm1, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm1
	vmulsd	%xmm11, %xmm6, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	500(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vmulsd	%xmm1, %xmm6, %xmm6
	vmovss	%xmm0, -4194104(%rbp)
	vmulsd	.LC29(%rip), %xmm5, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm1, %xmm4, %xmm0
	vmovss	508(%r13), %xmm1
	vsubsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194100(%rbp)
	vmovups	0(%r13), %ymm0
	vshufps	$136, 32(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm7
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	64(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm7, %ymm7
	vshufps	$136, 96(%r13), %ymm0, %ymm0
	vmovaps	%ymm7, -2097200(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm6
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm6, %ymm6
	vmovaps	%ymm6, -2097168(%rbp)
	vmovups	128(%r13), %ymm0
	vshufps	$136, 160(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm5
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	192(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm5, %ymm5
	vshufps	$136, 224(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm4
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	256(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	vshufps	$136, 288(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm3
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	320(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 352(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	384(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm2
	vshufps	$136, 416(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm8
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovss	448(%r13), %xmm0
	vinsertf128	$1, %xmm1, %ymm8, %ymm1
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	%ymm4, -2097104(%rbp)
	vmovss	%xmm0, -2096976(%rbp)
	vmovss	456(%r13), %xmm0
	vmovaps	%ymm3, -2097072(%rbp)
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	464(%r13), %xmm0
	vmovaps	%ymm2, -2097040(%rbp)
	vmovss	%xmm0, -2096968(%rbp)
	vmovss	472(%r13), %xmm0
	vmovaps	%ymm1, -2097008(%rbp)
	vmovss	%xmm0, -2096964(%rbp)
	vmovss	480(%r13), %xmm0
	vmovss	%xmm0, -2096960(%rbp)
	vmovss	488(%r13), %xmm0
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	496(%r13), %xmm0
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	504(%r13), %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	testb	%r14b, %r14b
	je	.L637
	vmovapd	.LC31(%rip), %ymm0
	vcvtps2pd	%xmm7, %ymm10
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmovaps	-4194352(%rbp), %ymm8
	vcvtps2pd	%xmm8, %ymm9
	vextractf128	$0x1, %ymm8, %xmm8
	vmulpd	%ymm0, %ymm9, %ymm9
	vcvtps2pd	%xmm8, %ymm8
	vaddpd	%ymm9, %ymm10, %ymm9
	vmulpd	%ymm0, %ymm8, %ymm8
	vaddpd	%ymm8, %ymm7, %ymm7
	vcvtpd2psy	%ymm9, %xmm9
	vcvtpd2psy	%ymm7, %xmm7
	vinsertf128	$0x1, %xmm7, %ymm9, %ymm7
	vcvtps2pd	%xmm6, %ymm9
	vmovaps	%ymm7, -2097200(%rbp)
	vmovaps	-4194320(%rbp), %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vcvtps2pd	%xmm7, %ymm8
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm8, %ymm9, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vaddpd	%ymm7, %ymm6, %ymm6
	vcvtps2pd	%xmm5, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vmovaps	%ymm6, -2097168(%rbp)
	vmovaps	-4194288(%rbp), %ymm6
	vcvtps2pd	%xmm6, %ymm8
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm8, %ymm7, %ymm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vaddpd	%ymm6, %ymm5, %ymm5
	vcvtpd2psy	%ymm7, %xmm6
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm6, %ymm5
	vcvtps2pd	%xmm4, %ymm6
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	-4194256(%rbp), %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm5, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm0, %ymm7, %ymm7
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm7, %ymm6, %ymm6
	vmulpd	%ymm0, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vcvtpd2psy	%ymm6, %xmm5
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm5, %ymm4
	vcvtps2pd	%xmm3, %ymm5
	vmovaps	%ymm4, -2097104(%rbp)
	vmovaps	-4194224(%rbp), %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm4, %ymm6
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm4, %ymm4
	vaddpd	%ymm6, %ymm5, %ymm5
	vmulpd	%ymm0, %ymm4, %ymm4
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtpd2psy	%ymm5, %xmm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm4, %ymm3
	vcvtps2pd	%xmm2, %ymm4
	vmovaps	%ymm3, -2097072(%rbp)
	vmovaps	-4194192(%rbp), %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm5, %ymm4, %ymm4
	vmulpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm4, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmovaps	%ymm2, -2097040(%rbp)
	vmovaps	-4194160(%rbp), %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm3, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	vmovaps	-4194128(%rbp), %ymm2
	vmovaps	%ymm1, -2097008(%rbp)
	vmovaps	-2096976(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2096976(%rbp)
.L637:
	movl	$256, %edx
	movq	%r13, %rdi
	vmovapd	%ymm15, -4194448(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovaps	%ymm14, -4194416(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	vmovaps	-4194416(%rbp), %ymm14
	vmovapd	-4194448(%rbp), %ymm15
	jmp	.L636
.L824:
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	0(%r13), %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	movabsq	$4599301119452119040, %rsi
	vcvtss2sd	8(%r13), %xmm1, %xmm1
	vmovq	%rsi, %xmm5
	movabsq	$4606619468846596096, %rdi
	vmovsd	.LC22(%rip), %xmm11
	vmovapd	.LC24(%rip), %ymm4
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovq	%rdi, %xmm5
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovq	%rsi, %xmm5
	leaq	-4194352(%rbp), %rax
	leaq	-4194352(%rbp), %rcx
	leaq	4(%rax), %rdx
	addq	$228, %rcx
	leaq	8(%r13), %rax
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%r13), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovapd	.LC25(%rip), %ymm5
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
.L644:
	vmovups	32(%rax), %ymm3
	addq	$32, %rdx
	addq	$64, %rax
	vmovups	-64(%rax), %ymm8
	vshufps	$136, %ymm3, %ymm8, %ymm2
	vshufps	$221, %ymm3, %ymm8, %ymm8
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm1
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vmovups	-48(%rax), %ymm2
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vshufps	$136, -16(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm6
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vmovups	-56(%rax), %ymm2
	vinsertf128	$1, %xmm0, %ymm6, %ymm6
	vshufps	$136, -24(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm7
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vmovups	-72(%rax), %ymm2
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vshufps	$136, -40(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm7
	vshufps	$68, %ymm7, %ymm2, %ymm9
	vshufps	$238, %ymm7, %ymm2, %ymm7
	vperm2f128	$3, %ymm8, %ymm8, %ymm2
	vinsertf128	$1, %xmm7, %ymm9, %ymm7
	vshufps	$68, %ymm2, %ymm8, %ymm3
	vshufps	$238, %ymm2, %ymm8, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm3
	vcvtps2pd	%xmm7, %ymm2
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmulpd	.LC23(%rip), %ymm2, %ymm8
	vmulpd	.LC23(%rip), %ymm7, %ymm7
	vcvtps2pd	%xmm1, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm4, %ymm2, %ymm2
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm2, %ymm8, %ymm8
	vmulpd	%ymm4, %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vaddpd	%ymm1, %ymm7, %ymm7
	vmulpd	%ymm4, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	%ymm4, %ymm1, %ymm1
	vextractf128	$0x1, %ymm6, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm5, %ymm0, %ymm0
	vaddpd	%ymm2, %ymm8, %ymm8
	vcvtps2pd	%xmm6, %ymm2
	vmulpd	%ymm5, %ymm2, %ymm2
	vaddpd	%ymm1, %ymm7, %ymm1
	vsubpd	%ymm2, %ymm8, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmovups	%ymm0, -32(%rdx)
	cmpq	%rcx, %rdx
	jne	.L644
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	456(%r13), %xmm5, %xmm5
	vcvtss2sd	448(%r13), %xmm2, %xmm2
	vmovsd	.LC26(%rip), %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	464(%r13), %xmm1, %xmm1
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovsd	.LC27(%rip), %xmm0
	vcvtss2sd	472(%r13), %xmm3, %xmm3
	vmovss	468(%r13), %xmm8
	vmulsd	%xmm6, %xmm5, %xmm7
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm1, %xmm4
	vmulsd	%xmm0, %xmm5, %xmm5
	vmulsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm2, %xmm2
	vmulsd	%xmm11, %xmm3, %xmm7
	vaddsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm4, %xmm4
	vsubsd	%xmm7, %xmm2, %xmm2
	vmovss	460(%r13), %xmm7
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm7, %xmm2
	vmulsd	%xmm6, %xmm3, %xmm7
	vmulsd	%xmm0, %xmm3, %xmm3
	vmovss	%xmm2, -4194124(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	480(%r13), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm5
	vaddsd	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm7, %xmm1, %xmm1
	vmovss	476(%r13), %xmm7
	vsubsd	%xmm5, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm8, %xmm4
	vmulsd	%xmm6, %xmm2, %xmm8
	vmovss	%xmm4, -4194120(%rbp)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	488(%r13), %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm9
	vaddsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm9, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm4, %xmm9
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vaddsd	%xmm3, %xmm8, %xmm7
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	496(%r13), %xmm3, %xmm3
	vmulsd	%xmm6, %xmm3, %xmm6
	vmovss	%xmm1, -4194116(%rbp)
	vmulsd	%xmm11, %xmm3, %xmm1
	vaddsd	%xmm9, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm7, %xmm7
	vmovss	484(%r13), %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vmovq	%rsi, %xmm7
	vmovss	%xmm1, -4194112(%rbp)
	vmulsd	%xmm0, %xmm2, %xmm1
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	504(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm11
	vmovss	492(%r13), %xmm6
	vsubsd	%xmm11, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmulsd	%xmm7, %xmm4, %xmm6
	vmovss	%xmm0, -4194108(%rbp)
	vsubsd	%xmm6, %xmm5, %xmm6
	vmovq	%rdi, %xmm5
	vmulsd	%xmm5, %xmm3, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm5
	vmulsd	%xmm7, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmovss	500(%r13), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm0, -4194104(%rbp)
	vmulsd	.LC29(%rip), %xmm4, %xmm0
	vmovsd	.LC30(%rip), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm3, %xmm0
	vsubsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	508(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194100(%rbp)
	vmovups	0(%r13), %ymm0
	vshufps	$136, 32(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm6
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	64(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm6, %ymm6
	vshufps	$136, 96(%r13), %ymm0, %ymm0
	vmovaps	%ymm6, -2097200(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm9
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm9, %ymm9
	vmovaps	%ymm9, -2097168(%rbp)
	vmovups	128(%r13), %ymm0
	vshufps	$136, 160(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm5
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	192(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm5, %ymm5
	vshufps	$136, 224(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm4
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	256(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	vshufps	$136, 288(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm3
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	320(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 352(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	384(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm2
	vshufps	$136, 416(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm7
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovss	448(%r13), %xmm0
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	%ymm4, -2097104(%rbp)
	vmovss	%xmm0, -2096976(%rbp)
	vmovss	456(%r13), %xmm0
	vmovaps	%ymm3, -2097072(%rbp)
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	464(%r13), %xmm0
	vmovaps	%ymm2, -2097040(%rbp)
	vmovss	%xmm0, -2096968(%rbp)
	vmovss	472(%r13), %xmm0
	vmovaps	%ymm1, -2097008(%rbp)
	vmovss	%xmm0, -2096964(%rbp)
	vmovss	480(%r13), %xmm0
	vmovss	%xmm0, -2096960(%rbp)
	vmovss	488(%r13), %xmm0
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	496(%r13), %xmm0
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	504(%r13), %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	testb	%r14b, %r14b
	je	.L647
	vmovapd	.LC31(%rip), %ymm0
	vcvtps2pd	%xmm6, %ymm11
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vmovaps	-4194352(%rbp), %ymm7
	vcvtps2pd	%xmm7, %ymm8
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm8, %ymm11, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vaddpd	%ymm7, %ymm6, %ymm6
	vmovaps	-4194320(%rbp), %ymm7
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vcvtps2pd	%xmm9, %ymm8
	vmovaps	%ymm6, -2097200(%rbp)
	vcvtps2pd	%xmm7, %ymm6
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm6, %ymm8, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vextractf128	$0x1, %ymm9, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm7, %ymm6, %ymm6
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vcvtps2pd	%xmm5, %ymm8
	vmovaps	%ymm6, -2097168(%rbp)
	vmovaps	-4194288(%rbp), %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vcvtps2pd	%xmm6, %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm0, %ymm7, %ymm7
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm7, %ymm8, %ymm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vaddpd	%ymm6, %ymm5, %ymm5
	vcvtpd2psy	%ymm7, %xmm7
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm7, %ymm5
	vcvtps2pd	%xmm4, %ymm7
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	-4194256(%rbp), %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm5, %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm6, %ymm7, %ymm6
	vmulpd	%ymm0, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vcvtpd2psy	%ymm6, %xmm6
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm6, %ymm4
	vcvtps2pd	%xmm3, %ymm6
	vmovaps	%ymm4, -2097104(%rbp)
	vmovaps	-4194224(%rbp), %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm4, %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm4, %ymm4
	vaddpd	%ymm5, %ymm6, %ymm5
	vmulpd	%ymm0, %ymm4, %ymm4
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vcvtpd2psy	%ymm5, %xmm5
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm5, %ymm3
	vmovaps	%ymm3, -2097072(%rbp)
	vmovaps	-4194192(%rbp), %ymm3
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm5, %ymm4, %ymm4
	vmulpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm4, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmovaps	%ymm2, -2097040(%rbp)
	vmovaps	-4194160(%rbp), %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm3, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	vmovaps	-4194128(%rbp), %ymm2
	vmovaps	%ymm1, -2097008(%rbp)
	vmovaps	-2096976(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2096976(%rbp)
.L647:
	movl	$256, %edx
	movq	%r13, %rdi
	vmovapd	%ymm15, -4194544(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovaps	%ymm14, -4194512(%rbp)
	vmovapd	%ymm10, -4194480(%rbp)
	vmovapd	%ymm13, -4194448(%rbp)
	vmovaps	%ymm12, -4194416(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	vmovaps	-4194416(%rbp), %ymm12
	vmovapd	-4194448(%rbp), %ymm13
	vmovapd	-4194480(%rbp), %ymm10
	vmovaps	-4194512(%rbp), %ymm14
	vmovapd	-4194544(%rbp), %ymm15
	jmp	.L646
.L826:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	0(%r13), %xmm0, %xmm0
	vcvtss2sd	8(%r13), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm10
	leaq	-4194352(%rbp), %r10
	xorl	%eax, %eax
	movabsq	$4606619468846596096, %rdi
	vmovsd	.LC22(%rip), %xmm11
	leaq	8(%r13), %rsi
	vmovq	%rdi, %xmm5
	vmovapd	.LC24(%rip), %ymm4
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovapd	.LC25(%rip), %ymm5
	vmulsd	%xmm10, %xmm0, %xmm0
	leaq	24(%r13), %rcx
	leaq	16(%r13), %rdx
	leaq	4(%r10), %r8
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%r13), %xmm1, %xmm1
	vmulsd	%xmm10, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
.L657:
	vmovups	32(%rsi,%rax,2), %ymm8
	vmovups	(%rsi,%rax,2), %ymm3
	vshufps	$136, %ymm8, %ymm3, %ymm1
	vshufps	$221, %ymm8, %ymm3, %ymm3
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	(%rcx,%rax,2), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vshufps	$136, 32(%rcx,%rax,2), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm7
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm7, %ymm7
	vmovups	(%rdx,%rax,2), %ymm2
	vshufps	$136, 32(%rdx,%rax,2), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm6
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm6, %ymm1
	vmovups	0(%r13,%rax,2), %ymm6
	vshufps	$136, 32(%r13,%rax,2), %ymm6, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm2
	vshufps	$68, %ymm2, %ymm6, %ymm9
	vshufps	$238, %ymm2, %ymm6, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm6
	vinsertf128	$1, %xmm2, %ymm9, %ymm2
	vshufps	$68, %ymm6, %ymm3, %ymm8
	vshufps	$238, %ymm6, %ymm3, %ymm6
	vcvtps2pd	%xmm0, %ymm3
	vinsertf128	$1, %xmm6, %ymm8, %ymm6
	vmulpd	%ymm4, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm8
	vmulpd	%ymm12, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm8, %ymm8
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	%ymm4, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm8, %ymm3
	vcvtps2pd	%xmm7, %ymm8
	vmulpd	%ymm5, %ymm8, %ymm8
	vsubpd	%ymm8, %ymm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm8
	vextractf128	$0x1, %ymm0, %xmm2
	vcvtps2pd	%xmm8, %ymm8
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm12, %ymm8, %ymm8
	vmulpd	%ymm4, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm8, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm1, %ymm0, %ymm1
	vextractf128	$0x1, %ymm7, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm5, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm3, %ymm0
	vsubps	%ymm0, %ymm6, %ymm0
	vmovups	%ymm0, (%r8,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L657
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	456(%r13), %xmm3, %xmm3
	vcvtss2sd	448(%r13), %xmm2, %xmm2
	vmovsd	.LC26(%rip), %xmm7
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	464(%r13), %xmm8, %xmm8
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC27(%rip), %xmm0
	vcvtss2sd	472(%r13), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm3, %xmm4
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm8, %xmm5
	vmulsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm0, %xmm8, %xmm8
	vaddsd	%xmm4, %xmm2, %xmm2
	vmulsd	%xmm11, %xmm1, %xmm4
	vaddsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm2, %xmm2
	vmovss	468(%r13), %xmm5
	vsubsd	%xmm4, %xmm2, %xmm2
	vmovss	460(%r13), %xmm4
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm4, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm4
	vmulsd	%xmm0, %xmm1, %xmm1
	vmovss	%xmm2, -4194124(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	480(%r13), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm6
	vaddsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	488(%r13), %xmm5, %xmm5
	vmulsd	%xmm11, %xmm5, %xmm8
	vmovss	%xmm3, -4194120(%rbp)
	vmulsd	%xmm7, %xmm2, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm1, %xmm3
	vsubsd	%xmm8, %xmm4, %xmm4
	vmovss	476(%r13), %xmm8
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm8, %xmm4
	vmulsd	%xmm7, %xmm5, %xmm8
	vmovss	%xmm4, -4194116(%rbp)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	496(%r13), %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm1
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm1, %xmm3, %xmm3
	vmovss	484(%r13), %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm1, %xmm1
	vmovss	%xmm1, -4194112(%rbp)
	vmulsd	%xmm0, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm4, %xmm0
	vmovq	%rdi, %xmm7
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	504(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm3
	vsubsd	%xmm3, %xmm0, %xmm0
	vmovss	492(%r13), %xmm3
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmovss	500(%r13), %xmm3
	vmovss	%xmm0, -4194108(%rbp)
	vmulsd	%xmm10, %xmm5, %xmm0
	vmulsd	%xmm10, %xmm1, %xmm10
	vsubsd	%xmm0, %xmm6, %xmm6
	vmulsd	%xmm7, %xmm4, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm0
	vaddsd	%xmm10, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmovsd	.LC30(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm1
	vmovss	%xmm0, -4194104(%rbp)
	vmulsd	.LC29(%rip), %xmm5, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm3, %xmm4, %xmm0
	vsubsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	508(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194100(%rbp)
	vmovups	0(%r13), %ymm0
	vshufps	$136, 32(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm7
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	64(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm7, %ymm7
	vshufps	$136, 96(%r13), %ymm0, %ymm0
	vmovaps	%ymm7, -2097200(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm6
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm6, %ymm6
	vmovaps	%ymm6, -2097168(%rbp)
	vmovups	128(%r13), %ymm0
	vshufps	$136, 160(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm5
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	192(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm5, %ymm5
	vshufps	$136, 224(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm4
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	256(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	vshufps	$136, 288(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm3
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	320(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 352(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	384(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm2
	vshufps	$136, 416(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm8
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovss	448(%r13), %xmm0
	vinsertf128	$1, %xmm1, %ymm8, %ymm1
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	%ymm4, -2097104(%rbp)
	vmovss	%xmm0, -2096976(%rbp)
	vmovss	456(%r13), %xmm0
	vmovaps	%ymm3, -2097072(%rbp)
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	464(%r13), %xmm0
	vmovaps	%ymm2, -2097040(%rbp)
	vmovss	%xmm0, -2096968(%rbp)
	vmovss	472(%r13), %xmm0
	vmovaps	%ymm1, -2097008(%rbp)
	vmovss	%xmm0, -2096964(%rbp)
	vmovss	480(%r13), %xmm0
	vmovss	%xmm0, -2096960(%rbp)
	vmovss	488(%r13), %xmm0
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	496(%r13), %xmm0
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	504(%r13), %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	testb	%r14b, %r14b
	je	.L660
	vmovapd	.LC31(%rip), %ymm0
	vcvtps2pd	%xmm7, %ymm10
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmovaps	-4194352(%rbp), %ymm8
	vcvtps2pd	%xmm8, %ymm9
	vextractf128	$0x1, %ymm8, %xmm8
	vmulpd	%ymm0, %ymm9, %ymm9
	vcvtps2pd	%xmm8, %ymm8
	vaddpd	%ymm9, %ymm10, %ymm9
	vmulpd	%ymm0, %ymm8, %ymm8
	vaddpd	%ymm8, %ymm7, %ymm7
	vcvtpd2psy	%ymm9, %xmm9
	vcvtpd2psy	%ymm7, %xmm7
	vinsertf128	$0x1, %xmm7, %ymm9, %ymm7
	vcvtps2pd	%xmm6, %ymm9
	vmovaps	%ymm7, -2097200(%rbp)
	vmovaps	-4194320(%rbp), %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vcvtps2pd	%xmm7, %ymm8
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm8, %ymm9, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vaddpd	%ymm7, %ymm6, %ymm6
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vcvtps2pd	%xmm5, %ymm8
	vmovaps	%ymm6, -2097168(%rbp)
	vmovaps	-4194288(%rbp), %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vcvtps2pd	%xmm6, %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm0, %ymm7, %ymm7
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm7, %ymm8, %ymm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vaddpd	%ymm6, %ymm5, %ymm5
	vcvtpd2psy	%ymm7, %xmm7
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm7, %ymm5
	vcvtps2pd	%xmm4, %ymm7
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	-4194256(%rbp), %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm5, %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm6, %ymm7, %ymm6
	vmulpd	%ymm0, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vcvtpd2psy	%ymm6, %xmm6
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm6, %ymm4
	vmovaps	%ymm4, -2097104(%rbp)
	vmovaps	-4194224(%rbp), %ymm4
	vcvtps2pd	%xmm4, %ymm6
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm4, %ymm4
	vaddpd	%ymm6, %ymm5, %ymm5
	vmulpd	%ymm0, %ymm4, %ymm4
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtpd2psy	%ymm5, %xmm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm4, %ymm3
	vcvtps2pd	%xmm2, %ymm4
	vmovaps	%ymm3, -2097072(%rbp)
	vmovaps	-4194192(%rbp), %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm5, %ymm4, %ymm4
	vmulpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm4, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmovaps	%ymm2, -2097040(%rbp)
	vmovaps	-4194160(%rbp), %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm3, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	vmovaps	-4194128(%rbp), %ymm2
	vmovaps	%ymm1, -2097008(%rbp)
	vmovaps	-2096976(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2096976(%rbp)
.L660:
	movl	$256, %edx
	movq	%r13, %rdi
	vmovapd	%ymm12, -4194512(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovapd	%ymm15, -4194480(%rbp)
	vmovapd	%ymm13, -4194448(%rbp)
	vmovaps	%ymm14, -4194416(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	vmovaps	-4194416(%rbp), %ymm14
	vmovapd	-4194448(%rbp), %ymm13
	vmovapd	-4194480(%rbp), %ymm15
	vmovapd	-4194512(%rbp), %ymm12
	jmp	.L659
.L727:
	cmpb	$0, -4194380(%rbp)
	je	.L734
	testl	%ebx, %ebx
	jle	.L738
.L735:
	xorl	%esi, %esi
	movl	$31, %r8d
	xorl	%edi, %edi
.L737:
	movl	(%r12,%rsi,4), %eax
	movl	%eax, %edx
	andl	$2147483647, %edx
	shrl	$23, %edx
	subl	$112, %edx
	cmpl	$31, %edx
	cmovg	%r8d, %edx
	movl	%edx, %ecx
	sall	$10, %ecx
	testl	%edx, %edx
	movl	%eax, %edx
	cmovle	%edi, %ecx
	andl	$-2147483648, %edx
	andl	$8388607, %eax
	shrl	$16, %edx
	orl	%ecx, %edx
	shrl	$13, %eax
	orl	%edx, %eax
	movw	%ax, (%r12,%rsi,2)
	addq	$1, %rsi
	cmpl	%esi, %ebx
	jg	.L737
.L738:
	leaq	2097152(%rbx,%rbx), %rax
.L816:
	addq	$4194592, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
.L748:
	.cfi_restore_state
	xorl	%edx, %edx
	jmp	.L729
.L728:
	cmpb	$0, -4194380(%rbp)
	jne	.L738
	jmp	.L734
	.cfi_endproc
.LFE1394:
	.size	_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi, .-_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,comdat
.LCOLDE58:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi,comdat
.LHOTE58:
	.section	.rodata.str1.1,"aMS",@progbits,1
.LC59:
	.string	"1.2.7"
.LC60:
	.string	"ZLIB COMPRESSION FAILURE!!"
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,comdat
	.align 2
.LCOLDB61:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,comdat
.LHOTB61:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi
	.type	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi, @function
_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi:
.LFB1401:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movzbl	%dl, %edx
	movzbl	%sil, %esi
	movq	%rdi, %rbx
	subq	$112, %rsp
	.cfi_def_cfa_offset 128
	call	_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi
	movq	%rsp, %rdi
	movl	$14, %ecx
	movl	$-1, %esi
	movq	%rax, %rdx
	xorl	%eax, %eax
	rep; stosq
	leaq	67108896(%rbx), %rax
	movl	%edx, 8(%rsp)
	addq	$136314920, %rbx
	movl	$.LC59, %edx
	movq	%rsp, %rdi
	movq	%rax, (%rsp)
	movl	$69206016, 32(%rsp)
	movq	%rbx, 24(%rsp)
	movb	$112, %cl
	call	deflateInit_
	testl	%eax, %eax
	je	.L837
.L838:
	movl	$.LC60, %edi
	call	puts
	call	abort
	.p2align 4,,10
	.p2align 3
.L837:
	movl	$4, %esi
	movq	%rsp, %rdi
	call	deflate
	cmpl	$1, %eax
	jne	.L838
	movq	40(%rsp), %rbx
	movq	%rsp, %rdi
	call	deflateEnd
	addq	$112, %rsp
	.cfi_def_cfa_offset 16
	movslq	%ebx, %rax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1401:
	.size	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi, .-_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,comdat
.LCOLDE61:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi,comdat
.LHOTE61:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbi,comdat
	.align 2
.LCOLDB62:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbi,comdat
.LHOTB62:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE8compressEfbi
	.type	_ZN24WaveletCompressorGenericILi256EfE8compressEfbi, @function
_ZN24WaveletCompressorGenericILi256EfE8compressEfbi:
.LFB1393:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movq	%rdi, %rax
	pushq	-8(%r10)
	pushq	%rbp
	addq	$24, %rax
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r15
	.cfi_escape 0x10,0xf,0x2,0x76,0x78
	movl	%edx, %r15d
	pushq	%r14
	pushq	%r13
	.cfi_escape 0x10,0xe,0x2,0x76,0x70
	.cfi_escape 0x10,0xd,0x2,0x76,0x68
	leaq	67108888(%rdi), %r13
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x58,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x60
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x50
	movq	%rax, %rbx
	subq	$4194592, %rsp
	movq	%rdi, -4194368(%rbp)
	vmovss	%xmm0, -4194620(%rbp)
	movl	%esi, -4194624(%rbp)
	movq	%rax, -4194360(%rbp)
	movq	%rax, -4194616(%rbp)
.L845:
	leaq	262144(%rbx), %r12
	movq	%rbx, %r14
	.p2align 4,,10
	.p2align 3
.L841:
	movq	%r14, %rdi
	movl	%r15d, %esi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	cmpq	%r12, %r14
	jne	.L841
	leaq	1024(%rbx), %r14
	xorl	%edi, %edi
	movq	%rbx, %rsi
	addl	$1, %edi
	movq	%r14, %r8
	cmpl	$256, %edi
	je	.L1029
.L1032:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L843:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$256, %eax
	jne	.L843
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$256, %edi
	jne	.L1032
.L1029:
	movq	%rbx, %rdi
	jmp	.L842
.L1033:
	addq	$1024, %r14
.L842:
	movl	%r15d, %esi
	call	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	movq	%r14, %rdi
	cmpq	%r14, %r12
	jne	.L1033
	movq	%r12, %rbx
	cmpq	%r12, %r13
	jne	.L845
	movq	-4194368(%rbp), %rax
	xorl	%r10d, %r10d
	addq	$262168, %rax
	movq	%rax, -4194376(%rbp)
	movq	%rax, %r11
.L851:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$256, %r8d
	je	.L847
.L1034:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L848:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$256, %edx
	jne	.L848
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$256, %r8d
	jne	.L1034
.L847:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$256, %r10d
	jne	.L851
	movq	-4194360(%rbp), %r12
.L852:
	leaq	262144(%r12), %rbx
	.p2align 4,,10
	.p2align 3
.L853:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi256ELb1EEEvPfi
	cmpq	%rbx, %r12
	jne	.L853
	cmpq	%r12, %r13
	jne	.L852
	movq	-4194368(%rbp), %rax
	cmpl	$2, %r15d
	movl	%r15d, -4194628(%rbp)
	vmovaps	.LC32(%rip), %ymm14
	leal	-1(%r15), %ebx
	sete	%r14b
	vmovapd	.LC38(%rip), %ymm15
	addq	$33554456, %rax
	movq	%rax, -4194384(%rbp)
	movq	-4194360(%rbp), %rax
	movq	%rax, %r12
.L871:
	movq	%r12, %r13
	movl	$128, %r15d
	.p2align 4,,10
	.p2align 3
.L861:
	cmpl	$1, %ebx
	jbe	.L1035
	vmovups	32(%r13), %ymm4
	leaq	-4194352(%rbp), %rdi
	xorl	%eax, %eax
	movabsq	$4602678819172646912, %rdx
	vmovups	0(%r13), %ymm1
	leaq	-2097200(%rbp), %rsi
	vmovq	%rdx, %xmm5
	movabsq	$4608871268660281344, %rcx
	vmovsd	.LC35(%rip), %xmm13
	vmovq	%rcx, %xmm7
	addq	$4, %rsi
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	leaq	8(%rdi), %r8
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	96(%r13), %ymm4
	addq	$4, %rdi
	vmovups	64(%r13), %ymm1
	vmovsd	.LC34(%rip), %xmm8
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194352(%rbp)
	vmovss	.LC37(%rip), %xmm9
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	160(%r13), %ymm4
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovups	128(%r13), %ymm1
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	224(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194320(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	192(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	288(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194288(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	256(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	352(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194256(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	320(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	416(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194224(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	384(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	480(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194192(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	448(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194160(%rbp)
	vcvtss2sd	-4194352(%rbp), %xmm4, %xmm4
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmulsd	%xmm13, %xmm4, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194344(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm2
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194128(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194348(%rbp), %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm5
	vmovss	4(%r13), %xmm0
	vsubss	0(%r13), %xmm0, %xmm3
	vmulsd	%xmm7, %xmm4, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm0, %xmm0
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm0, %xmm3, %xmm3
	vmulss	%xmm9, %xmm3, %xmm0
	vmovss	%xmm0, -2097200(%rbp)
	.p2align 4,,10
	.p2align 3
.L860:
	vmovups	8(%r13,%rax,2), %ymm10
	vmovups	40(%r13,%rax,2), %ymm3
	vmovups	(%rdi,%rax), %ymm0
	vmovaps	-4194352(%rbp,%rax), %ymm5
	vshufps	$221, %ymm3, %ymm10, %ymm11
	vshufps	$136, %ymm3, %ymm10, %ymm3
	vperm2f128	$3, %ymm11, %ymm11, %ymm4
	vcvtps2pd	%xmm0, %ymm1
	vshufps	$68, %ymm4, %ymm11, %ymm12
	vshufps	$238, %ymm4, %ymm11, %ymm4
	vperm2f128	$3, %ymm3, %ymm3, %ymm11
	vcvtps2pd	%xmm5, %ymm7
	vinsertf128	$1, %xmm4, %ymm12, %ymm12
	vshufps	$68, %ymm11, %ymm3, %ymm4
	vshufps	$238, %ymm11, %ymm3, %ymm11
	vmulpd	.LC39(%rip), %ymm7, %ymm3
	vaddpd	%ymm1, %ymm3, %ymm3
	vmulpd	%ymm15, %ymm7, %ymm7
	vmovups	(%r8,%rax), %ymm2
	vextractf128	$0x1, %ymm5, %xmm5
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm5, %ymm5
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm2, %ymm6
	vaddpd	%ymm7, %ymm1, %ymm1
	vmulpd	%ymm15, %ymm6, %ymm6
	vextractf128	$0x1, %ymm2, %xmm2
	vaddpd	%ymm6, %ymm3, %ymm10
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm15, %ymm2, %ymm2
	vmulpd	.LC39(%rip), %ymm5, %ymm3
	vmulpd	%ymm15, %ymm5, %ymm5
	vaddpd	%ymm0, %ymm3, %ymm3
	vinsertf128	$1, %xmm11, %ymm4, %ymm11
	vaddpd	%ymm5, %ymm0, %ymm0
	vsubpd	%ymm6, %ymm1, %ymm1
	vcvtpd2psy	%ymm10, %xmm10
	vaddpd	%ymm2, %ymm3, %ymm3
	vsubpd	%ymm2, %ymm0, %ymm0
	vsubps	%ymm11, %ymm12, %ymm4
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm10, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vsubps	%ymm0, %ymm4, %ymm4
	vmulps	%ymm14, %ymm4, %ymm4
	vmovups	%ymm4, (%rsi,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L860
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194128(%rbp), %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC40(%rip), %xmm5
	vcvtss2sd	-4194120(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovss	460(%r13), %xmm1
	vsubss	456(%r13), %xmm1, %xmm7
	vmulsd	%xmm5, %xmm0, %xmm1
	vcvtss2sd	-4194124(%rbp), %xmm3, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	leaq	-4194352(%rbp), %rsi
	movq	%r13, %rdi
	vmovapd	%ymm15, -4194448(%rbp)
	vmovaps	%ymm14, -4194416(%rbp)
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmulsd	%xmm8, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm3, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmulsd	%xmm5, %xmm3, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm8, %xmm3, %xmm3
	vcvtss2sd	-4194116(%rbp), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	%xmm8, %xmm1, %xmm7
	vaddsd	%xmm6, %xmm2, %xmm6
	vaddsd	%xmm3, %xmm2, %xmm3
	vmulss	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm7, %xmm6, %xmm6
	vsubsd	%xmm7, %xmm3, %xmm3
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	468(%r13), %xmm0
	vsubss	464(%r13), %xmm0, %xmm10
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmovss	476(%r13), %xmm3
	vsubss	%xmm0, %xmm10, %xmm0
	vsubss	472(%r13), %xmm3, %xmm10
	vmulsd	%xmm5, %xmm2, %xmm3
	vmulss	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm1, %xmm3
	vmovss	%xmm0, -2096968(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194112(%rbp), %xmm0, %xmm0
	vmulsd	%xmm8, %xmm0, %xmm6
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm6, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm4, %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194108(%rbp), %xmm4, %xmm4
	vmulsd	%xmm8, %xmm4, %xmm3
	vaddsd	%xmm6, %xmm4, %xmm6
	vsubss	%xmm2, %xmm10, %xmm2
	vmulss	%xmm9, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm2, -2096964(%rbp)
	vmovss	484(%r13), %xmm2
	vsubss	480(%r13), %xmm2, %xmm10
	vmulsd	%xmm5, %xmm1, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm7, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194104(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm7
	vsubss	%xmm1, %xmm10, %xmm1
	vmulss	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm7, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm1, -2096960(%rbp)
	vmovss	492(%r13), %xmm1
	vsubss	488(%r13), %xmm1, %xmm10
	vmulsd	%xmm5, %xmm0, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm6, %xmm1, %xmm0
	vmulsd	%xmm5, %xmm4, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-4194100(%rbp), %xmm6, %xmm6
	vmulsd	%xmm8, %xmm6, %xmm8
	vaddsd	%xmm3, %xmm2, %xmm5
	vsubss	%xmm0, %xmm10, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm4
	vmulss	%xmm9, %xmm0, %xmm0
	vaddsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm8, %xmm5, %xmm8
	vxorps	%xmm5, %xmm5, %xmm5
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	500(%r13), %xmm0
	vsubss	496(%r13), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm8, %xmm5, %xmm5
	vsubss	%xmm5, %xmm4, %xmm4
	vmovq	%rdx, %xmm5
	movl	$256, %edx
	vmulsd	%xmm5, %xmm2, %xmm2
	vmovq	%rcx, %xmm5
	vsubss	%xmm4, %xmm0, %xmm0
	vmulss	%xmm9, %xmm0, %xmm0
	vsubsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm2, %xmm2
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	508(%r13), %xmm0
	vsubss	504(%r13), %xmm0, %xmm7
	vmulsd	%xmm5, %xmm6, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm13, %xmm6, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulss	%xmm9, %xmm0, %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-2097200(%rbp), %rsi
	call	memcpy
	vmovapd	-4194448(%rbp), %ymm15
	vmovaps	-4194416(%rbp), %ymm14
.L858:
	addq	$1024, %r13
	subl	$1, %r15d
	jne	.L861
	leaq	1024(%r12), %r8
	xorl	%edi, %edi
	movq	%r12, %rsi
	addl	$1, %edi
	cmpl	$128, %edi
	je	.L958
.L1036:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L863:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$128, %eax
	jne	.L863
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$128, %edi
	jne	.L1036
.L958:
	vmovaps	.LC32(%rip), %ymm12
	movq	%r12, %r13
	movl	$128, %r15d
	vmovapd	.LC38(%rip), %ymm13
	vmovapd	.LC39(%rip), %ymm10
	.p2align 4,,10
	.p2align 3
.L862:
	cmpl	$1, %ebx
	jbe	.L1037
	vmovups	32(%r13), %ymm4
	xorl	%eax, %eax
	movabsq	$4603804719079489536, %rcx
	movabsq	$4602678819172646912, %rdx
	vmovups	0(%r13), %ymm1
	leaq	-4194352(%rbp), %rsi
	vmovq	%rdx, %xmm5
	movabsq	$4608871268660281344, %rdi
	vmovsd	.LC34(%rip), %xmm7
	leaq	8(%rsi), %r9
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	leaq	4(%rsi), %r8
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	96(%r13), %ymm4
	vmovsd	%xmm7, -4194448(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovups	64(%r13), %ymm1
	addq	$4, %rsi
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194352(%rbp)
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vmovups	160(%r13), %ymm4
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	128(%r13), %ymm1
	vmulps	%ymm12, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vmovaps	%ymm0, -4194320(%rbp)
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vmovups	224(%r13), %ymm4
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	192(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	288(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194288(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	256(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	352(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194256(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	320(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	416(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194224(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	384(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	480(%r13), %ymm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194192(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	448(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194160(%rbp)
	vcvtss2sd	-4194352(%rbp), %xmm4, %xmm4
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194344(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovq	%rcx, %xmm7
	vmulsd	%xmm7, %xmm4, %xmm1
	vmovq	%rdi, %xmm7
	vmulps	%ymm12, %ymm0, %ymm0
	vmovaps	%ymm0, -4194128(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194348(%rbp), %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm5
	vmovss	4(%r13), %xmm0
	vsubss	0(%r13), %xmm0, %xmm3
	vmulsd	%xmm7, %xmm4, %xmm0
	vmovss	.LC37(%rip), %xmm7
	vmovss	%xmm7, -4194416(%rbp)
	vaddsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm0, %xmm0
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097200(%rbp)
	.p2align 4,,10
	.p2align 3
.L870:
	vmovups	(%r8,%rax), %ymm3
	vmovups	8(%r13,%rax,2), %ymm9
	vmovups	(%r9,%rax), %ymm2
	vcvtps2pd	%xmm3, %ymm1
	vextractf128	$0x1, %ymm3, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmovups	40(%r13,%rax,2), %ymm6
	vmovaps	-4194352(%rbp,%rax), %ymm4
	vcvtps2pd	%xmm2, %ymm7
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vshufps	$221, %ymm6, %ymm9, %ymm11
	vmulpd	%ymm13, %ymm2, %ymm5
	vshufps	$136, %ymm6, %ymm9, %ymm6
	vperm2f128	$3, %ymm11, %ymm11, %ymm2
	vcvtps2pd	%xmm4, %ymm8
	vshufps	$68, %ymm2, %ymm11, %ymm3
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vshufps	$238, %ymm2, %ymm11, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm11
	vmulpd	%ymm13, %ymm7, %ymm7
	vperm2f128	$3, %ymm6, %ymm6, %ymm2
	vshufps	$68, %ymm2, %ymm6, %ymm3
	vshufps	$238, %ymm2, %ymm6, %ymm2
	vmulpd	%ymm10, %ymm4, %ymm6
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddpd	%ymm0, %ymm6, %ymm6
	vmulpd	%ymm13, %ymm4, %ymm4
	vsubps	%ymm2, %ymm11, %ymm3
	vmulpd	%ymm10, %ymm8, %ymm2
	vmulpd	%ymm13, %ymm8, %ymm8
	vaddpd	%ymm1, %ymm2, %ymm2
	vaddpd	%ymm4, %ymm0, %ymm0
	vaddpd	%ymm8, %ymm1, %ymm1
	vaddpd	%ymm7, %ymm2, %ymm2
	vaddpd	%ymm5, %ymm6, %ymm6
	vsubpd	%ymm7, %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmulps	%ymm12, %ymm0, %ymm0
	vmovups	%ymm0, (%rsi,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L870
	vmovaps	%ymm14, -4194576(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194128(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	-4194448(%rbp), %xmm14
	vcvtss2sd	-4194120(%rbp), %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194124(%rbp), %xmm0, %xmm0
	vmovsd	.LC40(%rip), %xmm4
	vmovapd	%ymm15, -4194608(%rbp)
	leaq	-4194352(%rbp), %rsi
	vmulsd	%xmm14, %xmm3, %xmm7
	vmovss	-4194416(%rbp), %xmm15
	vmovapd	%ymm10, -4194544(%rbp)
	vmulsd	%xmm4, %xmm3, %xmm6
	vmovapd	%ymm13, -4194512(%rbp)
	vmovss	460(%r13), %xmm1
	vmulsd	%xmm14, %xmm2, %xmm5
	vmovaps	%ymm12, -4194480(%rbp)
	vsubss	456(%r13), %xmm1, %xmm1
	vmulsd	%xmm14, %xmm0, %xmm8
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm5, %xmm7, %xmm7
	vaddsd	%xmm5, %xmm6, %xmm6
	vaddsd	%xmm8, %xmm2, %xmm8
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm3
	vmovss	468(%r13), %xmm6
	vsubss	464(%r13), %xmm6, %xmm9
	vmulsd	%xmm4, %xmm0, %xmm6
	vsubss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm15, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm2, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovss	%xmm1, -2096972(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194116(%rbp), %xmm1, %xmm1
	vmulsd	%xmm14, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm1, %xmm2
	vaddsd	%xmm3, %xmm6, %xmm6
	vsubsd	%xmm3, %xmm8, %xmm8
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm6, %xmm7
	vaddsd	%xmm1, %xmm5, %xmm8
	vsubss	%xmm7, %xmm9, %xmm0
	vmovss	476(%r13), %xmm7
	vsubss	472(%r13), %xmm7, %xmm7
	vmulss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -2096968(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194112(%rbp), %xmm0, %xmm0
	vmulsd	%xmm14, %xmm0, %xmm6
	vaddsd	%xmm3, %xmm0, %xmm9
	vaddsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm2, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm8
	vxorps	%xmm1, %xmm1, %xmm1
	vsubss	%xmm5, %xmm7, %xmm2
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194108(%rbp), %xmm5, %xmm5
	vmovss	484(%r13), %xmm7
	vsubss	480(%r13), %xmm7, %xmm7
	vaddsd	%xmm0, %xmm8, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm0
	vmulss	%xmm15, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm5, %xmm4
	vmovss	%xmm2, -2096964(%rbp)
	vmulsd	%xmm14, %xmm5, %xmm2
	vaddsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm8, %xmm8
	vsubsd	%xmm2, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm1, %xmm1
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm1, %xmm3
	vsubss	%xmm3, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-4194104(%rbp), %xmm7, %xmm7
	vmulsd	%xmm14, %xmm7, %xmm8
	vmovss	492(%r13), %xmm3
	vsubss	488(%r13), %xmm3, %xmm3
	vmulss	%xmm15, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm0, %xmm0
	vmovss	%xmm1, -2096960(%rbp)
	vaddsd	%xmm6, %xmm5, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm7, %xmm6
	vaddsd	%xmm7, %xmm4, %xmm5
	vsubsd	%xmm8, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194100(%rbp), %xmm3, %xmm3
	vmulsd	%xmm14, %xmm3, %xmm1
	vmulss	%xmm15, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm5, %xmm5
	vsubsd	%xmm1, %xmm6, %xmm1
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	500(%r13), %xmm0
	vsubss	496(%r13), %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovq	%rdx, %xmm5
	movl	$256, %edx
	vmulsd	%xmm5, %xmm7, %xmm7
	vsubss	%xmm1, %xmm0, %xmm0
	vmovq	%rdi, %xmm1
	movq	%r13, %rdi
	vmulsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm7, %xmm2, %xmm2
	vmulss	%xmm15, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm7, %xmm4
	vmovq	%rcx, %xmm7
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm3, %xmm2
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	508(%r13), %xmm0
	vsubss	504(%r13), %xmm0, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm4, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-2097200(%rbp), %rsi
	call	memcpy
	vmovapd	-4194608(%rbp), %ymm15
	vmovaps	-4194576(%rbp), %ymm14
	vmovapd	-4194544(%rbp), %ymm10
	vmovapd	-4194512(%rbp), %ymm13
	vmovaps	-4194480(%rbp), %ymm12
.L868:
	addq	$1024, %r13
	subl	$1, %r15d
	jne	.L862
	addq	$262144, %r12
	cmpq	-4194384(%rbp), %r12
	jne	.L871
	movl	-4194628(%rbp), %r15d
	xorl	%r10d, %r10d
	movq	-4194376(%rbp), %r11
.L872:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$128, %r8d
	je	.L873
.L1038:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L874:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$128, %edx
	jne	.L874
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$128, %r8d
	jne	.L1038
.L873:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$128, %r10d
	jne	.L872
	movq	-4194360(%rbp), %rax
	vmovaps	.LC32(%rip), %ymm14
	vmovapd	.LC38(%rip), %ymm13
	vmovapd	.LC39(%rip), %ymm15
	vmovapd	.LC23(%rip), %ymm12
	movq	%rax, -4194576(%rbp)
.L877:
	movq	-4194576(%rbp), %r13
	movl	$128, %r12d
	.p2align 4,,10
	.p2align 3
.L884:
	cmpl	$1, %ebx
	jbe	.L1039
	vmovups	32(%r13), %ymm4
	vxorpd	%xmm5, %xmm5, %xmm5
	leaq	8(%r13), %rdx
	xorl	%eax, %eax
	vmovups	0(%r13), %ymm1
	leaq	-4194352(%rbp), %r11
	movabsq	$4602678819172646912, %rsi
	movabsq	$4603804719079489536, %rcx
	vmovsd	.LC34(%rip), %xmm11
	leaq	8(%r11), %r10
	vmovq	%rsi, %xmm7
	movabsq	$4608871268660281344, %rdi
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	96(%r13), %ymm4
	vmovups	64(%r13), %ymm1
	leaq	4(%r11), %r9
	leaq	-2097200(%rbp), %r11
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vmulps	%ymm14, %ymm0, %ymm0
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vmovaps	%ymm0, -4194352(%rbp)
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vcvtss2sd	-4194352(%rbp), %xmm5, %xmm5
	leaq	4(%r11), %r8
	vmovups	160(%r13), %ymm4
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmovups	128(%r13), %ymm1
	vmulps	%ymm14, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vmovaps	%ymm0, -4194320(%rbp)
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vmovups	224(%r13), %ymm4
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	192(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	288(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194288(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	256(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	352(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194256(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	320(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	416(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194224(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	384(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmovups	480(%r13), %ymm4
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194192(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	448(%r13), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vshufps	$136, %ymm4, %ymm1, %ymm3
	vshufps	$221, %ymm4, %ymm1, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194160(%rbp)
	vshufps	$68, %ymm2, %ymm3, %ymm0
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm0, %ymm0
	vmulps	%ymm14, %ymm0, %ymm0
	vmovaps	%ymm0, -4194128(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194348(%rbp), %xmm0, %xmm0
	vmulsd	%xmm7, %xmm0, %xmm4
	vmovq	%rcx, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194344(%rbp), %xmm0, %xmm0
	vmulsd	%xmm7, %xmm5, %xmm1
	vmovq	%rdi, %xmm7
	vmulsd	%xmm11, %xmm0, %xmm3
	vmovss	4(%r13), %xmm0
	vsubss	0(%r13), %xmm0, %xmm2
	vmulsd	%xmm7, %xmm5, %xmm0
	vmovss	.LC37(%rip), %xmm7
	vaddsd	%xmm4, %xmm1, %xmm1
	vmovss	%xmm7, -4194416(%rbp)
	vsubsd	%xmm4, %xmm0, %xmm0
	vsubsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm3, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097200(%rbp)
	.p2align 4,,10
	.p2align 3
.L883:
	vmovups	(%r9,%rax), %ymm3
	vmovups	(%rdx,%rax,2), %ymm9
	vmovups	32(%rdx,%rax,2), %ymm6
	vcvtps2pd	%xmm3, %ymm1
	vextractf128	$0x1, %ymm3, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmovups	(%r10,%rax), %ymm2
	vmovaps	-4194352(%rbp,%rax), %ymm4
	vshufps	$221, %ymm6, %ymm9, %ymm10
	vshufps	$136, %ymm6, %ymm9, %ymm6
	vcvtps2pd	%xmm2, %ymm7
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm13, %ymm2, %ymm5
	vcvtps2pd	%xmm4, %ymm8
	vperm2f128	$3, %ymm10, %ymm10, %ymm2
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vshufps	$68, %ymm2, %ymm10, %ymm3
	vshufps	$238, %ymm2, %ymm10, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm10
	vperm2f128	$3, %ymm6, %ymm6, %ymm2
	vshufps	$68, %ymm2, %ymm6, %ymm3
	vshufps	$238, %ymm2, %ymm6, %ymm2
	vmulpd	%ymm15, %ymm4, %ymm6
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddpd	%ymm0, %ymm6, %ymm6
	vmulpd	%ymm13, %ymm4, %ymm4
	vmulpd	%ymm13, %ymm7, %ymm7
	vsubps	%ymm2, %ymm10, %ymm3
	vmulpd	%ymm15, %ymm8, %ymm2
	vmulpd	%ymm13, %ymm8, %ymm8
	vaddpd	%ymm1, %ymm2, %ymm2
	vaddpd	%ymm4, %ymm0, %ymm0
	vaddpd	%ymm8, %ymm1, %ymm1
	vaddpd	%ymm7, %ymm2, %ymm2
	vaddpd	%ymm5, %ymm6, %ymm6
	vsubpd	%ymm7, %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmulps	%ymm14, %ymm0, %ymm0
	vmovups	%ymm0, (%r8,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L883
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194128(%rbp), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm6
	vmovsd	.LC40(%rip), %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194120(%rbp), %xmm2, %xmm2
	vmovss	460(%r13), %xmm1
	vmulsd	%xmm11, %xmm2, %xmm5
	vsubss	456(%r13), %xmm1, %xmm7
	vmulsd	%xmm4, %xmm3, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194124(%rbp), %xmm0, %xmm0
	vmovaps	%ymm14, -4194448(%rbp)
	vmovss	-4194416(%rbp), %xmm14
	vmulsd	%xmm11, %xmm0, %xmm8
	movl	$256, %edx
	vmovapd	%ymm12, -4194544(%rbp)
	vaddsd	%xmm6, %xmm0, %xmm3
	vmovapd	%ymm15, -4194512(%rbp)
	vmovapd	%ymm13, -4194480(%rbp)
	vaddsd	%xmm0, %xmm1, %xmm1
	vsubsd	%xmm5, %xmm3, %xmm6
	vxorps	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm2, %xmm8
	vaddsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm6, %xmm3, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm3
	vmovss	468(%r13), %xmm1
	vsubss	464(%r13), %xmm1, %xmm1
	vsubss	%xmm3, %xmm7, %xmm3
	vmulsd	%xmm4, %xmm0, %xmm7
	vmulss	%xmm14, %xmm3, %xmm3
	vaddsd	%xmm7, %xmm2, %xmm7
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovss	%xmm3, -2096972(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194116(%rbp), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm6
	vaddsd	%xmm3, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm7, %xmm7
	vsubsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm7, %xmm0
	vmovss	476(%r13), %xmm8
	vsubss	472(%r13), %xmm8, %xmm8
	vsubss	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vmulss	%xmm14, %xmm0, %xmm0
	vmovss	%xmm0, -2096968(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194112(%rbp), %xmm0, %xmm0
	vmulsd	%xmm11, %xmm0, %xmm7
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm0, %xmm6
	vaddsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194108(%rbp), %xmm5, %xmm5
	vmulsd	%xmm11, %xmm5, %xmm2
	vsubss	%xmm1, %xmm8, %xmm1
	vmovss	484(%r13), %xmm8
	vsubss	480(%r13), %xmm8, %xmm8
	vmulss	%xmm14, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vsubsd	%xmm2, %xmm6, %xmm6
	vmovss	%xmm1, -2096964(%rbp)
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vmovss	492(%r13), %xmm3
	vsubss	%xmm6, %xmm1, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-4194104(%rbp), %xmm6, %xmm6
	vsubss	488(%r13), %xmm3, %xmm3
	vsubss	%xmm1, %xmm8, %xmm1
	vmulsd	%xmm11, %xmm6, %xmm8
	vmulss	%xmm14, %xmm1, %xmm1
	vmovss	%xmm1, -2096960(%rbp)
	vmulsd	%xmm4, %xmm0, %xmm1
	vaddsd	%xmm7, %xmm5, %xmm0
	vmulsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm6, %xmm5
	vaddsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm0, %xmm8
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm6, %xmm4, %xmm1
	vsubss	%xmm0, %xmm3, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194100(%rbp), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm11
	vmulss	%xmm14, %xmm0, %xmm0
	vaddsd	%xmm11, %xmm1, %xmm1
	vsubsd	%xmm11, %xmm5, %xmm11
	vxorps	%xmm5, %xmm5, %xmm5
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	500(%r13), %xmm0
	vsubss	496(%r13), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm11, %xmm5, %xmm5
	vsubss	%xmm5, %xmm1, %xmm1
	vmovq	%rsi, %xmm5
	vmulsd	%xmm5, %xmm6, %xmm6
	leaq	-4194352(%rbp), %rsi
	vsubss	%xmm1, %xmm0, %xmm0
	vmovq	%rdi, %xmm1
	movq	%r13, %rdi
	vmulsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm6, %xmm2, %xmm2
	vmulss	%xmm14, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm6, %xmm6
	vmovq	%rcx, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm4, %xmm3, %xmm2
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	508(%r13), %xmm0
	vsubss	504(%r13), %xmm0, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm6, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm1, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm14, %xmm0, %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-2097200(%rbp), %rsi
	call	memcpy
	vmovapd	-4194544(%rbp), %ymm12
	vmovapd	-4194512(%rbp), %ymm15
	vmovapd	-4194480(%rbp), %ymm13
	vmovaps	-4194448(%rbp), %ymm14
.L881:
	addq	$1024, %r13
	subl	$1, %r12d
	jne	.L884
	addq	$262144, -4194576(%rbp)
	movq	-4194576(%rbp), %rax
	cmpq	%rax, -4194384(%rbp)
	jne	.L877
	movq	-4194368(%rbp), %rax
	movq	-4194360(%rbp), %r9
	vmovapd	.LC38(%rip), %ymm15
	leaq	16777240(%rax), %r12
.L898:
	movq	%r9, %rax
	movl	$64, %edx
	vmovapd	.LC31(%rip), %ymm11
	jmp	.L890
.L886:
	vmovups	32(%rax), %ymm4
	addq	$1024, %rax
	vmovups	-1024(%rax), %ymm0
	vmovups	-928(%rax), %ymm5
	vshufps	$136, %ymm4, %ymm0, %ymm3
	vshufps	$221, %ymm4, %ymm0, %ymm0
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm1
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-960(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm1, %ymm2
	vmovups	-864(%rax), %ymm6
	vmulps	.LC32(%rip), %ymm2, %ymm2
	vmovaps	%ymm2, -4194352(%rbp)
	vcvtps2pd	%xmm2, %ymm10
	vshufps	$136, %ymm5, %ymm0, %ymm4
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm4, %ymm4, %ymm3
	vshufps	$68, %ymm3, %ymm4, %ymm1
	vshufps	$238, %ymm3, %ymm4, %ymm3
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm4
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vmovups	-896(%rax), %ymm0
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vaddps	%ymm3, %ymm1, %ymm1
	vmovups	-800(%rax), %ymm7
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulps	.LC32(%rip), %ymm1, %ymm1
	vshufps	$136, %ymm6, %ymm0, %ymm5
	vshufps	$221, %ymm6, %ymm0, %ymm0
	vperm2f128	$3, %ymm5, %ymm5, %ymm4
	vshufps	$68, %ymm4, %ymm5, %ymm3
	vshufps	$238, %ymm4, %ymm5, %ymm4
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vshufps	$68, %ymm4, %ymm0, %ymm5
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vinsertf128	$1, %xmm4, %ymm5, %ymm0
	vmovups	-832(%rax), %ymm4
	vaddps	%ymm0, %ymm3, %ymm0
	vmovaps	%ymm1, -4194320(%rbp)
	vmovups	-984(%rax), %ymm13
	vmulps	.LC32(%rip), %ymm0, %ymm0
	vshufps	$136, %ymm7, %ymm4, %ymm6
	vshufps	$221, %ymm7, %ymm4, %ymm4
	vperm2f128	$3, %ymm6, %ymm6, %ymm5
	vxorpd	%xmm7, %xmm7, %xmm7
	vshufps	$68, %ymm5, %ymm6, %ymm3
	vcvtss2sd	-4194352(%rbp), %xmm7, %xmm7
	vshufps	$238, %ymm5, %ymm6, %ymm5
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vshufps	$68, %ymm5, %ymm4, %ymm6
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vmulsd	.LC35(%rip), %xmm7, %xmm4
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vaddps	%ymm5, %ymm3, %ymm3
	vmulps	.LC32(%rip), %ymm3, %ymm3
	vmovaps	%ymm3, -4194256(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194348(%rbp), %xmm3, %xmm3
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	.LC33(%rip), %xmm3, %xmm8
	vmovss	-1020(%rax), %xmm3
	vcvtss2sd	-4194344(%rbp), %xmm5, %xmm5
	vmovaps	%ymm0, -4194288(%rbp)
	vsubss	-1024(%rax), %xmm3, %xmm6
	vmulsd	.LC36(%rip), %xmm7, %xmm3
	vmulsd	.LC34(%rip), %xmm5, %xmm5
	vmovss	.LC37(%rip), %xmm7
	vaddsd	%xmm8, %xmm4, %xmm4
	vsubsd	%xmm8, %xmm3, %xmm3
	vmovups	-1016(%rax), %ymm8
	vsubsd	%xmm5, %xmm4, %xmm4
	vshufps	$221, %ymm13, %ymm8, %ymm12
	vshufps	$136, %ymm13, %ymm8, %ymm13
	vperm2f128	$3, %ymm13, %ymm13, %ymm8
	vaddsd	%xmm3, %xmm5, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm4, %xmm3
	vperm2f128	$3, %ymm12, %ymm12, %ymm4
	vshufps	$68, %ymm4, %ymm12, %ymm14
	vshufps	$238, %ymm4, %ymm12, %ymm4
	vshufps	$68, %ymm8, %ymm13, %ymm12
	vinsertf128	$1, %xmm4, %ymm14, %ymm4
	vshufps	$238, %ymm8, %ymm13, %ymm8
	vinsertf128	$1, %xmm8, %ymm12, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vmulpd	.LC39(%rip), %ymm10, %ymm12
	vmulpd	%ymm15, %ymm10, %ymm10
	vmulpd	.LC39(%rip), %ymm2, %ymm4
	vsubss	%xmm3, %xmm6, %xmm3
	vmulpd	%ymm15, %ymm2, %ymm2
	vmovups	-4194344(%rbp), %ymm6
	vcvtps2pd	%xmm6, %ymm9
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm15, %ymm9, %ymm9
	vcvtps2pd	%xmm6, %ymm6
	vmulss	%xmm7, %xmm3, %xmm3
	vmulpd	%ymm15, %ymm6, %ymm6
	vmovss	%xmm3, -2097200(%rbp)
	vmovups	-4194348(%rbp), %ymm3
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm5, %ymm12, %ymm12
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm10, %ymm5, %ymm5
	vaddpd	%ymm3, %ymm4, %ymm4
	vaddpd	%ymm2, %ymm3, %ymm3
	vaddpd	%ymm9, %ymm12, %ymm12
	vaddpd	%ymm6, %ymm4, %ymm4
	vsubpd	%ymm9, %ymm5, %ymm9
	vsubpd	%ymm6, %ymm3, %ymm3
	vcvtpd2psy	%ymm12, %xmm12
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm12, %ymm4
	vcvtpd2psy	%ymm9, %xmm2
	vcvtps2pd	%xmm1, %ymm9
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm6
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vsubps	%ymm2, %ymm4, %ymm2
	vsubps	%ymm2, %ymm8, %ymm2
	vmulps	.LC32(%rip), %ymm2, %ymm2
	vmovups	-920(%rax), %ymm10
	vmovups	%ymm2, -2097196(%rbp)
	vmovups	-952(%rax), %ymm4
	vmovups	-4194316(%rbp), %ymm5
	vshufps	$221, %ymm10, %ymm4, %ymm12
	vshufps	$136, %ymm10, %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm10
	vmovups	-4194312(%rbp), %ymm3
	vcvtps2pd	%xmm5, %ymm2
	vextractf128	$0x1, %ymm5, %xmm1
	vperm2f128	$3, %ymm12, %ymm12, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vshufps	$68, %ymm5, %ymm12, %ymm13
	vshufps	$238, %ymm5, %ymm12, %ymm5
	vshufps	$68, %ymm10, %ymm4, %ymm12
	vinsertf128	$1, %xmm5, %ymm13, %ymm5
	vshufps	$238, %ymm10, %ymm4, %ymm10
	vinsertf128	$1, %xmm10, %ymm12, %ymm10
	vsubps	%ymm10, %ymm5, %ymm5
	vmulpd	.LC39(%rip), %ymm6, %ymm4
	vmulpd	%ymm15, %ymm6, %ymm6
	vaddpd	%ymm1, %ymm4, %ymm4
	vcvtps2pd	%xmm3, %ymm8
	vmulpd	%ymm15, %ymm8, %ymm8
	vmulpd	.LC39(%rip), %ymm9, %ymm10
	vaddpd	%ymm6, %ymm1, %ymm1
	vmulpd	%ymm15, %ymm9, %ymm9
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm2, %ymm10, %ymm10
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm15, %ymm3, %ymm3
	vmovups	-4194284(%rbp), %ymm6
	vaddpd	%ymm9, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm9
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm3, %ymm4, %ymm4
	vaddpd	%ymm8, %ymm10, %ymm10
	vsubpd	%ymm8, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm1, %ymm3
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm4, %ymm10, %ymm4
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm3, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vsubps	%ymm1, %ymm4, %ymm1
	vmovups	-888(%rax), %ymm3
	vcvtps2pd	%xmm6, %ymm4
	vmovups	-4194280(%rbp), %ymm2
	vsubps	%ymm1, %ymm5, %ymm1
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm6, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulps	.LC32(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm2, %ymm8
	vmovups	%ymm1, -2097164(%rbp)
	vmovups	-856(%rax), %ymm1
	vmulpd	%ymm15, %ymm8, %ymm8
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm15, %ymm2, %ymm2
	vshufps	$221, %ymm1, %ymm3, %ymm10
	vshufps	$136, %ymm1, %ymm3, %ymm1
	vperm2f128	$3, %ymm10, %ymm10, %ymm6
	vperm2f128	$3, %ymm1, %ymm1, %ymm3
	vshufps	$68, %ymm6, %ymm10, %ymm12
	vshufps	$238, %ymm6, %ymm10, %ymm6
	vshufps	$68, %ymm3, %ymm1, %ymm10
	vinsertf128	$1, %xmm6, %ymm12, %ymm6
	vshufps	$238, %ymm3, %ymm1, %ymm3
	vinsertf128	$1, %xmm3, %ymm10, %ymm3
	vsubps	%ymm3, %ymm6, %ymm6
	vmulpd	.LC39(%rip), %ymm9, %ymm1
	vmulpd	.LC39(%rip), %ymm5, %ymm3
	vaddpd	%ymm4, %ymm1, %ymm1
	vmulpd	%ymm15, %ymm5, %ymm5
	vaddpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm5, %ymm0, %ymm0
	vmovsd	.LC40(%rip), %xmm5
	vaddpd	%ymm8, %ymm1, %ymm1
	vaddpd	%ymm2, %ymm3, %ymm3
	vsubpd	%ymm2, %ymm0, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm1, %ymm3
	vmulpd	%ymm15, %ymm9, %ymm1
	vaddpd	%ymm1, %ymm4, %ymm1
	vsubpd	%ymm8, %ymm1, %ymm1
	vcvtpd2psy	%ymm1, %xmm0
	vcvtpd2psy	%ymm2, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vsubps	%ymm0, %ymm3, %ymm0
	vcvtss2sd	-4194256(%rbp), %xmm1, %xmm1
	vmovss	-820(%rax), %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vsubss	-824(%rax), %xmm3, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm3
	vcvtss2sd	-4194248(%rbp), %xmm2, %xmm2
	vmulsd	.LC34(%rip), %xmm2, %xmm4
	vsubps	%ymm0, %ymm6, %ymm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmulps	.LC32(%rip), %ymm0, %ymm0
	vmovups	%ymm0, -2097132(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194252(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm6, %xmm6
	vmulsd	.LC34(%rip), %xmm1, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm1
	vsubsd	%xmm4, %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm1, %xmm6, %xmm1
	vmovss	-812(%rax), %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	vsubss	-816(%rax), %xmm3, %xmm9
	vmulsd	%xmm5, %xmm0, %xmm3
	vsubss	%xmm1, %xmm8, %xmm1
	vmulss	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vmovss	%xmm1, -2097100(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194244(%rbp), %xmm1, %xmm1
	vmulsd	.LC34(%rip), %xmm1, %xmm8
	vaddsd	%xmm4, %xmm1, %xmm4
	vaddsd	%xmm8, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm6, %xmm6
	vmulsd	.LC34(%rip), %xmm0, %xmm3
	vaddsd	%xmm2, %xmm3, %xmm0
	vsubsd	%xmm8, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vsubss	%xmm0, %xmm9, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194240(%rbp), %xmm0, %xmm0
	vmovss	-804(%rax), %xmm3
	vmulsd	.LC34(%rip), %xmm0, %xmm6
	vaddsd	%xmm8, %xmm0, %xmm8
	vsubss	-808(%rax), %xmm3, %xmm9
	vmulsd	%xmm5, %xmm2, %xmm3
	movq	-4194352(%rbp), %rcx
	vsubsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm1, %xmm3, %xmm3
	movq	%rcx, -1024(%rax)
	movq	-4194344(%rbp), %rcx
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm6, %xmm3, %xmm3
	movq	%rcx, -1016(%rax)
	movq	-4194336(%rbp), %rcx
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194236(%rbp), %xmm4, %xmm4
	vmulsd	.LC34(%rip), %xmm4, %xmm3
	vaddsd	%xmm6, %xmm4, %xmm6
	movq	%rcx, -1008(%rax)
	movq	-4194328(%rbp), %rcx
	vsubss	%xmm2, %xmm9, %xmm2
	vmulss	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vmovss	%xmm2, -2097092(%rbp)
	vmovss	-796(%rax), %xmm2
	vsubss	-800(%rax), %xmm2, %xmm9
	vmulsd	%xmm5, %xmm1, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm8, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194232(%rbp), %xmm2, %xmm2
	vmulsd	.LC34(%rip), %xmm2, %xmm8
	vsubss	%xmm1, %xmm9, %xmm1
	vmulss	%xmm7, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm1, -2097088(%rbp)
	vmovss	-788(%rax), %xmm1
	vsubss	-792(%rax), %xmm1, %xmm9
	vmulsd	%xmm5, %xmm0, %xmm1
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm1, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-4194228(%rbp), %xmm8, %xmm8
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm6, %xmm1, %xmm0
	vmulsd	%xmm5, %xmm4, %xmm1
	vmulsd	.LC34(%rip), %xmm8, %xmm6
	vaddsd	%xmm3, %xmm2, %xmm5
	vsubss	%xmm0, %xmm9, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vmulss	%xmm7, %xmm0, %xmm0
	vsubsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm6, %xmm4, %xmm4
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	-780(%rax), %xmm0
	vsubsd	%xmm2, %xmm3, %xmm3
	vsubss	-784(%rax), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm2
	vsubss	%xmm4, %xmm0, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	-772(%rax), %xmm0
	vsubss	-776(%rax), %xmm0, %xmm6
	vmulsd	.LC36(%rip), %xmm8, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm8, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmulss	%xmm7, %xmm0, %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	movq	%rcx, -1000(%rax)
	movq	-4194320(%rbp), %rcx
	movq	-2097200(%rbp), %rsi
	movq	%rcx, -992(%rax)
	movq	-4194312(%rbp), %rcx
	movq	%rsi, -896(%rax)
	movq	-2097192(%rbp), %rsi
	movq	%rcx, -984(%rax)
	movq	-4194304(%rbp), %rcx
	movq	%rsi, -888(%rax)
	movq	-2097184(%rbp), %rsi
	movq	%rcx, -976(%rax)
	movq	-4194296(%rbp), %rcx
	movq	%rsi, -880(%rax)
	movq	-2097176(%rbp), %rsi
	movq	%rcx, -968(%rax)
	movq	-4194288(%rbp), %rcx
	movq	%rsi, -872(%rax)
	movq	-2097168(%rbp), %rsi
	movq	%rcx, -960(%rax)
	movq	-4194280(%rbp), %rcx
	movq	%rcx, -952(%rax)
	movq	-4194272(%rbp), %rcx
	movq	%rcx, -944(%rax)
	movq	-4194264(%rbp), %rcx
	movq	%rcx, -936(%rax)
	movq	-4194256(%rbp), %rcx
	movq	%rcx, -928(%rax)
	movq	-4194248(%rbp), %rcx
	movq	%rcx, -920(%rax)
	movq	-4194240(%rbp), %rcx
	movq	%rcx, -912(%rax)
	movq	-4194232(%rbp), %rcx
	movq	%rcx, -904(%rax)
	movq	%rsi, -864(%rax)
	movq	-2097160(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-2097152(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-2097144(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-2097136(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-2097128(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-2097120(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-2097112(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-2097104(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-2097096(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-2097088(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-2097080(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %edx
	je	.L1040
.L890:
	cmpl	$1, %ebx
	ja	.L886
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rax), %xmm0, %xmm0
	vcvtss2sd	8(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm0, %xmm0
	vmulsd	.LC21(%rip), %xmm1, %xmm1
	vmovups	40(%rax), %ymm5
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
	vmovups	8(%rax), %ymm0
	vshufps	$136, %ymm5, %ymm0, %ymm2
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm3
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vmovups	24(%rax), %ymm2
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 56(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm4
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovups	16(%rax), %ymm4
	vshufps	$136, 48(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm6
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vmovups	(%rax), %ymm4
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vshufps	$136, 32(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm12
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vinsertf128	$1, %xmm6, %ymm12, %ymm12
	vshufps	$68, %ymm4, %ymm0, %ymm5
	vcvtps2pd	%xmm12, %ymm7
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vmulpd	.LC23(%rip), %ymm7, %ymm0
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm12, %xmm8
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm8, %ymm8
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vmulpd	.LC23(%rip), %ymm8, %ymm5
	vaddpd	%ymm3, %ymm5, %ymm3
	vmovups	104(%rax), %ymm5
	vcvtpd2psy	%ymm0, %xmm0
	vaddpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm1, %ymm2, %ymm1
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm1
	vsubps	%ymm1, %ymm4, %ymm1
	vmovups	%ymm1, -4194348(%rbp)
	vmovups	72(%rax), %ymm1
	vshufps	$136, %ymm5, %ymm1, %ymm2
	vshufps	$221, %ymm5, %ymm1, %ymm1
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm3
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovups	88(%rax), %ymm3
	vshufps	$136, 120(%rax), %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm4
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vmovups	80(%rax), %ymm4
	vshufps	$136, 112(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm3
	vshufps	$68, %ymm3, %ymm4, %ymm6
	vshufps	$238, %ymm3, %ymm4, %ymm3
	vmovups	64(%rax), %ymm4
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	vshufps	$136, 96(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm10
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm1, %ymm1, %ymm4
	vinsertf128	$1, %xmm6, %ymm10, %ymm10
	vshufps	$68, %ymm4, %ymm1, %ymm5
	vcvtps2pd	%xmm10, %ymm6
	vshufps	$238, %ymm4, %ymm1, %ymm4
	vcvtps2pd	%xmm0, %ymm1
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vmulpd	.LC23(%rip), %ymm6, %ymm5
	vaddpd	%ymm1, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmovups	168(%rax), %ymm13
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtps2pd	%xmm2, %ymm5
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vsubpd	%ymm5, %ymm1, %ymm1
	vextractf128	$0x1, %ymm10, %xmm5
	vcvtps2pd	%xmm5, %ymm14
	vmulpd	.LC23(%rip), %ymm14, %ymm5
	vaddpd	%ymm0, %ymm5, %ymm0
	vcvtpd2psy	%ymm1, %xmm1
	vaddpd	%ymm3, %ymm0, %ymm3
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm0, %ymm3, %ymm0
	vmovups	136(%rax), %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm4, %ymm0
	vshufps	$136, %ymm13, %ymm3, %ymm1
	vshufps	$221, %ymm13, %ymm3, %ymm3
	vmovups	%ymm0, -4194316(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	152(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	vshufps	$136, 184(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm4
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmovups	144(%rax), %ymm4
	vshufps	$136, 176(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm1
	vshufps	$68, %ymm1, %ymm4, %ymm5
	vshufps	$238, %ymm1, %ymm4, %ymm1
	vmovups	128(%rax), %ymm4
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vshufps	$136, 160(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vshufps	$68, %ymm5, %ymm4, %ymm9
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vperm2f128	$3, %ymm3, %ymm3, %ymm4
	vinsertf128	$1, %xmm5, %ymm9, %ymm9
	vshufps	$68, %ymm4, %ymm3, %ymm13
	vcvtps2pd	%xmm9, %ymm5
	vshufps	$238, %ymm4, %ymm3, %ymm4
	vmovapd	%ymm5, -4194512(%rbp)
	vcvtps2pd	%xmm2, %ymm3
	vinsertf128	$1, %xmm4, %ymm13, %ymm13
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmulpd	.LC23(%rip), %ymm5, %ymm5
	vaddpd	%ymm3, %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm3, %ymm5, %ymm3
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm5, %ymm3, %ymm3
	vextractf128	$0x1, %ymm9, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	.LC23(%rip), %ymm5, %ymm4
	vaddpd	%ymm4, %ymm2, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm1, %ymm2, %ymm2
	vmovsd	.LC26(%rip), %xmm1
	vsubpd	%ymm0, %ymm2, %ymm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vsubps	%ymm2, %ymm13, %ymm13
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	200(%rax), %xmm3, %xmm3
	vmovups	%ymm13, -4194284(%rbp)
	vmovapd	%xmm3, %xmm13
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	208(%rax), %xmm3, %xmm3
	vmulsd	%xmm3, %xmm1, %xmm2
	vmovsd	%xmm3, -4194480(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	216(%rax), %xmm3, %xmm3
	vmovsd	%xmm13, -4194416(%rbp)
	vcvtss2sd	192(%rax), %xmm0, %xmm0
	vmulsd	%xmm13, %xmm1, %xmm13
	vmovsd	.LC27(%rip), %xmm4
	vmulsd	.LC27(%rip), %xmm0, %xmm0
	vaddsd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm3, %xmm13
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm13, %xmm0, %xmm0
	vmovss	204(%rax), %xmm13
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm13, %xmm0
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	224(%rax), %xmm13, %xmm13
	vmovss	%xmm0, -4194252(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm0
	vmovsd	%xmm0, -4194448(%rbp)
	vmulsd	-4194416(%rbp), %xmm4, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm13, %xmm2
	vaddsd	-4194448(%rbp), %xmm0, %xmm0
	vsubsd	%xmm2, %xmm0, %xmm0
	vmovsd	%xmm2, -4194544(%rbp)
	vmovss	212(%rax), %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm13, %xmm2
	vmovss	%xmm0, -4194248(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	232(%rax), %xmm0, %xmm0
	vmovsd	%xmm0, -4194416(%rbp)
	vmulsd	-4194480(%rbp), %xmm4, %xmm0
	vaddsd	-4194448(%rbp), %xmm0, %xmm0
	vmovsd	.LC22(%rip), %xmm4
	vmulsd	-4194416(%rbp), %xmm4, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm0
	vmovss	220(%rax), %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, -4194244(%rbp)
	vmulsd	.LC27(%rip), %xmm3, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	240(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm4
	vaddsd	%xmm0, %xmm2, %xmm0
	vmulsd	-4194416(%rbp), %xmm1, %xmm2
	vmulsd	%xmm1, %xmm3, %xmm1
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm0
	vmovss	228(%rax), %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, -4194240(%rbp)
	vmulsd	.LC27(%rip), %xmm13, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	248(%rax), %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm2, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	236(%rax), %xmm1
	vmovaps	%ymm12, -2097200(%rbp)
	vmovsd	.LC20(%rip), %xmm4
	vmovaps	%ymm10, -2097168(%rbp)
	vmulsd	.LC28(%rip), %xmm13, %xmm13
	vmovaps	%ymm9, -2097136(%rbp)
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	-4194544(%rbp), %xmm1
	vmovss	%xmm0, -4194236(%rbp)
	vmulsd	-4194416(%rbp), %xmm4, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC21(%rip), %xmm3, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm2, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	244(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vmovss	%xmm0, -4194232(%rbp)
	vmovsd	-4194416(%rbp), %xmm0
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm13, %xmm13
	vmulsd	%xmm1, %xmm3, %xmm0
	vmulsd	%xmm1, %xmm2, %xmm1
	vsubsd	%xmm0, %xmm13, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	252(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194228(%rbp)
	vmovss	192(%rax), %xmm0
	vmovss	%xmm0, -2097104(%rbp)
	vmovss	200(%rax), %xmm0
	vmovss	%xmm0, -2097100(%rbp)
	vmovss	208(%rax), %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vmovss	216(%rax), %xmm0
	vmovss	%xmm0, -2097092(%rbp)
	vmovss	224(%rax), %xmm0
	vmovss	%xmm0, -2097088(%rbp)
	vmovss	232(%rax), %xmm0
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	240(%rax), %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	248(%rax), %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	testb	%r14b, %r14b
	je	.L889
	vmovaps	-4194352(%rbp), %ymm1
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm7, %ymm0
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm8, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194320(%rbp), %ymm1
	vmovaps	%ymm0, -2097200(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm6, %ymm0
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm14, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194288(%rbp), %ymm1
	vmovaps	%ymm0, -2097168(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	-4194512(%rbp), %ymm0, %ymm0
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194256(%rbp), %ymm1
	vmovaps	%ymm0, -2097136(%rbp)
	vmovaps	-2097104(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm11, %ymm3, %ymm3
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vmulpd	%ymm11, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2097104(%rbp)
.L889:
	movq	-2097200(%rbp), %rcx
	addq	$1024, %rax
	movq	-4194352(%rbp), %rsi
	movq	%rcx, -1024(%rax)
	movq	-2097192(%rbp), %rcx
	movq	%rcx, -1016(%rax)
	movq	-2097184(%rbp), %rcx
	movq	%rcx, -1008(%rax)
	movq	-2097176(%rbp), %rcx
	movq	%rcx, -1000(%rax)
	movq	-2097168(%rbp), %rcx
	movq	%rcx, -992(%rax)
	movq	-2097160(%rbp), %rcx
	movq	%rcx, -984(%rax)
	movq	-2097152(%rbp), %rcx
	movq	%rcx, -976(%rax)
	movq	-2097144(%rbp), %rcx
	movq	%rcx, -968(%rax)
	movq	-2097136(%rbp), %rcx
	movq	%rcx, -960(%rax)
	movq	-2097128(%rbp), %rcx
	movq	%rcx, -952(%rax)
	movq	-2097120(%rbp), %rcx
	movq	%rcx, -944(%rax)
	movq	-2097112(%rbp), %rcx
	movq	%rcx, -936(%rax)
	movq	-2097104(%rbp), %rcx
	movq	%rcx, -928(%rax)
	movq	-2097096(%rbp), %rcx
	movq	%rcx, -920(%rax)
	movq	-2097088(%rbp), %rcx
	movq	%rcx, -912(%rax)
	movq	-2097080(%rbp), %rcx
	movq	%rcx, -904(%rax)
	movq	%rsi, -896(%rax)
	movq	-4194344(%rbp), %rsi
	movq	%rsi, -888(%rax)
	movq	-4194336(%rbp), %rsi
	movq	%rsi, -880(%rax)
	movq	-4194328(%rbp), %rsi
	movq	%rsi, -872(%rax)
	movq	-4194320(%rbp), %rsi
	movq	%rsi, -864(%rax)
	movq	-4194312(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-4194304(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-4194296(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-4194288(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-4194280(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-4194272(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-4194264(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-4194256(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-4194248(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-4194240(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-4194232(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %edx
	jne	.L890
.L1040:
	leaq	1024(%r9), %r8
	xorl	%edi, %edi
	movq	%r9, %rsi
	addl	$1, %edi
	cmpl	$64, %edi
	je	.L959
.L1041:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L892:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$64, %eax
	jne	.L892
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$64, %edi
	jne	.L1041
.L959:
	movq	%r9, %rax
	movl	$64, %ecx
	vmovaps	.LC32(%rip), %ymm11
	vmovapd	.LC38(%rip), %ymm10
	jmp	.L891
	.p2align 4,,10
	.p2align 3
.L894:
	vmovups	32(%rax), %ymm3
	addq	$1024, %rax
	vmovups	-1024(%rax), %ymm0
	vmovups	-928(%rax), %ymm5
	vshufps	$136, %ymm3, %ymm0, %ymm2
	vshufps	$221, %ymm3, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm4
	vshufps	$68, %ymm4, %ymm2, %ymm1
	vshufps	$238, %ymm4, %ymm2, %ymm4
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vshufps	$68, %ymm2, %ymm0, %ymm4
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-960(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm4, %ymm4
	vaddps	%ymm4, %ymm1, %ymm4
	vmovups	-864(%rax), %ymm6
	vshufps	$136, %ymm5, %ymm0, %ymm3
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm3, %ymm3, %ymm2
	vshufps	$68, %ymm2, %ymm3, %ymm1
	vshufps	$238, %ymm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-896(%rax), %ymm0
	vmulps	%ymm11, %ymm4, %ymm4
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovups	-800(%rax), %ymm7
	vaddps	%ymm2, %ymm1, %ymm1
	vcvtps2pd	%xmm4, %ymm9
	vmovaps	%ymm4, -4194352(%rbp)
	vshufps	$136, %ymm6, %ymm0, %ymm5
	vshufps	$221, %ymm6, %ymm0, %ymm0
	vperm2f128	$3, %ymm5, %ymm5, %ymm3
	vshufps	$68, %ymm3, %ymm5, %ymm2
	vshufps	$238, %ymm3, %ymm5, %ymm3
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm5
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm5, %ymm0
	vaddps	%ymm0, %ymm2, %ymm0
	vmulps	%ymm11, %ymm1, %ymm1
	vmovups	-832(%rax), %ymm3
	vmovaps	%ymm1, -4194320(%rbp)
	vmovups	-984(%rax), %ymm13
	vshufps	$136, %ymm7, %ymm3, %ymm6
	vshufps	$221, %ymm7, %ymm3, %ymm3
	vperm2f128	$3, %ymm6, %ymm6, %ymm5
	vshufps	$68, %ymm5, %ymm6, %ymm2
	vshufps	$238, %ymm5, %ymm6, %ymm5
	vinsertf128	$1, %xmm5, %ymm2, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm5
	vshufps	$68, %ymm5, %ymm3, %ymm6
	vshufps	$238, %ymm5, %ymm3, %ymm5
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vaddps	%ymm5, %ymm2, %ymm2
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-4194352(%rbp), %xmm7, %xmm7
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194344(%rbp), %xmm5, %xmm5
	vmulps	%ymm11, %ymm0, %ymm0
	vmulsd	.LC35(%rip), %xmm7, %xmm3
	vmovaps	%ymm0, -4194288(%rbp)
	vmulsd	.LC34(%rip), %xmm5, %xmm5
	vmulps	%ymm11, %ymm2, %ymm2
	vmovaps	%ymm2, -4194256(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194348(%rbp), %xmm2, %xmm2
	vmulsd	.LC33(%rip), %xmm2, %xmm8
	vmovss	-1020(%rax), %xmm2
	vsubss	-1024(%rax), %xmm2, %xmm6
	vmulsd	.LC36(%rip), %xmm7, %xmm2
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm8, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm5, %xmm2
	vmovups	-4194344(%rbp), %ymm5
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtps2pd	%xmm5, %ymm8
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm10, %ymm8, %ymm8
	vcvtps2pd	%xmm5, %ymm5
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm3, %xmm2
	vmovups	-4194348(%rbp), %ymm3
	vmulpd	%ymm10, %ymm5, %ymm5
	vsubss	%xmm2, %xmm6, %xmm2
	vmovups	-1016(%rax), %ymm6
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vshufps	$221, %ymm13, %ymm6, %ymm12
	vshufps	$136, %ymm13, %ymm6, %ymm6
	vperm2f128	$3, %ymm12, %ymm12, %ymm7
	vperm2f128	$3, %ymm6, %ymm6, %ymm13
	vshufps	$68, %ymm7, %ymm12, %ymm14
	vshufps	$238, %ymm7, %ymm12, %ymm7
	vshufps	$68, %ymm13, %ymm6, %ymm12
	vinsertf128	$1, %xmm7, %ymm14, %ymm7
	vshufps	$238, %ymm13, %ymm6, %ymm13
	vinsertf128	$1, %xmm13, %ymm12, %ymm12
	vsubps	%ymm12, %ymm7, %ymm7
	vmulpd	.LC39(%rip), %ymm9, %ymm6
	vmovss	%xmm2, -2097200(%rbp)
	vmulpd	%ymm10, %ymm9, %ymm9
	vextractf128	$0x1, %ymm4, %xmm2
	vcvtps2pd	%xmm3, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm6, %ymm6
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	.LC39(%rip), %ymm2, %ymm12
	vaddpd	%ymm9, %ymm4, %ymm4
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm10, %ymm2, %ymm2
	vaddpd	%ymm3, %ymm12, %ymm12
	vaddpd	%ymm2, %ymm3, %ymm3
	vaddpd	%ymm8, %ymm6, %ymm6
	vaddpd	%ymm5, %ymm12, %ymm12
	vsubpd	%ymm8, %ymm4, %ymm8
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtpd2psy	%ymm6, %xmm6
	vmovups	-4194312(%rbp), %ymm5
	vcvtpd2psy	%ymm12, %xmm12
	vinsertf128	$0x1, %xmm12, %ymm6, %ymm6
	vmovups	-920(%rax), %ymm12
	vcvtpd2psy	%ymm8, %xmm2
	vcvtps2pd	%xmm1, %ymm8
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vsubps	%ymm2, %ymm6, %ymm2
	vmovups	-952(%rax), %ymm6
	vshufps	$221, %ymm12, %ymm6, %ymm9
	vshufps	$136, %ymm12, %ymm6, %ymm12
	vperm2f128	$3, %ymm12, %ymm12, %ymm6
	vsubps	%ymm2, %ymm7, %ymm7
	vmulps	%ymm11, %ymm7, %ymm2
	vmovups	%ymm2, -2097196(%rbp)
	vcvtps2pd	%xmm5, %ymm7
	vmovups	-4194316(%rbp), %ymm2
	vmulpd	%ymm10, %ymm7, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	%ymm10, %ymm5, %ymm5
	vcvtps2pd	%xmm2, %ymm3
	vextractf128	$0x1, %ymm2, %xmm1
	vperm2f128	$3, %ymm9, %ymm9, %ymm2
	vshufps	$68, %ymm2, %ymm9, %ymm13
	vshufps	$238, %ymm2, %ymm9, %ymm2
	vshufps	$68, %ymm6, %ymm12, %ymm9
	vshufps	$238, %ymm6, %ymm12, %ymm6
	vinsertf128	$1, %xmm6, %ymm9, %ymm6
	vmulpd	.LC39(%rip), %ymm8, %ymm9
	vmulpd	%ymm10, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm9, %ymm9
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm8, %ymm3, %ymm3
	vinsertf128	$1, %xmm2, %ymm13, %ymm2
	vmovups	-856(%rax), %ymm8
	vsubps	%ymm6, %ymm2, %ymm6
	vmulpd	.LC39(%rip), %ymm4, %ymm2
	vaddpd	%ymm7, %ymm9, %ymm9
	vaddpd	%ymm1, %ymm2, %ymm2
	vsubpd	%ymm7, %ymm3, %ymm7
	vmulpd	%ymm10, %ymm4, %ymm3
	vmovups	-4194284(%rbp), %ymm4
	vaddpd	%ymm3, %ymm1, %ymm3
	vcvtpd2psy	%ymm9, %xmm9
	vaddpd	%ymm5, %ymm2, %ymm2
	vcvtpd2psy	%ymm7, %xmm1
	vcvtps2pd	%xmm0, %ymm7
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtps2pd	%xmm0, %ymm5
	vextractf128	$0x1, %ymm4, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm9, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm1
	vmovups	-888(%rax), %ymm3
	vmovups	-4194280(%rbp), %ymm2
	vshufps	$221, %ymm8, %ymm3, %ymm9
	vshufps	$136, %ymm8, %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm8
	vsubps	%ymm1, %ymm6, %ymm1
	vcvtps2pd	%xmm2, %ymm6
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm10, %ymm6, %ymm6
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm10, %ymm2, %ymm2
	vmulps	%ymm11, %ymm1, %ymm1
	vmovups	%ymm1, -2097164(%rbp)
	vcvtps2pd	%xmm4, %ymm1
	vperm2f128	$3, %ymm9, %ymm9, %ymm4
	vshufps	$68, %ymm4, %ymm9, %ymm12
	vshufps	$238, %ymm4, %ymm9, %ymm4
	vshufps	$68, %ymm8, %ymm3, %ymm9
	vinsertf128	$1, %xmm4, %ymm12, %ymm4
	vshufps	$238, %ymm8, %ymm3, %ymm8
	vinsertf128	$1, %xmm8, %ymm9, %ymm8
	vsubps	%ymm8, %ymm4, %ymm4
	vmulpd	.LC39(%rip), %ymm5, %ymm3
	vmulpd	.LC39(%rip), %ymm7, %ymm8
	vaddpd	%ymm0, %ymm3, %ymm3
	vmulpd	%ymm10, %ymm7, %ymm7
	vmulpd	%ymm10, %ymm5, %ymm5
	vaddpd	%ymm1, %ymm8, %ymm8
	vaddpd	%ymm5, %ymm0, %ymm0
	vxorps	%xmm5, %xmm5, %xmm5
	vaddpd	%ymm7, %ymm1, %ymm1
	vaddpd	%ymm6, %ymm8, %ymm8
	vaddpd	%ymm2, %ymm3, %ymm3
	vsubpd	%ymm6, %ymm1, %ymm1
	vsubpd	%ymm2, %ymm0, %ymm2
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm8, %ymm3
	vcvtpd2psy	%ymm1, %xmm0
	vcvtpd2psy	%ymm2, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovss	-820(%rax), %xmm3
	vcvtss2sd	-4194256(%rbp), %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194248(%rbp), %xmm2, %xmm2
	vsubss	-824(%rax), %xmm3, %xmm7
	vmulsd	.LC34(%rip), %xmm2, %xmm6
	vsubps	%ymm0, %ymm4, %ymm0
	vmovsd	.LC40(%rip), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm3
	vmulps	%ymm11, %ymm0, %ymm0
	vmovups	%ymm0, -2097132(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194252(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	.LC34(%rip), %xmm1, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm1
	vsubsd	%xmm6, %xmm1, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovss	-812(%rax), %xmm3
	vxorps	%xmm5, %xmm5, %xmm5
	vsubss	-816(%rax), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm3
	vsubss	%xmm1, %xmm7, %xmm1
	vmulss	.LC37(%rip), %xmm1, %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vmovss	%xmm1, -2097100(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194244(%rbp), %xmm1, %xmm1
	vmulsd	.LC34(%rip), %xmm1, %xmm7
	vaddsd	%xmm1, %xmm6, %xmm6
	vaddsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmulsd	.LC34(%rip), %xmm0, %xmm3
	vaddsd	%xmm2, %xmm3, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmovss	-804(%rax), %xmm3
	vsubss	%xmm0, %xmm8, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vsubss	-808(%rax), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm2, %xmm3
	vmovss	%xmm0, -2097096(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194240(%rbp), %xmm0, %xmm0
	vmulsd	.LC34(%rip), %xmm0, %xmm5
	vaddsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm1, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm6, %xmm6
	vaddsd	%xmm5, %xmm3, %xmm3
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm6, %xmm3, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vsubss	%xmm2, %xmm8, %xmm2
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vmovss	%xmm2, -2097092(%rbp)
	vmovss	-796(%rax), %xmm6
	vcvtss2sd	-4194236(%rbp), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm3, %xmm5
	vsubss	-800(%rax), %xmm6, %xmm8
	vmulsd	%xmm4, %xmm1, %xmm6
	vmulsd	.LC34(%rip), %xmm3, %xmm2
	movq	-4194352(%rbp), %rdx
	vaddsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm2, %xmm7, %xmm7
	movq	%rdx, -1024(%rax)
	movq	-4194344(%rbp), %rdx
	vaddsd	%xmm2, %xmm6, %xmm6
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	movq	%rdx, -1016(%rax)
	movq	-4194336(%rbp), %rdx
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-4194232(%rbp), %xmm6, %xmm6
	vmulsd	.LC34(%rip), %xmm6, %xmm7
	vsubss	%xmm1, %xmm8, %xmm1
	movq	%rdx, -1008(%rax)
	vmulss	.LC37(%rip), %xmm1, %xmm1
	movq	-4194328(%rbp), %rdx
	vsubsd	%xmm7, %xmm5, %xmm5
	vmovss	%xmm1, -2097088(%rbp)
	vmovss	-788(%rax), %xmm1
	vsubss	-792(%rax), %xmm1, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm1
	movq	%rdx, -1000(%rax)
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm5, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm3, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194228(%rbp), %xmm5, %xmm5
	vaddsd	%xmm2, %xmm6, %xmm4
	vmulsd	.LC34(%rip), %xmm5, %xmm7
	vsubss	%xmm0, %xmm8, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm3
	vmulsd	.LC33(%rip), %xmm6, %xmm6
	vsubsd	%xmm7, %xmm4, %xmm4
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	-780(%rax), %xmm0
	vaddsd	%xmm7, %xmm3, %xmm3
	vsubss	-784(%rax), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm2, %xmm2
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm6, %xmm6
	vsubss	%xmm3, %xmm0, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	-772(%rax), %xmm0
	vsubss	-776(%rax), %xmm0, %xmm7
	vmulsd	.LC36(%rip), %xmm5, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm5, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm6, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	movq	-4194320(%rbp), %rdx
	movq	-2097200(%rbp), %rsi
	movq	%rdx, -992(%rax)
	movq	-4194312(%rbp), %rdx
	movq	%rsi, -896(%rax)
	movq	-2097192(%rbp), %rsi
	movq	%rdx, -984(%rax)
	movq	-4194304(%rbp), %rdx
	movq	%rsi, -888(%rax)
	movq	-2097184(%rbp), %rsi
	movq	%rdx, -976(%rax)
	movq	-4194296(%rbp), %rdx
	movq	%rsi, -880(%rax)
	movq	-2097176(%rbp), %rsi
	movq	%rdx, -968(%rax)
	movq	-4194288(%rbp), %rdx
	movq	%rsi, -872(%rax)
	movq	-2097168(%rbp), %rsi
	movq	%rdx, -960(%rax)
	movq	-4194280(%rbp), %rdx
	movq	%rdx, -952(%rax)
	movq	-4194272(%rbp), %rdx
	movq	%rdx, -944(%rax)
	movq	-4194264(%rbp), %rdx
	movq	%rdx, -936(%rax)
	movq	-4194256(%rbp), %rdx
	movq	%rdx, -928(%rax)
	movq	-4194248(%rbp), %rdx
	movq	%rdx, -920(%rax)
	movq	-4194240(%rbp), %rdx
	movq	%rdx, -912(%rax)
	movq	-4194232(%rbp), %rdx
	movq	%rdx, -904(%rax)
	movq	%rsi, -864(%rax)
	movq	-2097160(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-2097152(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-2097144(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-2097136(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-2097128(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-2097120(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-2097112(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-2097104(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-2097096(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-2097088(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-2097080(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %ecx
	je	.L1042
.L891:
	cmpl	$1, %ebx
	ja	.L894
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	(%rax), %xmm0, %xmm0
	vcvtss2sd	8(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm0, %xmm0
	vmulsd	.LC21(%rip), %xmm1, %xmm1
	vmovups	40(%rax), %ymm5
	vmovups	104(%rax), %ymm8
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
	vmovups	8(%rax), %ymm0
	vshufps	$136, %ymm5, %ymm0, %ymm2
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm3
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vmovups	24(%rax), %ymm2
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 56(%rax), %ymm2, %ymm2
	vcvtps2pd	%xmm3, %ymm5
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm4
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovups	16(%rax), %ymm4
	vshufps	$136, 48(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm6
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vmovups	(%rax), %ymm4
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vshufps	$136, 32(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm12
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vinsertf128	$1, %xmm6, %ymm12, %ymm12
	vshufps	$68, %ymm4, %ymm0, %ymm6
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vinsertf128	$1, %xmm4, %ymm6, %ymm6
	vcvtps2pd	%xmm12, %ymm4
	vmulpd	.LC23(%rip), %ymm4, %ymm0
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm5, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vsubpd	%ymm5, %ymm0, %ymm0
	vextractf128	$0x1, %ymm12, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	.LC23(%rip), %ymm5, %ymm7
	vaddpd	%ymm3, %ymm7, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vaddpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm1, %ymm2, %ymm1
	vmovups	72(%rax), %ymm2
	vshufps	$136, %ymm8, %ymm2, %ymm3
	vshufps	$221, %ymm8, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm1
	vsubps	%ymm1, %ymm6, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vmovups	%ymm1, -4194348(%rbp)
	vshufps	$68, %ymm0, %ymm3, %ymm1
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vmovups	88(%rax), %ymm0
	vshufps	$136, 120(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm6
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm6, %ymm6
	vmovups	80(%rax), %ymm3
	vshufps	$136, 112(%rax), %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vshufps	$68, %ymm0, %ymm3, %ymm7
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vmovups	64(%rax), %ymm3
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vshufps	$136, 96(%rax), %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm7
	vshufps	$68, %ymm7, %ymm3, %ymm9
	vshufps	$238, %ymm7, %ymm3, %ymm7
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vinsertf128	$1, %xmm7, %ymm9, %ymm9
	vshufps	$68, %ymm3, %ymm2, %ymm7
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vcvtps2pd	%xmm1, %ymm2
	vinsertf128	$1, %xmm3, %ymm7, %ymm7
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vcvtps2pd	%xmm9, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC23(%rip), %ymm3, %ymm8
	vaddpd	%ymm2, %ymm8, %ymm8
	vcvtps2pd	%xmm0, %ymm2
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vmovups	168(%rax), %ymm13
	vaddpd	%ymm2, %ymm8, %ymm2
	vcvtps2pd	%xmm6, %ymm8
	vmovapd	%ymm3, -4194512(%rbp)
	vmulpd	.LC25(%rip), %ymm8, %ymm8
	vsubpd	%ymm8, %ymm2, %ymm2
	vextractf128	$0x1, %ymm9, %xmm8
	vcvtps2pd	%xmm8, %ymm14
	vmulpd	.LC23(%rip), %ymm14, %ymm8
	vaddpd	%ymm1, %ymm8, %ymm1
	vmovapd	%ymm14, -4194544(%rbp)
	vcvtpd2psy	%ymm2, %xmm2
	vaddpd	%ymm0, %ymm1, %ymm1
	vextractf128	$0x1, %ymm6, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm0, %ymm7, %ymm0
	vmovups	136(%rax), %ymm7
	vshufps	$136, %ymm13, %ymm7, %ymm1
	vshufps	$221, %ymm13, %ymm7, %ymm7
	vmovups	%ymm0, -4194316(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	152(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	vshufps	$136, 184(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm6
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm6, %ymm0
	vmovups	144(%rax), %ymm6
	vshufps	$136, 176(%rax), %ymm6, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm1
	vshufps	$68, %ymm1, %ymm6, %ymm8
	vshufps	$238, %ymm1, %ymm6, %ymm1
	vinsertf128	$1, %xmm1, %ymm8, %ymm1
	vmovups	128(%rax), %ymm8
	vshufps	$136, 160(%rax), %ymm8, %ymm8
	vperm2f128	$3, %ymm8, %ymm8, %ymm6
	vshufps	$68, %ymm6, %ymm8, %ymm14
	vshufps	$238, %ymm6, %ymm8, %ymm6
	vperm2f128	$3, %ymm7, %ymm7, %ymm8
	vinsertf128	$1, %xmm6, %ymm14, %ymm6
	vshufps	$68, %ymm8, %ymm7, %ymm13
	vcvtps2pd	%xmm6, %ymm14
	vshufps	$238, %ymm8, %ymm7, %ymm8
	vcvtps2pd	%xmm2, %ymm7
	vinsertf128	$1, %xmm8, %ymm13, %ymm8
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm7, %ymm7
	vmulpd	.LC23(%rip), %ymm14, %ymm13
	vaddpd	%ymm7, %ymm13, %ymm13
	vcvtps2pd	%xmm1, %ymm7
	vmulpd	.LC24(%rip), %ymm7, %ymm7
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm7, %ymm13, %ymm7
	vcvtps2pd	%xmm0, %ymm13
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	.LC25(%rip), %ymm13, %ymm13
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm13, %ymm7, %ymm7
	vextractf128	$0x1, %ymm6, %xmm13
	vcvtps2pd	%xmm13, %ymm13
	vmulpd	.LC23(%rip), %ymm13, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm7, %xmm7
	vaddpd	%ymm1, %ymm2, %ymm2
	vmovsd	.LC26(%rip), %xmm1
	vsubpd	%ymm0, %ymm2, %ymm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm7, %ymm2
	vsubps	%ymm2, %ymm8, %ymm2
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	200(%rax), %xmm7, %xmm7
	vmovapd	%xmm7, %xmm3
	vxorpd	%xmm7, %xmm7, %xmm7
	vmovups	%ymm2, -4194284(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	208(%rax), %xmm2, %xmm2
	vmovsd	%xmm2, -4194480(%rbp)
	vcvtss2sd	216(%rax), %xmm7, %xmm7
	vcvtss2sd	192(%rax), %xmm0, %xmm0
	vmovapd	%xmm7, %xmm8
	vmulsd	%xmm3, %xmm1, %xmm7
	vmovsd	%xmm3, -4194416(%rbp)
	vmovsd	.LC22(%rip), %xmm3
	vmulsd	%xmm2, %xmm1, %xmm2
	vmovsd	%xmm8, -4194448(%rbp)
	vmulsd	.LC27(%rip), %xmm0, %xmm0
	vaddsd	%xmm7, %xmm0, %xmm0
	vmulsd	%xmm8, %xmm3, %xmm7
	vmovsd	.LC27(%rip), %xmm3
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm0
	vmovss	204(%rax), %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	%xmm8, %xmm1, %xmm7
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	224(%rax), %xmm8, %xmm8
	vmovss	%xmm0, -4194252(%rbp)
	vmulsd	-4194416(%rbp), %xmm3, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm8, %xmm2
	vaddsd	%xmm7, %xmm0, %xmm0
	vmovsd	%xmm2, -4194576(%rbp)
	vsubsd	%xmm2, %xmm0, %xmm0
	vmovss	212(%rax), %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm8, %xmm2
	vmovss	%xmm0, -4194248(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	232(%rax), %xmm0, %xmm0
	vmovsd	%xmm0, -4194416(%rbp)
	vmulsd	-4194480(%rbp), %xmm3, %xmm0
	vmovsd	.LC22(%rip), %xmm3
	vaddsd	%xmm0, %xmm7, %xmm0
	vmulsd	-4194416(%rbp), %xmm3, %xmm7
	vmovsd	.LC27(%rip), %xmm3
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm0
	vmovss	220(%rax), %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	-4194416(%rbp), %xmm1, %xmm7
	vmovss	%xmm0, -4194244(%rbp)
	vmulsd	-4194448(%rbp), %xmm3, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	240(%rax), %xmm2, %xmm2
	vmulsd	%xmm1, %xmm2, %xmm1
	vmulsd	.LC22(%rip), %xmm2, %xmm3
	vaddsd	%xmm7, %xmm0, %xmm0
	vsubsd	%xmm3, %xmm0, %xmm0
	vmovss	228(%rax), %xmm3
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm0, -4194240(%rbp)
	vmulsd	.LC27(%rip), %xmm8, %xmm0
	vaddsd	%xmm7, %xmm0, %xmm0
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	248(%rax), %xmm7, %xmm7
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm7, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	236(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194236(%rbp)
	vmovsd	-4194576(%rbp), %xmm1
	vmovsd	.LC20(%rip), %xmm3
	vmovaps	%ymm12, -2097200(%rbp)
	vmulsd	-4194416(%rbp), %xmm3, %xmm0
	vmovaps	%ymm9, -2097168(%rbp)
	vmulsd	.LC28(%rip), %xmm8, %xmm8
	vmovaps	%ymm6, -2097136(%rbp)
	vsubsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm3, %xmm7, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	244(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vmovss	%xmm0, -4194232(%rbp)
	vmovsd	-4194416(%rbp), %xmm0
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm8, %xmm8
	vmulsd	%xmm1, %xmm2, %xmm0
	vmulsd	%xmm1, %xmm7, %xmm1
	vsubsd	%xmm0, %xmm8, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	252(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194228(%rbp)
	vmovss	192(%rax), %xmm0
	vmovss	%xmm0, -2097104(%rbp)
	vmovss	200(%rax), %xmm0
	vmovss	%xmm0, -2097100(%rbp)
	vmovss	208(%rax), %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vmovss	216(%rax), %xmm0
	vmovss	%xmm0, -2097092(%rbp)
	vmovss	224(%rax), %xmm0
	vmovss	%xmm0, -2097088(%rbp)
	vmovss	232(%rax), %xmm0
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	240(%rax), %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	248(%rax), %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	testb	%r14b, %r14b
	je	.L897
	vmovaps	-4194352(%rbp), %ymm1
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm4, %ymm0
	vaddpd	%ymm1, %ymm5, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194320(%rbp), %ymm1
	vmovaps	%ymm0, -2097200(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	-4194512(%rbp), %ymm0, %ymm0
	vaddpd	-4194544(%rbp), %ymm1, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194288(%rbp), %ymm1
	vmovaps	%ymm0, -2097168(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm14, %ymm0
	vaddpd	%ymm1, %ymm13, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194256(%rbp), %ymm1
	vmovaps	%ymm0, -2097136(%rbp)
	vmovaps	-2097104(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2097104(%rbp)
.L897:
	movq	-2097200(%rbp), %rdx
	addq	$1024, %rax
	movq	-4194352(%rbp), %rsi
	movq	%rdx, -1024(%rax)
	movq	-2097192(%rbp), %rdx
	movq	%rdx, -1016(%rax)
	movq	-2097184(%rbp), %rdx
	movq	%rdx, -1008(%rax)
	movq	-2097176(%rbp), %rdx
	movq	%rdx, -1000(%rax)
	movq	-2097168(%rbp), %rdx
	movq	%rdx, -992(%rax)
	movq	-2097160(%rbp), %rdx
	movq	%rdx, -984(%rax)
	movq	-2097152(%rbp), %rdx
	movq	%rdx, -976(%rax)
	movq	-2097144(%rbp), %rdx
	movq	%rdx, -968(%rax)
	movq	-2097136(%rbp), %rdx
	movq	%rdx, -960(%rax)
	movq	-2097128(%rbp), %rdx
	movq	%rdx, -952(%rax)
	movq	-2097120(%rbp), %rdx
	movq	%rdx, -944(%rax)
	movq	-2097112(%rbp), %rdx
	movq	%rdx, -936(%rax)
	movq	-2097104(%rbp), %rdx
	movq	%rdx, -928(%rax)
	movq	-2097096(%rbp), %rdx
	movq	%rdx, -920(%rax)
	movq	-2097088(%rbp), %rdx
	movq	%rdx, -912(%rax)
	movq	-2097080(%rbp), %rdx
	movq	%rdx, -904(%rax)
	movq	%rsi, -896(%rax)
	movq	-4194344(%rbp), %rsi
	movq	%rsi, -888(%rax)
	movq	-4194336(%rbp), %rsi
	movq	%rsi, -880(%rax)
	movq	-4194328(%rbp), %rsi
	movq	%rsi, -872(%rax)
	movq	-4194320(%rbp), %rsi
	movq	%rsi, -864(%rax)
	movq	-4194312(%rbp), %rsi
	movq	%rsi, -856(%rax)
	movq	-4194304(%rbp), %rsi
	movq	%rsi, -848(%rax)
	movq	-4194296(%rbp), %rsi
	movq	%rsi, -840(%rax)
	movq	-4194288(%rbp), %rsi
	movq	%rsi, -832(%rax)
	movq	-4194280(%rbp), %rsi
	movq	%rsi, -824(%rax)
	movq	-4194272(%rbp), %rsi
	movq	%rsi, -816(%rax)
	movq	-4194264(%rbp), %rsi
	movq	%rsi, -808(%rax)
	movq	-4194256(%rbp), %rsi
	movq	%rsi, -800(%rax)
	movq	-4194248(%rbp), %rsi
	movq	%rsi, -792(%rax)
	movq	-4194240(%rbp), %rsi
	movq	%rsi, -784(%rax)
	movq	-4194232(%rbp), %rsi
	movq	%rsi, -776(%rax)
	subl	$1, %ecx
	jne	.L891
.L1042:
	addq	$262144, %r9
	cmpq	%r12, %r9
	jne	.L898
	movq	-4194376(%rbp), %r11
	xorl	%r10d, %r10d
.L899:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$64, %r8d
	je	.L900
.L1043:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L901:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$64, %edx
	jne	.L901
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$64, %r8d
	jne	.L1043
.L900:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$64, %r10d
	jne	.L899
	movq	-4194360(%rbp), %rsi
	vmovaps	.LC32(%rip), %ymm15
	vmovapd	.LC38(%rip), %ymm14
	vmovapd	.LC39(%rip), %ymm13
.L904:
	movq	%rsi, %rax
	movl	$64, %edx
	jmp	.L909
.L905:
	vmovups	32(%rax), %ymm4
	addq	$1024, %rax
	vmovups	-1024(%rax), %ymm0
	vmovups	-928(%rax), %ymm5
	vshufps	$136, %ymm4, %ymm0, %ymm2
	vshufps	$221, %ymm4, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm3
	vshufps	$68, %ymm3, %ymm2, %ymm1
	vshufps	$238, %ymm3, %ymm2, %ymm3
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-960(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm3, %ymm3
	vaddps	%ymm3, %ymm1, %ymm3
	vmovups	-864(%rax), %ymm6
	vshufps	$136, %ymm5, %ymm0, %ymm4
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm1
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm4
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-896(%rax), %ymm0
	vmulps	%ymm15, %ymm3, %ymm3
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vmovups	-800(%rax), %ymm7
	vaddps	%ymm2, %ymm1, %ymm1
	vcvtps2pd	%xmm3, %ymm9
	vmovaps	%ymm3, -4194352(%rbp)
	vshufps	$136, %ymm6, %ymm0, %ymm5
	vshufps	$221, %ymm6, %ymm0, %ymm0
	vperm2f128	$3, %ymm5, %ymm5, %ymm4
	vshufps	$68, %ymm4, %ymm5, %ymm2
	vshufps	$238, %ymm4, %ymm5, %ymm4
	vinsertf128	$1, %xmm4, %ymm2, %ymm2
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vshufps	$68, %ymm4, %ymm0, %ymm5
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vinsertf128	$1, %xmm4, %ymm5, %ymm0
	vaddps	%ymm0, %ymm2, %ymm0
	vmulps	%ymm15, %ymm1, %ymm1
	vmovups	-832(%rax), %ymm4
	vmovaps	%ymm1, -4194320(%rbp)
	vmovups	-984(%rax), %ymm11
	vshufps	$136, %ymm7, %ymm4, %ymm6
	vshufps	$221, %ymm7, %ymm4, %ymm4
	vperm2f128	$3, %ymm6, %ymm6, %ymm5
	vshufps	$68, %ymm5, %ymm6, %ymm2
	vshufps	$238, %ymm5, %ymm6, %ymm5
	vinsertf128	$1, %xmm5, %ymm2, %ymm2
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vmulps	%ymm15, %ymm0, %ymm0
	vshufps	$68, %ymm5, %ymm4, %ymm6
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vxorpd	%xmm4, %xmm4, %xmm4
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vcvtss2sd	-4194348(%rbp), %xmm4, %xmm4
	vaddps	%ymm5, %ymm2, %ymm2
	vmulsd	.LC33(%rip), %xmm4, %xmm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194352(%rbp), %xmm5, %xmm5
	vmovss	-1020(%rax), %xmm7
	vmovaps	%ymm0, -4194288(%rbp)
	vmulsd	.LC35(%rip), %xmm5, %xmm6
	vmulsd	.LC36(%rip), %xmm5, %xmm5
	vmulps	%ymm15, %ymm2, %ymm2
	vmovaps	%ymm2, -4194256(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194344(%rbp), %xmm2, %xmm2
	vsubss	-1024(%rax), %xmm7, %xmm7
	vmulsd	.LC34(%rip), %xmm2, %xmm2
	vaddsd	%xmm4, %xmm6, %xmm6
	vsubsd	%xmm4, %xmm5, %xmm5
	vsubsd	%xmm2, %xmm6, %xmm6
	vaddsd	%xmm5, %xmm2, %xmm2
	vmovups	-4194344(%rbp), %ymm5
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vcvtps2pd	%xmm5, %ymm8
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm14, %ymm8, %ymm8
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm6, %xmm2
	vmovups	-1016(%rax), %ymm6
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	%ymm14, %ymm5, %ymm5
	vshufps	$221, %ymm11, %ymm6, %ymm10
	vshufps	$136, %ymm11, %ymm6, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm11
	vsubss	%xmm2, %xmm7, %xmm2
	vmovups	-4194348(%rbp), %ymm7
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vcvtps2pd	%xmm7, %ymm4
	vmovss	%xmm2, -2097200(%rbp)
	vextractf128	$0x1, %ymm3, %xmm2
	vextractf128	$0x1, %ymm7, %xmm3
	vcvtps2pd	%xmm2, %ymm2
	vperm2f128	$3, %ymm10, %ymm10, %ymm7
	vcvtps2pd	%xmm3, %ymm3
	vshufps	$68, %ymm7, %ymm10, %ymm12
	vshufps	$238, %ymm7, %ymm10, %ymm7
	vshufps	$68, %ymm11, %ymm6, %ymm10
	vinsertf128	$1, %xmm7, %ymm12, %ymm7
	vshufps	$238, %ymm11, %ymm6, %ymm11
	vmulpd	%ymm13, %ymm9, %ymm6
	vinsertf128	$1, %xmm11, %ymm10, %ymm10
	vaddpd	%ymm4, %ymm6, %ymm6
	vmulpd	%ymm14, %ymm9, %ymm9
	vsubps	%ymm10, %ymm7, %ymm7
	vmulpd	%ymm13, %ymm2, %ymm10
	vmulpd	%ymm14, %ymm2, %ymm2
	vaddpd	%ymm9, %ymm4, %ymm4
	vaddpd	%ymm3, %ymm10, %ymm10
	vaddpd	%ymm2, %ymm3, %ymm3
	vaddpd	%ymm8, %ymm6, %ymm6
	vaddpd	%ymm5, %ymm10, %ymm10
	vsubpd	%ymm8, %ymm4, %ymm8
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtpd2psy	%ymm6, %xmm6
	vmovups	-4194312(%rbp), %ymm5
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm10, %ymm6, %ymm6
	vmovups	-920(%rax), %ymm10
	vcvtpd2psy	%ymm8, %xmm2
	vcvtps2pd	%xmm1, %ymm8
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vsubps	%ymm2, %ymm6, %ymm2
	vmovups	-952(%rax), %ymm6
	vshufps	$221, %ymm10, %ymm6, %ymm9
	vshufps	$136, %ymm10, %ymm6, %ymm10
	vperm2f128	$3, %ymm10, %ymm10, %ymm6
	vsubps	%ymm2, %ymm7, %ymm7
	vmulps	%ymm15, %ymm7, %ymm2
	vmovups	%ymm2, -2097196(%rbp)
	vcvtps2pd	%xmm5, %ymm7
	vmovups	-4194316(%rbp), %ymm2
	vmulpd	%ymm14, %ymm7, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	%ymm14, %ymm5, %ymm5
	vcvtps2pd	%xmm2, %ymm3
	vextractf128	$0x1, %ymm2, %xmm1
	vperm2f128	$3, %ymm9, %ymm9, %ymm2
	vshufps	$68, %ymm2, %ymm9, %ymm11
	vshufps	$238, %ymm2, %ymm9, %ymm2
	vshufps	$68, %ymm6, %ymm10, %ymm9
	vshufps	$238, %ymm6, %ymm10, %ymm6
	vinsertf128	$1, %xmm6, %ymm9, %ymm6
	vmulpd	%ymm13, %ymm8, %ymm9
	vmulpd	%ymm14, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm9, %ymm9
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm8, %ymm3, %ymm3
	vinsertf128	$1, %xmm2, %ymm11, %ymm2
	vsubps	%ymm6, %ymm2, %ymm6
	vmulpd	%ymm13, %ymm4, %ymm2
	vaddpd	%ymm7, %ymm9, %ymm9
	vaddpd	%ymm1, %ymm2, %ymm2
	vsubpd	%ymm7, %ymm3, %ymm7
	vmulpd	%ymm14, %ymm4, %ymm3
	vaddpd	%ymm3, %ymm1, %ymm3
	vcvtpd2psy	%ymm9, %xmm9
	vaddpd	%ymm5, %ymm2, %ymm2
	vcvtpd2psy	%ymm7, %xmm1
	vcvtps2pd	%xmm0, %ymm7
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm5, %ymm3, %ymm3
	vcvtps2pd	%xmm0, %ymm5
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm9, %ymm2
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm1, %ymm1
	vsubps	%ymm1, %ymm2, %ymm1
	vsubps	%ymm1, %ymm6, %ymm1
	vmulps	%ymm15, %ymm1, %ymm1
	vmovups	%ymm1, -2097164(%rbp)
	vmovups	-888(%rax), %ymm3
	vmovups	-856(%rax), %ymm8
	vmovups	-4194284(%rbp), %ymm4
	vshufps	$221, %ymm8, %ymm3, %ymm9
	vshufps	$136, %ymm8, %ymm3, %ymm3
	vperm2f128	$3, %ymm3, %ymm3, %ymm8
	vmovups	-4194280(%rbp), %ymm2
	vcvtps2pd	%xmm4, %ymm1
	vextractf128	$0x1, %ymm4, %xmm0
	vperm2f128	$3, %ymm9, %ymm9, %ymm4
	vcvtps2pd	%xmm0, %ymm0
	vshufps	$68, %ymm4, %ymm9, %ymm10
	vshufps	$238, %ymm4, %ymm9, %ymm4
	vshufps	$68, %ymm8, %ymm3, %ymm9
	vinsertf128	$1, %xmm4, %ymm10, %ymm4
	vshufps	$238, %ymm8, %ymm3, %ymm8
	vmulpd	%ymm13, %ymm5, %ymm3
	vinsertf128	$1, %xmm8, %ymm9, %ymm8
	vaddpd	%ymm0, %ymm3, %ymm3
	vsubps	%ymm8, %ymm4, %ymm4
	vmulpd	%ymm14, %ymm5, %ymm5
	vmulpd	%ymm13, %ymm7, %ymm8
	vmulpd	%ymm14, %ymm7, %ymm7
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm6
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm14, %ymm6, %ymm6
	vaddpd	%ymm1, %ymm8, %ymm8
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm14, %ymm2, %ymm2
	vmovsd	.LC40(%rip), %xmm10
	vaddpd	%ymm7, %ymm1, %ymm1
	vmovss	-820(%rax), %xmm7
	vaddpd	%ymm2, %ymm3, %ymm3
	vmovss	-812(%rax), %xmm9
	vsubpd	%ymm2, %ymm0, %ymm2
	vsubpd	%ymm6, %ymm1, %ymm1
	vaddpd	%ymm6, %ymm8, %ymm8
	vcvtpd2psy	%ymm3, %xmm3
	vsubss	-824(%rax), %xmm7, %xmm7
	vcvtpd2psy	%ymm1, %xmm0
	vcvtpd2psy	%ymm2, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtpd2psy	%ymm8, %xmm8
	vinsertf128	$0x1, %xmm3, %ymm8, %ymm3
	vsubps	%ymm0, %ymm3, %ymm0
	vcvtss2sd	-4194248(%rbp), %xmm1, %xmm1
	vmulsd	.LC34(%rip), %xmm1, %xmm3
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-4194252(%rbp), %xmm8, %xmm8
	vsubss	-816(%rax), %xmm9, %xmm9
	vsubps	%ymm0, %ymm4, %ymm0
	vmulps	%ymm15, %ymm0, %ymm0
	vmovups	%ymm0, -2097132(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194256(%rbp), %xmm0, %xmm0
	vmulsd	%xmm10, %xmm0, %xmm5
	vmulsd	.LC34(%rip), %xmm0, %xmm0
	vaddsd	%xmm8, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm8, %xmm6
	vaddsd	%xmm3, %xmm5, %xmm5
	vsubsd	%xmm3, %xmm6, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulsd	%xmm10, %xmm8, %xmm5
	vmulsd	.LC34(%rip), %xmm8, %xmm8
	vsubss	%xmm0, %xmm7, %xmm2
	vmovss	-804(%rax), %xmm7
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vaddsd	%xmm1, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm1, %xmm8
	vmulsd	%xmm10, %xmm1, %xmm1
	vsubss	-808(%rax), %xmm7, %xmm7
	vmovss	%xmm2, -2097100(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194244(%rbp), %xmm2, %xmm2
	vmulsd	.LC34(%rip), %xmm2, %xmm6
	vaddsd	%xmm2, %xmm1, %xmm1
	vsubsd	%xmm6, %xmm8, %xmm0
	vaddsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm2, %xmm8
	vmulsd	%xmm10, %xmm2, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm0, %xmm5, %xmm0
	vsubss	%xmm0, %xmm9, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194240(%rbp), %xmm0, %xmm0
	vmulsd	.LC34(%rip), %xmm0, %xmm5
	vmulss	.LC37(%rip), %xmm4, %xmm4
	vaddsd	%xmm0, %xmm2, %xmm2
	vsubsd	%xmm5, %xmm8, %xmm3
	vmovss	%xmm4, -2097096(%rbp)
	vaddsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm0, %xmm8
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4194236(%rbp), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm3, %xmm5
	vsubss	%xmm1, %xmm7, %xmm1
	vmovss	-796(%rax), %xmm7
	vmulss	.LC37(%rip), %xmm1, %xmm1
	vsubss	-800(%rax), %xmm7, %xmm7
	vmovss	%xmm1, -2097092(%rbp)
	vmulsd	.LC34(%rip), %xmm3, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm8, %xmm4
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm2, %xmm2
	vmulsd	%xmm10, %xmm0, %xmm4
	vsubss	%xmm2, %xmm7, %xmm2
	vmulss	.LC37(%rip), %xmm2, %xmm2
	vaddsd	%xmm3, %xmm4, %xmm4
	vmovss	%xmm2, -2097088(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194232(%rbp), %xmm2, %xmm2
	vmulsd	.LC34(%rip), %xmm2, %xmm6
	vmovss	-788(%rax), %xmm7
	vsubss	-792(%rax), %xmm7, %xmm7
	movq	-4194352(%rbp), %rcx
	vsubsd	%xmm6, %xmm5, %xmm0
	vaddsd	%xmm6, %xmm4, %xmm4
	vmovss	-780(%rax), %xmm6
	movq	%rcx, -1024(%rax)
	movq	-4194344(%rbp), %rcx
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	-784(%rax), %xmm6, %xmm6
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm10, %xmm3, %xmm0
	movq	%rcx, -1016(%rax)
	movq	-4194336(%rbp), %rcx
	vsubss	%xmm4, %xmm7, %xmm4
	vmulss	.LC37(%rip), %xmm4, %xmm4
	vaddsd	%xmm1, %xmm2, %xmm7
	vaddsd	%xmm2, %xmm0, %xmm3
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	movq	%rcx, -1008(%rax)
	movq	-4194328(%rbp), %rcx
	vmovss	%xmm4, -2097084(%rbp)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194228(%rbp), %xmm4, %xmm4
	vmulsd	.LC34(%rip), %xmm4, %xmm5
	vsubsd	%xmm2, %xmm1, %xmm1
	movq	%rcx, -1000(%rax)
	movq	-4194320(%rbp), %rcx
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm4, %xmm0
	vaddsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm7, %xmm5
	movq	%rcx, -992(%rax)
	movq	-4194312(%rbp), %rcx
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm3, %xmm3
	vmovss	-772(%rax), %xmm5
	vaddsd	%xmm0, %xmm2, %xmm0
	vsubss	-776(%rax), %xmm5, %xmm5
	movq	%rcx, -984(%rax)
	vsubss	%xmm3, %xmm6, %xmm3
	movq	-4194304(%rbp), %rcx
	vmulss	.LC37(%rip), %xmm3, %xmm3
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	movq	%rcx, -976(%rax)
	vmovss	%xmm3, -2097080(%rbp)
	vmulsd	.LC36(%rip), %xmm4, %xmm3
	vaddsd	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm0, %xmm1, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	.LC37(%rip), %xmm0, %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	movq	-4194296(%rbp), %rcx
	movq	-2097200(%rbp), %rdi
	movq	%rcx, -968(%rax)
	movq	-4194288(%rbp), %rcx
	movq	%rdi, -896(%rax)
	movq	-2097192(%rbp), %rdi
	movq	%rcx, -960(%rax)
	movq	-4194280(%rbp), %rcx
	movq	%rdi, -888(%rax)
	movq	-2097184(%rbp), %rdi
	movq	%rcx, -952(%rax)
	movq	-4194272(%rbp), %rcx
	movq	%rdi, -880(%rax)
	movq	-2097176(%rbp), %rdi
	movq	%rcx, -944(%rax)
	movq	-4194264(%rbp), %rcx
	movq	%rdi, -872(%rax)
	movq	-2097168(%rbp), %rdi
	movq	%rcx, -936(%rax)
	movq	-4194256(%rbp), %rcx
	movq	%rdi, -864(%rax)
	movq	-2097160(%rbp), %rdi
	movq	%rcx, -928(%rax)
	movq	-4194248(%rbp), %rcx
	movq	%rdi, -856(%rax)
	movq	-2097152(%rbp), %rdi
	movq	%rcx, -920(%rax)
	movq	-4194240(%rbp), %rcx
	movq	%rdi, -848(%rax)
	movq	-2097144(%rbp), %rdi
	movq	%rcx, -912(%rax)
	movq	-4194232(%rbp), %rcx
	movq	%rcx, -904(%rax)
	movq	%rdi, -840(%rax)
	movq	-2097136(%rbp), %rdi
	movq	%rdi, -832(%rax)
	movq	-2097128(%rbp), %rdi
	movq	%rdi, -824(%rax)
	movq	-2097120(%rbp), %rdi
	movq	%rdi, -816(%rax)
	movq	-2097112(%rbp), %rdi
	movq	%rdi, -808(%rax)
	movq	-2097104(%rbp), %rdi
	movq	%rdi, -800(%rax)
	movq	-2097096(%rbp), %rdi
	movq	%rdi, -792(%rax)
	movq	-2097088(%rbp), %rdi
	movq	%rdi, -784(%rax)
	movq	-2097080(%rbp), %rdi
	movq	%rdi, -776(%rax)
	subl	$1, %edx
	je	.L1044
.L909:
	cmpl	$1, %ebx
	ja	.L905
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rax), %xmm1, %xmm1
	vcvtss2sd	8(%rax), %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm1, %xmm1
	vmulsd	.LC21(%rip), %xmm0, %xmm0
	vmovups	40(%rax), %ymm5
	vmovups	104(%rax), %ymm8
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%rax), %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
	vmovups	8(%rax), %ymm0
	vshufps	$136, %ymm5, %ymm0, %ymm2
	vshufps	$221, %ymm5, %ymm0, %ymm0
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm3
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vmovups	24(%rax), %ymm2
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 56(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm4
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovups	16(%rax), %ymm4
	vshufps	$136, 48(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm2
	vshufps	$68, %ymm2, %ymm4, %ymm6
	vshufps	$238, %ymm2, %ymm4, %ymm2
	vmovups	(%rax), %ymm4
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vshufps	$136, 32(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm6
	vshufps	$68, %ymm6, %ymm4, %ymm11
	vshufps	$238, %ymm6, %ymm4, %ymm6
	vperm2f128	$3, %ymm0, %ymm0, %ymm4
	vinsertf128	$1, %xmm6, %ymm11, %ymm11
	vshufps	$68, %ymm4, %ymm0, %ymm5
	vcvtps2pd	%xmm3, %ymm6
	vshufps	$238, %ymm4, %ymm0, %ymm4
	vmulpd	.LC24(%rip), %ymm6, %ymm6
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vcvtps2pd	%xmm11, %ymm5
	vmulpd	.LC23(%rip), %ymm5, %ymm0
	vaddpd	%ymm6, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm6
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	.LC24(%rip), %ymm6, %ymm6
	vcvtps2pd	%xmm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vaddpd	%ymm6, %ymm0, %ymm0
	vcvtps2pd	%xmm1, %ymm6
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm6, %ymm6
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vsubpd	%ymm6, %ymm0, %ymm0
	vextractf128	$0x1, %ymm11, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vmulpd	.LC23(%rip), %ymm6, %ymm7
	vaddpd	%ymm3, %ymm7, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vaddpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm1, %ymm2, %ymm1
	vmovups	72(%rax), %ymm2
	vshufps	$136, %ymm8, %ymm2, %ymm3
	vshufps	$221, %ymm8, %ymm2, %ymm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm1
	vsubps	%ymm1, %ymm4, %ymm1
	vperm2f128	$3, %ymm3, %ymm3, %ymm0
	vmovups	%ymm1, -4194348(%rbp)
	vshufps	$68, %ymm0, %ymm3, %ymm1
	vshufps	$238, %ymm0, %ymm3, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vmovups	88(%rax), %ymm0
	vshufps	$136, 120(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm4
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovups	80(%rax), %ymm4
	vshufps	$136, 112(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm0
	vshufps	$68, %ymm0, %ymm4, %ymm7
	vshufps	$238, %ymm0, %ymm4, %ymm0
	vmovups	64(%rax), %ymm4
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vshufps	$136, 96(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm7
	vshufps	$68, %ymm7, %ymm4, %ymm10
	vshufps	$238, %ymm7, %ymm4, %ymm7
	vperm2f128	$3, %ymm2, %ymm2, %ymm4
	vinsertf128	$1, %xmm7, %ymm10, %ymm10
	vshufps	$68, %ymm4, %ymm2, %ymm7
	vshufps	$238, %ymm4, %ymm2, %ymm4
	vcvtps2pd	%xmm1, %ymm2
	vinsertf128	$1, %xmm4, %ymm7, %ymm4
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vcvtps2pd	%xmm10, %ymm7
	vmovapd	%ymm7, -4194512(%rbp)
	vmulpd	.LC23(%rip), %ymm7, %ymm7
	vaddpd	%ymm2, %ymm7, %ymm7
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vaddpd	%ymm2, %ymm7, %ymm2
	vcvtps2pd	%xmm3, %ymm7
	vmovups	168(%rax), %ymm8
	vmulpd	.LC25(%rip), %ymm7, %ymm7
	vsubpd	%ymm7, %ymm2, %ymm2
	vextractf128	$0x1, %ymm10, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmovapd	%ymm7, -4194544(%rbp)
	vmulpd	.LC23(%rip), %ymm7, %ymm7
	vaddpd	%ymm1, %ymm7, %ymm1
	vcvtpd2psy	%ymm2, %xmm2
	vaddpd	%ymm0, %ymm1, %ymm1
	vextractf128	$0x1, %ymm3, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vmovups	136(%rax), %ymm3
	vsubpd	%ymm0, %ymm1, %ymm0
	vshufps	$136, %ymm8, %ymm3, %ymm1
	vshufps	$221, %ymm8, %ymm3, %ymm3
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm0, %ymm4, %ymm0
	vmovups	%ymm0, -4194316(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	152(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	vshufps	$136, 184(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm4
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmovups	144(%rax), %ymm4
	vshufps	$136, 176(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm1
	vshufps	$68, %ymm1, %ymm4, %ymm7
	vshufps	$238, %ymm1, %ymm4, %ymm1
	vmovups	128(%rax), %ymm4
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	vshufps	$136, 160(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm7
	vshufps	$68, %ymm7, %ymm4, %ymm9
	vshufps	$238, %ymm7, %ymm4, %ymm7
	vperm2f128	$3, %ymm3, %ymm3, %ymm4
	vinsertf128	$1, %xmm7, %ymm9, %ymm7
	vshufps	$68, %ymm4, %ymm3, %ymm9
	vshufps	$238, %ymm4, %ymm3, %ymm4
	vinsertf128	$1, %xmm4, %ymm9, %ymm9
	vcvtps2pd	%xmm7, %ymm4
	vmulpd	.LC23(%rip), %ymm4, %ymm8
	vmovapd	%ymm4, -4194576(%rbp)
	vcvtps2pd	%xmm2, %ymm4
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vaddpd	%ymm4, %ymm8, %ymm8
	vcvtps2pd	%xmm1, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm4, %ymm8, %ymm4
	vcvtps2pd	%xmm0, %ymm8
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	.LC25(%rip), %ymm8, %ymm8
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vsubpd	%ymm8, %ymm4, %ymm8
	vextractf128	$0x1, %ymm7, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vmulpd	.LC23(%rip), %ymm4, %ymm12
	vaddpd	%ymm2, %ymm12, %ymm2
	vcvtpd2psy	%ymm8, %xmm8
	vaddpd	%ymm1, %ymm2, %ymm2
	vsubpd	%ymm0, %ymm2, %ymm2
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm8, %ymm2
	vsubps	%ymm2, %ymm9, %ymm2
	vmovups	%ymm2, -4194284(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	200(%rax), %xmm2, %xmm2
	vmovapd	%xmm2, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	208(%rax), %xmm2, %xmm2
	vmovsd	.LC26(%rip), %xmm0
	vmovsd	%xmm2, -4194416(%rbp)
	vmovsd	.LC27(%rip), %xmm8
	vmovsd	%xmm9, -4194480(%rbp)
	vmulsd	%xmm2, %xmm0, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	216(%rax), %xmm2, %xmm2
	vmovapd	%xmm2, %xmm12
	vmulsd	%xmm9, %xmm0, %xmm2
	vmovsd	%xmm12, -4194448(%rbp)
	vmovapd	%xmm1, %xmm3
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	192(%rax), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm12, %xmm0, %xmm2
	vaddsd	%xmm3, %xmm1, %xmm9
	vmovsd	.LC22(%rip), %xmm1
	vmulsd	%xmm12, %xmm1, %xmm1
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	224(%rax), %xmm12, %xmm12
	vsubsd	%xmm1, %xmm9, %xmm1
	vmovss	204(%rax), %xmm9
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm9, %xmm9
	vmulsd	.LC22(%rip), %xmm12, %xmm1
	vmovss	%xmm9, -4194252(%rbp)
	vmulsd	-4194480(%rbp), %xmm8, %xmm9
	vmovsd	%xmm1, -4194480(%rbp)
	vaddsd	%xmm3, %xmm9, %xmm9
	vaddsd	%xmm2, %xmm9, %xmm9
	vsubsd	%xmm1, %xmm9, %xmm1
	vmovss	212(%rax), %xmm9
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm9, %xmm9
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	232(%rax), %xmm1, %xmm1
	vmovss	%xmm9, -4194248(%rbp)
	vmulsd	%xmm0, %xmm12, %xmm9
	vmovapd	%xmm9, %xmm3
	vmulsd	-4194416(%rbp), %xmm8, %xmm9
	vmovsd	%xmm3, -4194416(%rbp)
	vaddsd	%xmm9, %xmm2, %xmm9
	vmovapd	%xmm1, %xmm2
	vaddsd	%xmm3, %xmm9, %xmm9
	vmovsd	.LC22(%rip), %xmm3
	vmulsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm1, %xmm9, %xmm1
	vmovss	220(%rax), %xmm9
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm9, %xmm9
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	240(%rax), %xmm1, %xmm1
	vmovss	%xmm9, -4194244(%rbp)
	vmulsd	-4194448(%rbp), %xmm8, %xmm9
	vmovsd	%xmm2, -4194448(%rbp)
	vmulsd	%xmm2, %xmm0, %xmm2
	vaddsd	-4194416(%rbp), %xmm9, %xmm9
	vmovsd	%xmm1, -4194416(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm1
	vmulsd	-4194416(%rbp), %xmm0, %xmm0
	vaddsd	%xmm2, %xmm9, %xmm9
	vsubsd	%xmm1, %xmm9, %xmm1
	vmovss	228(%rax), %xmm9
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm9, %xmm9
	vmulsd	%xmm8, %xmm12, %xmm1
	vmovss	%xmm9, -4194240(%rbp)
	vaddsd	%xmm2, %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	248(%rax), %xmm2, %xmm2
	vaddsd	%xmm0, %xmm1, %xmm8
	vmulsd	%xmm3, %xmm2, %xmm0
	vsubsd	%xmm0, %xmm8, %xmm0
	vmovss	236(%rax), %xmm8
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm8, %xmm8
	vmovss	%xmm8, -4194236(%rbp)
	vmovsd	.LC20(%rip), %xmm3
	vmovsd	-4194448(%rbp), %xmm9
	vmovaps	%ymm11, -2097200(%rbp)
	vmovsd	-4194480(%rbp), %xmm8
	vmovaps	%ymm10, -2097168(%rbp)
	vmulsd	%xmm9, %xmm3, %xmm0
	vmovaps	%ymm7, -2097136(%rbp)
	vmovsd	.LC21(%rip), %xmm3
	vmulsd	.LC28(%rip), %xmm12, %xmm12
	vsubsd	%xmm0, %xmm8, %xmm1
	vmulsd	-4194416(%rbp), %xmm3, %xmm0
	vmovss	244(%rax), %xmm8
	vaddsd	%xmm0, %xmm1, %xmm0
	vmulsd	.LC20(%rip), %xmm2, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm8, %xmm8
	vmulsd	.LC29(%rip), %xmm9, %xmm0
	vmovss	%xmm8, -4194232(%rbp)
	vmovss	252(%rax), %xmm8
	vaddsd	%xmm0, %xmm12, %xmm12
	vmulsd	-4194416(%rbp), %xmm1, %xmm0
	vsubsd	%xmm0, %xmm12, %xmm12
	vmulsd	%xmm1, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm12, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm8, %xmm8
	vmovss	192(%rax), %xmm0
	vmovss	%xmm0, -2097104(%rbp)
	vmovss	200(%rax), %xmm0
	vmovss	%xmm8, -4194228(%rbp)
	vmovss	%xmm0, -2097100(%rbp)
	vmovss	208(%rax), %xmm0
	vmovss	%xmm0, -2097096(%rbp)
	vmovss	216(%rax), %xmm0
	vmovss	%xmm0, -2097092(%rbp)
	vmovss	224(%rax), %xmm0
	vmovss	%xmm0, -2097088(%rbp)
	vmovss	232(%rax), %xmm0
	vmovss	%xmm0, -2097084(%rbp)
	vmovss	240(%rax), %xmm0
	vmovss	%xmm0, -2097080(%rbp)
	vmovss	248(%rax), %xmm0
	vmovss	%xmm0, -2097076(%rbp)
	testb	%r14b, %r14b
	je	.L908
	vmovaps	-4194352(%rbp), %ymm1
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	%ymm0, %ymm5, %ymm0
	vaddpd	%ymm1, %ymm6, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194320(%rbp), %ymm1
	vmovaps	%ymm0, -2097200(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	-4194512(%rbp), %ymm0, %ymm0
	vaddpd	-4194544(%rbp), %ymm1, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194288(%rbp), %ymm1
	vmovaps	%ymm0, -2097168(%rbp)
	vcvtps2pd	%xmm1, %ymm0
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vaddpd	-4194576(%rbp), %ymm0, %ymm0
	vaddpd	%ymm1, %ymm4, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm0, %ymm0
	vmovaps	-4194256(%rbp), %ymm1
	vmovaps	%ymm0, -2097136(%rbp)
	vmovaps	-2097104(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2097104(%rbp)
.L908:
	movq	-2097200(%rbp), %rcx
	addq	$1024, %rax
	movq	-4194352(%rbp), %rdi
	movq	%rcx, -1024(%rax)
	movq	-2097192(%rbp), %rcx
	movq	%rcx, -1016(%rax)
	movq	-2097184(%rbp), %rcx
	movq	%rcx, -1008(%rax)
	movq	-2097176(%rbp), %rcx
	movq	%rcx, -1000(%rax)
	movq	-2097168(%rbp), %rcx
	movq	%rcx, -992(%rax)
	movq	-2097160(%rbp), %rcx
	movq	%rcx, -984(%rax)
	movq	-2097152(%rbp), %rcx
	movq	%rcx, -976(%rax)
	movq	-2097144(%rbp), %rcx
	movq	%rcx, -968(%rax)
	movq	-2097136(%rbp), %rcx
	movq	%rcx, -960(%rax)
	movq	-2097128(%rbp), %rcx
	movq	%rcx, -952(%rax)
	movq	-2097120(%rbp), %rcx
	movq	%rcx, -944(%rax)
	movq	-2097112(%rbp), %rcx
	movq	%rcx, -936(%rax)
	movq	-2097104(%rbp), %rcx
	movq	%rcx, -928(%rax)
	movq	-2097096(%rbp), %rcx
	movq	%rcx, -920(%rax)
	movq	-2097088(%rbp), %rcx
	movq	%rcx, -912(%rax)
	movq	-2097080(%rbp), %rcx
	movq	%rcx, -904(%rax)
	movq	%rdi, -896(%rax)
	movq	-4194344(%rbp), %rdi
	movq	%rdi, -888(%rax)
	movq	-4194336(%rbp), %rdi
	movq	%rdi, -880(%rax)
	movq	-4194328(%rbp), %rdi
	movq	%rdi, -872(%rax)
	movq	-4194320(%rbp), %rdi
	movq	%rdi, -864(%rax)
	movq	-4194312(%rbp), %rdi
	movq	%rdi, -856(%rax)
	movq	-4194304(%rbp), %rdi
	movq	%rdi, -848(%rax)
	movq	-4194296(%rbp), %rdi
	movq	%rdi, -840(%rax)
	movq	-4194288(%rbp), %rdi
	movq	%rdi, -832(%rax)
	movq	-4194280(%rbp), %rdi
	movq	%rdi, -824(%rax)
	movq	-4194272(%rbp), %rdi
	movq	%rdi, -816(%rax)
	movq	-4194264(%rbp), %rdi
	movq	%rdi, -808(%rax)
	movq	-4194256(%rbp), %rdi
	movq	%rdi, -800(%rax)
	movq	-4194248(%rbp), %rdi
	movq	%rdi, -792(%rax)
	movq	-4194240(%rbp), %rdi
	movq	%rdi, -784(%rax)
	movq	-4194232(%rbp), %rdi
	movq	%rdi, -776(%rax)
	subl	$1, %edx
	jne	.L909
.L1044:
	addq	$262144, %rsi
	cmpq	%rsi, %r12
	jne	.L904
	movq	-4194368(%rbp), %rax
	movq	-4194360(%rbp), %r13
	leaq	8388632(%rax), %rbx
.L915:
	leaq	32768(%r13), %r12
	movq	%r13, %r14
.L911:
	movq	%r14, %rdi
	movl	%r15d, %esi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	cmpq	%r14, %r12
	jne	.L911
	leaq	1024(%r13), %r8
	xorl	%edi, %edi
	movq	%r13, %rsi
	addl	$1, %edi
	cmpl	$32, %edi
	je	.L960
.L1045:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L913:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$32, %eax
	jne	.L913
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$32, %edi
	jne	.L1045
.L960:
	movq	%r13, %r14
.L912:
	movq	%r14, %rdi
	movl	%r15d, %esi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	cmpq	%r14, %r12
	jne	.L912
	addq	$262144, %r13
	cmpq	%rbx, %r13
	jne	.L915
	movq	-4194376(%rbp), %r11
	xorl	%r8d, %r8d
.L916:
	xorl	%r9d, %r9d
	movslq	%r8d, %rdi
	movq	-4194360(%rbp), %rsi
	movq	%r11, %r10
	addl	$1, %r9d
	salq	$8, %rdi
	cmpl	$32, %r9d
	je	.L917
.L1046:
	movq	%r10, %rcx
	movl	%r9d, %edx
	.p2align 4,,10
	.p2align 3
.L918:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$32, %edx
	jne	.L918
	addl	$1, %r9d
	addq	$262148, %r10
	addq	$262144, %rsi
	cmpl	$32, %r9d
	jne	.L1046
.L917:
	addl	$1, %r8d
	addq	$1024, %r11
	cmpl	$32, %r8d
	jne	.L916
	movq	-4194360(%rbp), %r13
.L921:
	leaq	32768(%r13), %r14
	movq	%r13, %r12
.L922:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb1EEEvPfi
	cmpq	%r14, %r12
	jne	.L922
	addq	$262144, %r13
	cmpq	%r13, %rbx
	jne	.L921
	movq	-4194368(%rbp), %rax
	movq	-4194360(%rbp), %rbx
	addq	$4194328, %rax
	movq	%rax, -4194448(%rbp)
.L927:
	leaq	16384(%rbx), %rax
	movq	%rbx, %r12
	movq	%rax, -4194416(%rbp)
.L924:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	cmpq	%r12, -4194416(%rbp)
	jne	.L924
	movq	%rbx, %rax
	movq	%rbx, %rcx
	xorl	%edi, %edi
	movl	$15, -4194480(%rbp)
	movl	$9, %r13d
	movl	$8, %r12d
	movl	$7, %r11d
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %esi
	movl	$2, %edx
	jmp	.L957
	.p2align 4,,10
	.p2align 3
.L1047:
	vmovss	4(%rax), %xmm0
	vmovss	1024(%rax), %xmm1
	vmovss	%xmm0, 1024(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %edx
	ja	.L926
	movslq	%edx, %r14
	vmovss	2048(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 2048(%rax)
	cmpl	$15, %esi
	ja	.L926
	movslq	%esi, %r14
	vmovss	3072(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 3072(%rax)
	cmpl	$15, %r8d
	ja	.L926
	movslq	%r8d, %r14
	vmovss	4096(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 4096(%rax)
	cmpl	$15, %r9d
	ja	.L926
	movslq	%r9d, %r14
	vmovss	5120(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 5120(%rax)
	cmpl	$15, %r10d
	ja	.L926
	movslq	%r10d, %r14
	vmovss	6144(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 6144(%rax)
	cmpl	$15, %r11d
	ja	.L926
	movslq	%r11d, %r14
	vmovss	7168(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 7168(%rax)
	cmpl	$15, %r12d
	ja	.L926
	movslq	%r12d, %r14
	vmovss	8192(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 8192(%rax)
	cmpl	$15, %r13d
	ja	.L926
	movslq	%r13d, %r14
	vmovss	9216(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	9(%rdi), %r14d
	vmovss	%xmm0, 9216(%rax)
	cmpl	$15, %r14d
	ja	.L926
	movslq	%r14d, %r14
	vmovss	10240(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	10(%rdi), %r14d
	vmovss	%xmm0, 10240(%rax)
	cmpl	$15, %r14d
	ja	.L926
	movslq	%r14d, %r14
	vmovss	11264(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	11(%rdi), %r14d
	vmovss	%xmm0, 11264(%rax)
	cmpl	$15, %r14d
	ja	.L926
	movslq	%r14d, %r14
	vmovss	12288(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	12(%rdi), %r14d
	vmovss	%xmm0, 12288(%rax)
	cmpl	$15, %r14d
	ja	.L926
	movslq	%r14d, %r14
	vmovss	13312(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	leal	13(%rdi), %r14d
	vmovss	%xmm0, 13312(%rax)
	cmpl	$15, %r14d
	ja	.L926
	movslq	%r14d, %r14
	cmpl	$15, -4194480(%rbp)
	vmovss	14336(%rax), %xmm1
	leaq	(%rcx,%r14,4), %r14
	vmovss	(%r14), %xmm0
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, 14336(%rax)
	jne	.L926
	vmovss	60(%rcx), %xmm0
	vmovss	15360(%rax), %xmm1
	vmovss	%xmm1, 60(%rcx)
	vmovss	%xmm0, 15360(%rax)
.L926:
	addl	$1, -4194480(%rbp)
	addq	$1024, %rcx
	addl	$1, %edx
	addl	$1, %esi
	addq	$1028, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %r12d
	addl	$1, %r13d
.L957:
	addl	$1, %edi
	cmpl	$16, %edi
	jne	.L1047
	movq	%rbx, %r12
.L925:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	cmpq	%r12, -4194416(%rbp)
	jne	.L925
	addq	$262144, %rbx
	cmpq	-4194448(%rbp), %rbx
	jne	.L927
	movq	-4194360(%rbp), %rax
	movl	$0, -4194416(%rbp)
	movq	%rax, -4194480(%rbp)
.L928:
	movslq	-4194416(%rbp), %rdi
	movl	$8, %r13d
	xorl	%esi, %esi
	movl	$7, %r12d
	movq	-4194480(%rbp), %rax
	movl	$6, %ebx
	movl	$5, %r11d
	movl	$4, %r10d
	movq	-4194360(%rbp), %rdx
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$15, %r14d
	movq	%rdi, %rcx
	salq	$10, %rdi
	salq	$8, %rcx
	movq	%rdi, -4194512(%rbp)
	jmp	.L931
	.p2align 4,,10
	.p2align 3
.L1048:
	vmovss	4(%rax), %xmm0
	vmovss	262144(%rax), %xmm1
	vmovss	%xmm0, 262144(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %r8d
	ja	.L930
	movslq	%r8d, %rdi
	vmovss	524288(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 524288(%rax)
	cmpl	$15, %r9d
	ja	.L930
	movslq	%r9d, %rdi
	vmovss	786432(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 786432(%rax)
	cmpl	$15, %r10d
	ja	.L930
	movslq	%r10d, %rdi
	vmovss	1048576(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1048576(%rax)
	cmpl	$15, %r11d
	ja	.L930
	movslq	%r11d, %rdi
	vmovss	1310720(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1310720(%rax)
	cmpl	$15, %ebx
	ja	.L930
	movslq	%ebx, %rdi
	vmovss	1572864(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1572864(%rax)
	cmpl	$15, %r12d
	ja	.L930
	movslq	%r12d, %rdi
	vmovss	1835008(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1835008(%rax)
	cmpl	$15, %r13d
	ja	.L930
	movslq	%r13d, %rdi
	vmovss	2097152(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	8(%rsi), %edi
	vmovss	%xmm0, 2097152(%rax)
	cmpl	$15, %edi
	ja	.L930
	movslq	%edi, %rdi
	vmovss	2359296(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	9(%rsi), %edi
	vmovss	%xmm0, 2359296(%rax)
	cmpl	$15, %edi
	ja	.L930
	movslq	%edi, %rdi
	vmovss	2621440(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	10(%rsi), %edi
	vmovss	%xmm0, 2621440(%rax)
	cmpl	$15, %edi
	ja	.L930
	movslq	%edi, %rdi
	vmovss	2883584(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	11(%rsi), %edi
	vmovss	%xmm0, 2883584(%rax)
	cmpl	$15, %edi
	ja	.L930
	movslq	%edi, %rdi
	vmovss	3145728(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	12(%rsi), %edi
	vmovss	%xmm0, 3145728(%rax)
	cmpl	$15, %edi
	ja	.L930
	movslq	%edi, %rdi
	vmovss	3407872(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	13(%rsi), %edi
	vmovss	%xmm0, 3407872(%rax)
	cmpl	$15, %edi
	ja	.L930
	movslq	%edi, %rdi
	vmovss	3670016(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 3670016(%rax)
	cmpl	$15, %r14d
	jne	.L930
	movq	-4194512(%rbp), %rdi
	vmovss	3932160(%rax), %xmm1
	addq	%rdx, %rdi
	vmovss	60(%rdi), %xmm0
	vmovss	%xmm1, 60(%rdi)
	vmovss	%xmm0, 3932160(%rax)
.L930:
	addl	$1, %r14d
	addq	$262144, %rdx
	addq	$262148, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
	addl	$1, %r13d
.L931:
	addl	$1, %esi
	cmpl	$16, %esi
	jne	.L1048
	addl	$1, -4194416(%rbp)
	movl	-4194416(%rbp), %eax
	addq	$1024, -4194480(%rbp)
	cmpl	$16, %eax
	jne	.L928
	movq	-4194360(%rbp), %r12
.L932:
	leaq	16384(%r12), %r13
	movq	%r12, %rbx
.L933:
	movq	%rbx, %rdi
	movl	%r15d, %esi
	addq	$1024, %rbx
	call	_ZN18WaveletsOnInterval3WI49transformILi16ELb1EEEvPfi
	cmpq	%r13, %rbx
	jne	.L933
	addq	$262144, %r12
	cmpq	-4194448(%rbp), %r12
	jne	.L932
	movq	-4194368(%rbp), %rax
	movq	-4194360(%rbp), %r12
	addq	$2097176, %rax
	movq	%rax, -4194416(%rbp)
.L940:
	leaq	8192(%r12), %rbx
	movq	%r12, %r13
.L935:
	movq	%r13, %rdi
	movl	%r15d, %esi
	addq	$1024, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r13, %rbx
	jne	.L935
	leaq	7168(%r12), %rax
	movq	%r12, %rcx
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %edi
	movl	$2, %esi
	movl	$7, %edx
.L937:
	cmpl	$14, %edx
	je	.L939
	vmovss	-7164(%rax), %xmm0
	vmovss	-6144(%rax), %xmm1
	vmovss	%xmm0, -6144(%rax)
	vmovss	%xmm1, -7164(%rax)
	cmpl	$7, %esi
	ja	.L939
	movslq	%esi, %r11
	vmovss	-5120(%rax), %xmm1
	leaq	(%rcx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -5120(%rax)
	cmpl	$7, %edi
	ja	.L939
	movslq	%edi, %r11
	vmovss	-4096(%rax), %xmm1
	leaq	(%rcx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -4096(%rax)
	cmpl	$7, %r8d
	ja	.L939
	movslq	%r8d, %r11
	vmovss	-3072(%rax), %xmm1
	leaq	(%rcx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -3072(%rax)
	cmpl	$7, %r9d
	ja	.L939
	movslq	%r9d, %r11
	vmovss	-2048(%rax), %xmm1
	leaq	(%rcx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -2048(%rax)
	cmpl	$7, %r10d
	ja	.L939
	movslq	%r10d, %r11
	vmovss	-1024(%rax), %xmm1
	leaq	(%rcx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -1024(%rax)
	cmpl	$7, %edx
	jne	.L939
	vmovss	28(%rcx), %xmm0
	vmovss	(%rax), %xmm1
	vmovss	%xmm1, 28(%rcx)
	vmovss	%xmm0, (%rax)
.L939:
	addl	$1, %edx
	addq	$1024, %rcx
	addq	$1028, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	cmpl	$15, %edx
	jne	.L937
	movq	%r12, %r13
.L938:
	movq	%r13, %rdi
	movl	%r15d, %esi
	addq	$1024, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r13, %rbx
	jne	.L938
	addq	$262144, %r12
	cmpq	-4194416(%rbp), %r12
	jne	.L940
	movq	-4194368(%rbp), %rax
	xorl	%r14d, %r14d
	leaq	28(%rax), %r13
.L944:
	movq	-4194360(%rbp), %rdx
	movslq	%r14d, %rdi
	movq	%r13, %rax
	movl	$6, %ebx
	movq	%rdi, %rcx
	movl	$5, %r11d
	salq	$10, %rdi
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$7, %esi
	salq	$8, %rcx
.L943:
	cmpl	$14, %esi
	je	.L942
	vmovss	(%rax), %xmm0
	vmovss	262140(%rax), %xmm1
	vmovss	%xmm0, 262140(%rax)
	vmovss	%xmm1, (%rax)
	cmpl	$7, %r8d
	ja	.L942
	movslq	%r8d, %r12
	vmovss	524284(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 524284(%rax)
	cmpl	$7, %r9d
	ja	.L942
	movslq	%r9d, %r12
	vmovss	786428(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 786428(%rax)
	cmpl	$7, %r10d
	ja	.L942
	movslq	%r10d, %r12
	vmovss	1048572(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1048572(%rax)
	cmpl	$7, %r11d
	ja	.L942
	movslq	%r11d, %r12
	vmovss	1310716(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1310716(%rax)
	cmpl	$7, %ebx
	ja	.L942
	movslq	%ebx, %r12
	vmovss	1572860(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1572860(%rax)
	cmpl	$7, %esi
	jne	.L942
	leaq	(%rdx,%rdi), %r12
	vmovss	1835004(%rax), %xmm1
	vmovss	28(%r12), %xmm0
	vmovss	%xmm1, 28(%r12)
	vmovss	%xmm0, 1835004(%rax)
.L942:
	addl	$1, %esi
	addq	$262144, %rdx
	addq	$262148, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	cmpl	$15, %esi
	jne	.L943
	addl	$1, %r14d
	addq	$1024, %r13
	cmpl	$8, %r14d
	jne	.L944
	movq	-4194616(%rbp), %rbx
.L945:
	leaq	8192(%rbx), %r13
	movq	%rbx, %r12
.L946:
	movq	%r12, %rdi
	movl	%r15d, %esi
	addq	$1024, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r13, %r12
	jne	.L946
	addq	$262144, %rbx
	cmpq	-4194416(%rbp), %rbx
	jne	.L945
	movl	$2097152, %edx
	xorl	%esi, %esi
	leaq	-4194352(%rbp), %rdi
	movl	$7, %r14d
	movl	$6, %r13d
	call	memset
	movq	-4194368(%rbp), %r15
	leaq	-4194352(%rbp), %rsi
	movq	-4194360(%rbp), %rcx
	vmovss	-4194620(%rbp), %xmm0
	leaq	69206048(%r15), %r12
	leaq	16(%r15), %rdi
	movq	%r12, %rdx
	call	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE9thresholdIfLi256EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA256_A256_Kf
	movl	$2097152, %edx
	leaq	-2097200(%rbp), %rdi
	movslq	%eax, %rbx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	leaq	67108896(%r15), %rdx
	xorl	%edi, %edi
	movl	$5, %r11d
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$1, %eax
	movq	$2, -4194360(%rbp)
	movl	$1, %r8d
.L948:
	movl	%r8d, %ecx
	movq	%rax, %rsi
	movq	%rax, %r15
	salq	%cl, %rsi
	movq	%r8, %rcx
	shrq	$6, %rcx
	testq	%rsi, -2097200(%rbp,%rcx,8)
	movl	-4194360(%rbp), %ecx
	setne	%sil
	salq	%cl, %r15
	movq	-4194360(%rbp), %rcx
	movzbl	%sil, %esi
	addl	%esi, %esi
	shrq	$6, %rcx
	testq	%r15, -2097200(%rbp,%rcx,8)
	movq	%rax, %r15
	setne	%cl
	movzbl	%cl, %ecx
	sall	$2, %ecx
	orl	%ecx, %esi
	movl	%edi, %ecx
	salq	%cl, %r15
	movq	%rdi, %rcx
	shrq	$6, %rcx
	testq	%r15, -2097200(%rbp,%rcx,8)
	movq	%rax, %r15
	setne	%cl
	movzbl	%cl, %ecx
	orl	%ecx, %esi
	movl	%r9d, %ecx
	salq	%cl, %r15
	movq	%r9, %rcx
	shrq	$6, %rcx
	testq	%r15, -2097200(%rbp,%rcx,8)
	movq	%rax, %r15
	setne	%cl
	movzbl	%cl, %ecx
	sall	$3, %ecx
	orl	%ecx, %esi
	movl	%r10d, %ecx
	salq	%cl, %r15
	movq	%r10, %rcx
	shrq	$6, %rcx
	testq	%r15, -2097200(%rbp,%rcx,8)
	movq	%rax, %r15
	setne	%cl
	movzbl	%cl, %ecx
	sall	$4, %ecx
	orl	%ecx, %esi
	movl	%r11d, %ecx
	salq	%cl, %r15
	movq	%r11, %rcx
	shrq	$6, %rcx
	testq	%r15, -2097200(%rbp,%rcx,8)
	movq	%rax, %r15
	setne	%cl
	movzbl	%cl, %ecx
	sall	$5, %ecx
	orl	%ecx, %esi
	movl	%r13d, %ecx
	salq	%cl, %r15
	movq	%r13, %rcx
	shrq	$6, %rcx
	testq	%r15, -2097200(%rbp,%rcx,8)
	movq	%rax, %r15
	setne	%cl
	movzbl	%cl, %ecx
	sall	$6, %ecx
	orl	%ecx, %esi
	movl	%r14d, %ecx
	salq	%cl, %r15
	movq	%r14, %rcx
	shrq	$6, %rcx
	testq	%r15, -2097200(%rbp,%rcx,8)
	setne	%cl
	addq	$8, %rdi
	addq	$8, %r8
	movzbl	%cl, %ecx
	addq	$8, %r9
	addq	$8, %r10
	sall	$7, %ecx
	addq	$8, %r11
	addq	$8, %r13
	orl	%ecx, %esi
	addq	$8, %r14
	addq	$1, %rdx
	addq	$8, -4194360(%rbp)
	movb	%sil, -1(%rdx)
	cmpq	$16777216, %rdi
	jne	.L948
	cmpb	$0, -4194624(%rbp)
	leaq	2097152(,%rbx,4), %rax
	je	.L1028
	xorl	%esi, %esi
	movl	$31, %r8d
	xorl	%edi, %edi
	testl	%ebx, %ebx
	jle	.L952
.L967:
	movl	(%r12,%rsi,4), %eax
	movl	%eax, %edx
	andl	$2147483647, %edx
	shrl	$23, %edx
	subl	$112, %edx
	cmpl	$31, %edx
	cmovg	%r8d, %edx
	movl	%edx, %ecx
	sall	$10, %ecx
	testl	%edx, %edx
	movl	%eax, %edx
	cmovle	%edi, %ecx
	andl	$-2147483648, %edx
	andl	$8388607, %eax
	shrl	$16, %edx
	orl	%ecx, %edx
	shrl	$13, %eax
	orl	%edx, %eax
	movw	%ax, (%r12,%rsi,2)
	addq	$1, %rsi
	cmpl	%esi, %ebx
	jg	.L967
.L952:
	leaq	2097152(%rbx,%rbx), %rax
.L1028:
	addq	$4194592, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L1035:
	.cfi_restore_state
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	0(%r13), %xmm0, %xmm0
	vmovsd	.LC20(%rip), %xmm11
	vmovsd	.LC21(%rip), %xmm12
	vcvtss2sd	8(%r13), %xmm1, %xmm1
	leaq	-4194352(%rbp), %rax
	vmovsd	.LC22(%rip), %xmm10
	vmulsd	%xmm11, %xmm0, %xmm0
	vmovapd	.LC24(%rip), %ymm6
	vmulsd	%xmm12, %xmm1, %xmm1
	vmovapd	.LC25(%rip), %ymm7
	leaq	-4194352(%rbp), %rdi
	leaq	4(%rax), %rdx
	leaq	228(%rdi), %rcx
	leaq	8(%r13), %rax
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%r13), %xmm1, %xmm1
	vmulsd	%xmm10, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
.L856:
	vmovups	32(%rax), %ymm8
	addq	$32, %rdx
	addq	$64, %rax
	vmovups	-64(%rax), %ymm1
	vshufps	$136, %ymm8, %ymm1, %ymm2
	vshufps	$221, %ymm8, %ymm1, %ymm1
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm5
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm5, %ymm5
	vmovups	-48(%rax), %ymm0
	vshufps	$136, -16(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm3
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vmovups	-56(%rax), %ymm0
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vshufps	$136, -24(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm3
	vshufps	$68, %ymm3, %ymm0, %ymm4
	vshufps	$238, %ymm3, %ymm0, %ymm3
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovups	-72(%rax), %ymm4
	vshufps	$136, -40(%rax), %ymm4, %ymm4
	vperm2f128	$3, %ymm4, %ymm4, %ymm0
	vshufps	$68, %ymm0, %ymm4, %ymm9
	vshufps	$238, %ymm0, %ymm4, %ymm0
	vperm2f128	$3, %ymm1, %ymm1, %ymm4
	vinsertf128	$1, %xmm0, %ymm9, %ymm0
	vshufps	$68, %ymm4, %ymm1, %ymm8
	vshufps	$238, %ymm4, %ymm1, %ymm4
	vcvtps2pd	%xmm0, %ymm1
	vinsertf128	$1, %xmm4, %ymm8, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm5, %ymm8
	vmulpd	.LC23(%rip), %ymm1, %ymm1
	vmulpd	%ymm6, %ymm8, %ymm8
	vcvtps2pd	%xmm0, %ymm0
	vaddpd	%ymm8, %ymm1, %ymm1
	vmulpd	.LC23(%rip), %ymm0, %ymm0
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vmulpd	%ymm6, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm8
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm6, %ymm8, %ymm8
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm8, %ymm1, %ymm1
	vmulpd	%ymm6, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm8
	vmulpd	%ymm7, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm0, %ymm0
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm7, %ymm2, %ymm2
	vsubpd	%ymm8, %ymm1, %ymm1
	vsubpd	%ymm2, %ymm0, %ymm0
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm4, %ymm0
	vmovups	%ymm0, -32(%rdx)
	cmpq	%rcx, %rdx
	jne	.L856
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	456(%r13), %xmm5, %xmm5
	vcvtss2sd	448(%r13), %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	464(%r13), %xmm7, %xmm7
	vxorpd	%xmm4, %xmm4, %xmm4
	vmovsd	.LC27(%rip), %xmm0
	vcvtss2sd	472(%r13), %xmm4, %xmm4
	vmovss	468(%r13), %xmm8
	vmulsd	%xmm6, %xmm5, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm7, %xmm3
	vmulsd	%xmm0, %xmm5, %xmm5
	vmulsd	%xmm0, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm10, %xmm4, %xmm2
	vaddsd	%xmm5, %xmm3, %xmm5
	vaddsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm2, %xmm1, %xmm1
	vmovss	460(%r13), %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	480(%r13), %xmm2, %xmm2
	vmulsd	%xmm10, %xmm2, %xmm3
	vmovss	%xmm1, -4194124(%rbp)
	vmulsd	%xmm6, %xmm4, %xmm1
	vmulsd	%xmm0, %xmm4, %xmm4
	vaddsd	%xmm1, %xmm5, %xmm5
	vaddsd	%xmm7, %xmm1, %xmm1
	vsubsd	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm8, %xmm5
	vmulsd	%xmm6, %xmm2, %xmm8
	vmovss	%xmm5, -4194120(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	488(%r13), %xmm5, %xmm5
	vmulsd	%xmm10, %xmm5, %xmm7
	vmulsd	%xmm6, %xmm5, %xmm9
	vaddsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm7, %xmm1, %xmm1
	vmovss	476(%r13), %xmm7
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vaddsd	%xmm4, %xmm8, %xmm7
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	496(%r13), %xmm4, %xmm4
	vmovss	%xmm1, -4194116(%rbp)
	vmulsd	%xmm10, %xmm4, %xmm1
	vaddsd	%xmm9, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm7, %xmm7
	vmovss	484(%r13), %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vmovss	%xmm1, -4194112(%rbp)
	vmulsd	%xmm0, %xmm2, %xmm1
	vmulsd	%xmm6, %xmm4, %xmm0
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	504(%r13), %xmm6, %xmm6
	vmulsd	%xmm10, %xmm6, %xmm10
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	492(%r13), %xmm1
	vsubsd	%xmm10, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm11, %xmm5, %xmm1
	vmovss	%xmm0, -4194108(%rbp)
	vmulsd	%xmm12, %xmm4, %xmm0
	vsubsd	%xmm1, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm3, %xmm1
	vmulsd	%xmm11, %xmm6, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	500(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovsd	.LC30(%rip), %xmm1
	vmulsd	%xmm1, %xmm6, %xmm6
	vmovss	%xmm0, -4194104(%rbp)
	vmulsd	.LC29(%rip), %xmm5, %xmm0
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm1, %xmm4, %xmm0
	vmovss	508(%r13), %xmm1
	vsubsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194100(%rbp)
	vmovups	0(%r13), %ymm0
	vshufps	$136, 32(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm7
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	64(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm7, %ymm7
	vshufps	$136, 96(%r13), %ymm0, %ymm0
	vmovaps	%ymm7, -2097200(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm6
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm6, %ymm6
	vmovaps	%ymm6, -2097168(%rbp)
	vmovups	128(%r13), %ymm0
	vshufps	$136, 160(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm5
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	192(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm5, %ymm5
	vshufps	$136, 224(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm4
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	256(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	vshufps	$136, 288(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm3
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	320(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 352(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	384(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm2
	vshufps	$136, 416(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm8
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovss	448(%r13), %xmm0
	vinsertf128	$1, %xmm1, %ymm8, %ymm1
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	%ymm4, -2097104(%rbp)
	vmovss	%xmm0, -2096976(%rbp)
	vmovss	456(%r13), %xmm0
	vmovaps	%ymm3, -2097072(%rbp)
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	464(%r13), %xmm0
	vmovaps	%ymm2, -2097040(%rbp)
	vmovss	%xmm0, -2096968(%rbp)
	vmovss	472(%r13), %xmm0
	vmovaps	%ymm1, -2097008(%rbp)
	vmovss	%xmm0, -2096964(%rbp)
	vmovss	480(%r13), %xmm0
	vmovss	%xmm0, -2096960(%rbp)
	vmovss	488(%r13), %xmm0
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	496(%r13), %xmm0
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	504(%r13), %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	testb	%r14b, %r14b
	je	.L859
	vmovapd	.LC31(%rip), %ymm0
	vcvtps2pd	%xmm7, %ymm10
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmovaps	-4194352(%rbp), %ymm8
	vcvtps2pd	%xmm8, %ymm9
	vextractf128	$0x1, %ymm8, %xmm8
	vmulpd	%ymm0, %ymm9, %ymm9
	vcvtps2pd	%xmm8, %ymm8
	vaddpd	%ymm9, %ymm10, %ymm9
	vmulpd	%ymm0, %ymm8, %ymm8
	vaddpd	%ymm8, %ymm7, %ymm7
	vcvtpd2psy	%ymm9, %xmm9
	vcvtpd2psy	%ymm7, %xmm7
	vinsertf128	$0x1, %xmm7, %ymm9, %ymm7
	vcvtps2pd	%xmm6, %ymm9
	vmovaps	%ymm7, -2097200(%rbp)
	vmovaps	-4194320(%rbp), %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vcvtps2pd	%xmm7, %ymm8
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm8, %ymm9, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vaddpd	%ymm7, %ymm6, %ymm6
	vcvtps2pd	%xmm5, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vmovaps	%ymm6, -2097168(%rbp)
	vmovaps	-4194288(%rbp), %ymm6
	vcvtps2pd	%xmm6, %ymm8
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm8, %ymm7, %ymm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vaddpd	%ymm6, %ymm5, %ymm5
	vcvtpd2psy	%ymm7, %xmm6
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm6, %ymm5
	vcvtps2pd	%xmm4, %ymm6
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	-4194256(%rbp), %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm5, %ymm7
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm0, %ymm7, %ymm7
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm7, %ymm6, %ymm6
	vmulpd	%ymm0, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vcvtpd2psy	%ymm6, %xmm5
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm5, %ymm4
	vcvtps2pd	%xmm3, %ymm5
	vmovaps	%ymm4, -2097104(%rbp)
	vmovaps	-4194224(%rbp), %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm4, %ymm6
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm4, %ymm4
	vaddpd	%ymm6, %ymm5, %ymm5
	vmulpd	%ymm0, %ymm4, %ymm4
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtpd2psy	%ymm5, %xmm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm4, %ymm3
	vcvtps2pd	%xmm2, %ymm4
	vmovaps	%ymm3, -2097072(%rbp)
	vmovaps	-4194192(%rbp), %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm5, %ymm4, %ymm4
	vmulpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm4, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmovaps	%ymm2, -2097040(%rbp)
	vmovaps	-4194160(%rbp), %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm3, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	vmovaps	-4194128(%rbp), %ymm2
	vmovaps	%ymm1, -2097008(%rbp)
	vmovaps	-2096976(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2096976(%rbp)
.L859:
	movl	$256, %edx
	movq	%r13, %rdi
	vmovapd	%ymm15, -4194448(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovaps	%ymm14, -4194416(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	vmovaps	-4194416(%rbp), %ymm14
	vmovapd	-4194448(%rbp), %ymm15
	jmp	.L858
.L1037:
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	0(%r13), %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	movabsq	$4599301119452119040, %rdi
	vcvtss2sd	8(%r13), %xmm1, %xmm1
	vmovq	%rdi, %xmm5
	movabsq	$4606619468846596096, %rsi
	vmovsd	.LC22(%rip), %xmm11
	vmovapd	.LC24(%rip), %ymm4
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovq	%rsi, %xmm5
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovq	%rdi, %xmm5
	leaq	-4194352(%rbp), %rax
	leaq	-4194352(%rbp), %rcx
	leaq	4(%rax), %rdx
	addq	$228, %rcx
	leaq	8(%r13), %rax
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%r13), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovapd	.LC25(%rip), %ymm5
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
.L866:
	vmovups	32(%rax), %ymm3
	addq	$32, %rdx
	addq	$64, %rax
	vmovups	-64(%rax), %ymm8
	vshufps	$136, %ymm3, %ymm8, %ymm2
	vshufps	$221, %ymm3, %ymm8, %ymm8
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm1
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vmovups	-48(%rax), %ymm2
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	vshufps	$136, -16(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm6
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vmovups	-56(%rax), %ymm2
	vinsertf128	$1, %xmm0, %ymm6, %ymm6
	vshufps	$136, -24(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm0
	vshufps	$68, %ymm0, %ymm2, %ymm7
	vshufps	$238, %ymm0, %ymm2, %ymm0
	vmovups	-72(%rax), %ymm2
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vshufps	$136, -40(%rax), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm7
	vshufps	$68, %ymm7, %ymm2, %ymm9
	vshufps	$238, %ymm7, %ymm2, %ymm7
	vperm2f128	$3, %ymm8, %ymm8, %ymm2
	vinsertf128	$1, %xmm7, %ymm9, %ymm7
	vshufps	$68, %ymm2, %ymm8, %ymm3
	vshufps	$238, %ymm2, %ymm8, %ymm2
	vinsertf128	$1, %xmm2, %ymm3, %ymm3
	vcvtps2pd	%xmm7, %ymm2
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmulpd	.LC23(%rip), %ymm2, %ymm8
	vmulpd	.LC23(%rip), %ymm7, %ymm7
	vcvtps2pd	%xmm1, %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm4, %ymm2, %ymm2
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm2, %ymm8, %ymm8
	vmulpd	%ymm4, %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vaddpd	%ymm1, %ymm7, %ymm7
	vmulpd	%ymm4, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	%ymm4, %ymm1, %ymm1
	vextractf128	$0x1, %ymm6, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm5, %ymm0, %ymm0
	vaddpd	%ymm2, %ymm8, %ymm8
	vcvtps2pd	%xmm6, %ymm2
	vmulpd	%ymm5, %ymm2, %ymm2
	vaddpd	%ymm1, %ymm7, %ymm1
	vsubpd	%ymm2, %ymm8, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmovups	%ymm0, -32(%rdx)
	cmpq	%rcx, %rdx
	jne	.L866
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	456(%r13), %xmm5, %xmm5
	vcvtss2sd	448(%r13), %xmm2, %xmm2
	vmovsd	.LC26(%rip), %xmm7
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	464(%r13), %xmm3, %xmm3
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC27(%rip), %xmm0
	vcvtss2sd	472(%r13), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm5, %xmm6
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm3, %xmm4
	vmulsd	%xmm0, %xmm5, %xmm5
	vmulsd	%xmm7, %xmm1, %xmm8
	vmulsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm2, %xmm2
	vmulsd	%xmm11, %xmm1, %xmm6
	vmulsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm2, %xmm2
	vmovss	460(%r13), %xmm6
	vaddsd	%xmm8, %xmm4, %xmm4
	vmovss	476(%r13), %xmm8
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm6, %xmm2
	vmovss	468(%r13), %xmm6
	vmovss	%xmm2, -4194124(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	480(%r13), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm5
	vsubsd	%xmm5, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm6, %xmm4
	vmulsd	%xmm7, %xmm2, %xmm6
	vmovss	%xmm4, -4194120(%rbp)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	488(%r13), %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm9
	vaddsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm1, %xmm6
	vsubsd	%xmm9, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm8, %xmm3
	vmulsd	%xmm7, %xmm4, %xmm8
	vmovss	%xmm3, -4194116(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	496(%r13), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm1
	vaddsd	%xmm8, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm6, %xmm6
	vmovss	484(%r13), %xmm1
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm1, %xmm1
	vmovss	%xmm1, -4194112(%rbp)
	vmulsd	%xmm0, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm3, %xmm0
	vmovq	%rdi, %xmm7
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	504(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm6
	vsubsd	%xmm6, %xmm0, %xmm0
	vmovss	492(%r13), %xmm6
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmulsd	%xmm7, %xmm4, %xmm6
	vmovss	%xmm0, -4194108(%rbp)
	vsubsd	%xmm6, %xmm5, %xmm6
	vmovq	%rsi, %xmm5
	vmulsd	%xmm5, %xmm3, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm5
	vmulsd	%xmm7, %xmm1, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmovss	500(%r13), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm0, -4194104(%rbp)
	vmulsd	.LC29(%rip), %xmm4, %xmm0
	vmovsd	.LC30(%rip), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm3, %xmm0
	vsubsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	508(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194100(%rbp)
	vmovups	0(%r13), %ymm0
	vshufps	$136, 32(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm6
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	64(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm6, %ymm6
	vshufps	$136, 96(%r13), %ymm0, %ymm0
	vmovaps	%ymm6, -2097200(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm9
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm9, %ymm9
	vmovaps	%ymm9, -2097168(%rbp)
	vmovups	128(%r13), %ymm0
	vshufps	$136, 160(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm5
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	192(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm5, %ymm5
	vshufps	$136, 224(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm4
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	256(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	vshufps	$136, 288(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm3
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	320(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 352(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	384(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm2
	vshufps	$136, 416(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm7
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovss	448(%r13), %xmm0
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	%ymm4, -2097104(%rbp)
	vmovss	%xmm0, -2096976(%rbp)
	vmovss	456(%r13), %xmm0
	vmovaps	%ymm3, -2097072(%rbp)
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	464(%r13), %xmm0
	vmovaps	%ymm2, -2097040(%rbp)
	vmovss	%xmm0, -2096968(%rbp)
	vmovss	472(%r13), %xmm0
	vmovaps	%ymm1, -2097008(%rbp)
	vmovss	%xmm0, -2096964(%rbp)
	vmovss	480(%r13), %xmm0
	vmovss	%xmm0, -2096960(%rbp)
	vmovss	488(%r13), %xmm0
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	496(%r13), %xmm0
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	504(%r13), %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	testb	%r14b, %r14b
	je	.L869
	vmovapd	.LC31(%rip), %ymm0
	vcvtps2pd	%xmm6, %ymm11
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vmovaps	-4194352(%rbp), %ymm7
	vcvtps2pd	%xmm7, %ymm8
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm8, %ymm11, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vaddpd	%ymm7, %ymm6, %ymm6
	vmovaps	-4194320(%rbp), %ymm7
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vcvtps2pd	%xmm9, %ymm8
	vmovaps	%ymm6, -2097200(%rbp)
	vcvtps2pd	%xmm7, %ymm6
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm6, %ymm8, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vextractf128	$0x1, %ymm9, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm7, %ymm6, %ymm6
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vcvtps2pd	%xmm5, %ymm8
	vmovaps	%ymm6, -2097168(%rbp)
	vmovaps	-4194288(%rbp), %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vcvtps2pd	%xmm6, %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm0, %ymm7, %ymm7
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm7, %ymm8, %ymm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vaddpd	%ymm6, %ymm5, %ymm5
	vcvtpd2psy	%ymm7, %xmm7
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm7, %ymm5
	vcvtps2pd	%xmm4, %ymm7
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	-4194256(%rbp), %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm5, %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm6, %ymm7, %ymm6
	vmulpd	%ymm0, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vcvtpd2psy	%ymm6, %xmm6
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm6, %ymm4
	vcvtps2pd	%xmm3, %ymm6
	vmovaps	%ymm4, -2097104(%rbp)
	vmovaps	-4194224(%rbp), %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm4, %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm4, %ymm4
	vaddpd	%ymm5, %ymm6, %ymm5
	vmulpd	%ymm0, %ymm4, %ymm4
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vcvtpd2psy	%ymm5, %xmm5
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm5, %ymm3
	vmovaps	%ymm3, -2097072(%rbp)
	vmovaps	-4194192(%rbp), %ymm3
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm5, %ymm4, %ymm4
	vmulpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm4, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmovaps	%ymm2, -2097040(%rbp)
	vmovaps	-4194160(%rbp), %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm3, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	vmovaps	-4194128(%rbp), %ymm2
	vmovaps	%ymm1, -2097008(%rbp)
	vmovaps	-2096976(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2096976(%rbp)
.L869:
	movl	$256, %edx
	movq	%r13, %rdi
	vmovapd	%ymm15, -4194544(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovaps	%ymm14, -4194512(%rbp)
	vmovapd	%ymm10, -4194480(%rbp)
	vmovapd	%ymm13, -4194448(%rbp)
	vmovaps	%ymm12, -4194416(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	vmovaps	-4194416(%rbp), %ymm12
	vmovapd	-4194448(%rbp), %ymm13
	vmovapd	-4194480(%rbp), %ymm10
	vmovaps	-4194512(%rbp), %ymm14
	vmovapd	-4194544(%rbp), %ymm15
	jmp	.L868
.L1039:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	0(%r13), %xmm0, %xmm0
	vcvtss2sd	8(%r13), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm10
	leaq	-4194352(%rbp), %rdi
	xorl	%eax, %eax
	movabsq	$4606619468846596096, %r8
	leaq	8(%r13), %rsi
	vmovq	%r8, %xmm5
	addq	$4, %rdi
	vmovsd	.LC22(%rip), %xmm11
	vmovapd	.LC24(%rip), %ymm4
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovapd	.LC25(%rip), %ymm5
	vmulsd	%xmm10, %xmm0, %xmm0
	leaq	24(%r13), %rcx
	leaq	16(%r13), %rdx
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	16(%r13), %xmm1, %xmm1
	vmulsd	%xmm10, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	24(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194352(%rbp)
.L879:
	vmovups	32(%rsi,%rax,2), %ymm8
	vmovups	(%rsi,%rax,2), %ymm3
	vshufps	$136, %ymm8, %ymm3, %ymm1
	vshufps	$221, %ymm8, %ymm3, %ymm3
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	(%rcx,%rax,2), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vshufps	$136, 32(%rcx,%rax,2), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm7
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vinsertf128	$1, %xmm2, %ymm7, %ymm7
	vmovups	(%rdx,%rax,2), %ymm2
	vshufps	$136, 32(%rdx,%rax,2), %ymm2, %ymm2
	vperm2f128	$3, %ymm2, %ymm2, %ymm1
	vshufps	$68, %ymm1, %ymm2, %ymm6
	vshufps	$238, %ymm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm1, %ymm6, %ymm1
	vmovups	0(%r13,%rax,2), %ymm6
	vshufps	$136, 32(%r13,%rax,2), %ymm6, %ymm6
	vperm2f128	$3, %ymm6, %ymm6, %ymm2
	vshufps	$68, %ymm2, %ymm6, %ymm9
	vshufps	$238, %ymm2, %ymm6, %ymm2
	vperm2f128	$3, %ymm3, %ymm3, %ymm6
	vinsertf128	$1, %xmm2, %ymm9, %ymm2
	vshufps	$68, %ymm6, %ymm3, %ymm8
	vshufps	$238, %ymm6, %ymm3, %ymm6
	vcvtps2pd	%xmm0, %ymm3
	vinsertf128	$1, %xmm6, %ymm8, %ymm6
	vmulpd	%ymm4, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm8
	vmulpd	%ymm12, %ymm8, %ymm8
	vaddpd	%ymm3, %ymm8, %ymm8
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	%ymm4, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm8, %ymm3
	vcvtps2pd	%xmm7, %ymm8
	vmulpd	%ymm5, %ymm8, %ymm8
	vsubpd	%ymm8, %ymm3, %ymm3
	vextractf128	$0x1, %ymm2, %xmm8
	vextractf128	$0x1, %ymm0, %xmm2
	vcvtps2pd	%xmm8, %ymm8
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm12, %ymm8, %ymm8
	vmulpd	%ymm4, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm8, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm1, %ymm0, %ymm1
	vextractf128	$0x1, %ymm7, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm5, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm3, %ymm0
	vsubps	%ymm0, %ymm6, %ymm0
	vmovups	%ymm0, (%rdi,%rax)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L879
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	456(%r13), %xmm3, %xmm3
	vcvtss2sd	448(%r13), %xmm2, %xmm2
	vmovsd	.LC26(%rip), %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	464(%r13), %xmm1, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vmovsd	.LC27(%rip), %xmm0
	vcvtss2sd	472(%r13), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm3, %xmm5
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm1, %xmm4
	vmulsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm7, %xmm8
	vmulsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm5, %xmm2, %xmm2
	vmulsd	%xmm11, %xmm7, %xmm5
	vmulsd	%xmm0, %xmm7, %xmm7
	vaddsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm2, %xmm2
	vmovss	460(%r13), %xmm5
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm5, %xmm2
	vmovss	468(%r13), %xmm5
	vmovss	%xmm2, -4194124(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	480(%r13), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm4
	vsubsd	%xmm4, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vmulsd	%xmm6, %xmm2, %xmm5
	vmovss	%xmm3, -4194120(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	488(%r13), %xmm3, %xmm3
	vmulsd	%xmm11, %xmm3, %xmm8
	vaddsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm5, %xmm7
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	496(%r13), %xmm5, %xmm5
	vsubsd	%xmm8, %xmm1, %xmm1
	vmovss	476(%r13), %xmm8
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm8, %xmm1
	vmulsd	%xmm6, %xmm3, %xmm8
	vmovss	%xmm1, -4194116(%rbp)
	vmulsd	%xmm11, %xmm5, %xmm1
	vaddsd	%xmm8, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm7, %xmm7
	vmovss	484(%r13), %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm7, %xmm1, %xmm1
	vmovq	%r8, %xmm7
	vmovss	%xmm1, -4194112(%rbp)
	vmulsd	%xmm0, %xmm2, %xmm1
	vmulsd	%xmm6, %xmm5, %xmm0
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	504(%r13), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm6
	vsubsd	%xmm6, %xmm0, %xmm0
	vmovss	492(%r13), %xmm6
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm6, %xmm0
	vmovss	%xmm0, -4194108(%rbp)
	vmulsd	%xmm10, %xmm3, %xmm0
	vmulsd	%xmm10, %xmm1, %xmm10
	vsubsd	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm7, %xmm5, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm0
	vmovss	500(%r13), %xmm4
	vaddsd	%xmm10, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm0, -4194104(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm0
	vmovsd	.LC30(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm3, %xmm5, %xmm0
	vsubsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	508(%r13), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, -4194100(%rbp)
	vmovups	0(%r13), %ymm0
	vshufps	$136, 32(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm7
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	64(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm7, %ymm7
	vshufps	$136, 96(%r13), %ymm0, %ymm0
	vmovaps	%ymm7, -2097200(%rbp)
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm6
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vinsertf128	$1, %xmm1, %ymm6, %ymm6
	vmovaps	%ymm6, -2097168(%rbp)
	vmovups	128(%r13), %ymm0
	vshufps	$136, 160(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm5
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	192(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm5, %ymm5
	vshufps	$136, 224(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm4
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	256(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	vshufps	$136, 288(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm3
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	320(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vshufps	$136, 352(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	384(%r13), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm2
	vshufps	$136, 416(%r13), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm8
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovss	448(%r13), %xmm0
	vinsertf128	$1, %xmm1, %ymm8, %ymm1
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	%ymm4, -2097104(%rbp)
	vmovss	%xmm0, -2096976(%rbp)
	vmovss	456(%r13), %xmm0
	vmovaps	%ymm3, -2097072(%rbp)
	vmovss	%xmm0, -2096972(%rbp)
	vmovss	464(%r13), %xmm0
	vmovaps	%ymm2, -2097040(%rbp)
	vmovss	%xmm0, -2096968(%rbp)
	vmovss	472(%r13), %xmm0
	vmovaps	%ymm1, -2097008(%rbp)
	vmovss	%xmm0, -2096964(%rbp)
	vmovss	480(%r13), %xmm0
	vmovss	%xmm0, -2096960(%rbp)
	vmovss	488(%r13), %xmm0
	vmovss	%xmm0, -2096956(%rbp)
	vmovss	496(%r13), %xmm0
	vmovss	%xmm0, -2096952(%rbp)
	vmovss	504(%r13), %xmm0
	vmovss	%xmm0, -2096948(%rbp)
	testb	%r14b, %r14b
	je	.L882
	vmovapd	.LC31(%rip), %ymm0
	vcvtps2pd	%xmm7, %ymm10
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmovaps	-4194352(%rbp), %ymm8
	vcvtps2pd	%xmm8, %ymm9
	vextractf128	$0x1, %ymm8, %xmm8
	vmulpd	%ymm0, %ymm9, %ymm9
	vcvtps2pd	%xmm8, %ymm8
	vaddpd	%ymm9, %ymm10, %ymm9
	vmulpd	%ymm0, %ymm8, %ymm8
	vaddpd	%ymm8, %ymm7, %ymm7
	vcvtpd2psy	%ymm9, %xmm9
	vcvtpd2psy	%ymm7, %xmm7
	vinsertf128	$0x1, %xmm7, %ymm9, %ymm7
	vcvtps2pd	%xmm6, %ymm9
	vmovaps	%ymm7, -2097200(%rbp)
	vmovaps	-4194320(%rbp), %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm6, %ymm6
	vcvtps2pd	%xmm7, %ymm8
	vextractf128	$0x1, %ymm7, %xmm7
	vmulpd	%ymm0, %ymm8, %ymm8
	vcvtps2pd	%xmm7, %ymm7
	vaddpd	%ymm8, %ymm9, %ymm8
	vmulpd	%ymm0, %ymm7, %ymm7
	vaddpd	%ymm7, %ymm6, %ymm6
	vcvtpd2psy	%ymm8, %xmm8
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm8, %ymm6
	vcvtps2pd	%xmm5, %ymm8
	vmovaps	%ymm6, -2097168(%rbp)
	vmovaps	-4194288(%rbp), %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vcvtps2pd	%xmm6, %ymm7
	vextractf128	$0x1, %ymm6, %xmm6
	vmulpd	%ymm0, %ymm7, %ymm7
	vcvtps2pd	%xmm6, %ymm6
	vaddpd	%ymm7, %ymm8, %ymm7
	vmulpd	%ymm0, %ymm6, %ymm6
	vaddpd	%ymm6, %ymm5, %ymm5
	vcvtpd2psy	%ymm7, %xmm7
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm7, %ymm5
	vcvtps2pd	%xmm4, %ymm7
	vmovaps	%ymm5, -2097136(%rbp)
	vmovaps	-4194256(%rbp), %ymm5
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm5, %ymm6
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm6, %ymm7, %ymm6
	vmulpd	%ymm0, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vcvtpd2psy	%ymm6, %xmm6
	vcvtpd2psy	%ymm4, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm6, %ymm4
	vmovaps	%ymm4, -2097104(%rbp)
	vmovaps	-4194224(%rbp), %ymm4
	vcvtps2pd	%xmm4, %ymm6
	vextractf128	$0x1, %ymm4, %xmm4
	vmulpd	%ymm0, %ymm6, %ymm6
	vcvtps2pd	%xmm4, %ymm4
	vaddpd	%ymm6, %ymm5, %ymm5
	vmulpd	%ymm0, %ymm4, %ymm4
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtpd2psy	%ymm5, %xmm4
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm3, %ymm4, %ymm3
	vcvtps2pd	%xmm2, %ymm4
	vmovaps	%ymm3, -2097072(%rbp)
	vmovaps	-4194192(%rbp), %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm3, %ymm5
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm0, %ymm5, %ymm5
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm5, %ymm4, %ymm4
	vmulpd	%ymm0, %ymm3, %ymm3
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtpd2psy	%ymm4, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmovaps	%ymm2, -2097040(%rbp)
	vmovaps	-4194160(%rbp), %ymm2
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm4, %ymm3, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm2
	vaddpd	%ymm2, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm2
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	vmovaps	-4194128(%rbp), %ymm2
	vmovaps	%ymm1, -2097008(%rbp)
	vmovaps	-2096976(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm4
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm0, %ymm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm0, %ymm2, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vaddpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -2096976(%rbp)
.L882:
	movl	$256, %edx
	movq	%r13, %rdi
	vmovapd	%ymm12, -4194512(%rbp)
	leaq	-2097200(%rbp), %rsi
	vmovapd	%ymm15, -4194480(%rbp)
	vmovapd	%ymm13, -4194448(%rbp)
	vmovaps	%ymm14, -4194416(%rbp)
	call	memcpy
	leaq	256(%r13), %rdi
	movl	$256, %edx
	leaq	-4194352(%rbp), %rsi
	call	memcpy
	vmovaps	-4194416(%rbp), %ymm14
	vmovapd	-4194448(%rbp), %ymm13
	vmovapd	-4194480(%rbp), %ymm15
	vmovapd	-4194512(%rbp), %ymm12
	jmp	.L881
	.cfi_endproc
.LFE1393:
	.size	_ZN24WaveletCompressorGenericILi256EfE8compressEfbi, .-_ZN24WaveletCompressorGenericILi256EfE8compressEfbi
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbi,comdat
.LCOLDE62:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE8compressEfbi,comdat
.LHOTE62:
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,comdat
	.align 2
.LCOLDB63:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,comdat
.LHOTB63:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi
	.type	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi, @function
_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi:
.LFB1400:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movzbl	%sil, %esi
	movq	%rdi, %rbx
	subq	$112, %rsp
	.cfi_def_cfa_offset 128
	call	_ZN24WaveletCompressorGenericILi256EfE8compressEfbi
	movq	%rsp, %rdi
	movl	$14, %ecx
	movl	$-1, %esi
	movq	%rax, %rdx
	xorl	%eax, %eax
	rep; stosq
	leaq	67108896(%rbx), %rax
	movl	%edx, 8(%rsp)
	addq	$136314920, %rbx
	movl	$.LC59, %edx
	movq	%rsp, %rdi
	movq	%rax, (%rsp)
	movl	$69206016, 32(%rsp)
	movq	%rbx, 24(%rsp)
	movb	$112, %cl
	call	deflateInit_
	testl	%eax, %eax
	je	.L1050
.L1051:
	movl	$.LC60, %edi
	call	puts
	call	abort
	.p2align 4,,10
	.p2align 3
.L1050:
	movl	$4, %esi
	movq	%rsp, %rdi
	call	deflate
	cmpl	$1, %eax
	jne	.L1051
	movq	40(%rsp), %rbx
	movq	%rsp, %rdi
	call	deflateEnd
	addq	$112, %rsp
	.cfi_def_cfa_offset 16
	movslq	%ebx, %rax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1400:
	.size	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi, .-_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,comdat
.LCOLDE63:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi,comdat
.LHOTE63:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,comdat
	.align 2
.LCOLDB64:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,comdat
.LHOTB64:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi
	.type	_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi, @function
_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi:
.LFB1382:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movq	%rdi, %rax
	pushq	-8(%r10)
	pushq	%rbp
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	.cfi_escape 0x10,0xf,0x2,0x76,0x78
	.cfi_escape 0x10,0xe,0x2,0x76,0x70
	leal	-1(%rcx), %r14d
	pushq	%r13
	.cfi_escape 0x10,0xd,0x2,0x76,0x68
	leaq	84(%rax), %r13
	pushq	%r12
	movq	%r13, %r15
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x58,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x60
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x50
	leaq	20(%rax), %rbx
	subq	$8288, %rsp
	cmpl	$2, %ecx
	movq	%rdi, -8264(%rbp)
	vmovapd	.LC38(%rip), %ymm14
	sete	-8256(%rbp)
	movzbl	-8256(%rbp), %edi
	movq	%rbx, %r13
	addq	$131092, %rax
	vmovapd	.LC39(%rip), %ymm15
	vmovss	%xmm0, -8324(%rbp)
	movl	%esi, -8284(%rbp)
	movl	%edx, -8328(%rbp)
	movl	%edi, %r12d
	movl	%ecx, -8288(%rbp)
	movq	%rbx, -8248(%rbp)
	movq	%rbx, -8320(%rbp)
	movq	%rax, -8272(%rbp)
.L1064:
	movq	%r15, %rdx
	movq	%r13, %rax
	movl	$32, %esi
	jmp	.L1057
	.p2align 4,,10
	.p2align 3
.L1180:
	vmovss	24(%rax), %xmm1
	subq	$-128, %rdx
	subq	$-128, %rax
	vaddss	-100(%rax), %xmm1, %xmm1
	vmovss	.LC37(%rip), %xmm0
	vmovss	-128(%rax), %xmm10
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	-124(%rax), %xmm8
	vmovss	-120(%rax), %xmm9
	vaddss	%xmm8, %xmm10, %xmm6
	vmovss	-112(%rax), %xmm7
	vaddss	-116(%rax), %xmm9, %xmm9
	vmovss	%xmm1, -8228(%rbp)
	vmovss	-96(%rax), %xmm1
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	-92(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm6, %xmm6
	vaddss	-108(%rax), %xmm7, %xmm7
	vmulss	%xmm0, %xmm9, %xmm9
	vmovss	-48(%rax), %xmm4
	vmulss	%xmm0, %xmm1, %xmm1
	vaddss	-44(%rax), %xmm4, %xmm4
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm6, -8240(%rbp)
	vcvtss2sd	%xmm6, %xmm6, %xmm6
	vmovss	%xmm9, -8236(%rbp)
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vmulss	%xmm0, %xmm4, %xmm5
	vmovss	%xmm1, -8224(%rbp)
	vmovss	-88(%rax), %xmm1
	vaddss	-84(%rax), %xmm1, %xmm1
	vmovss	%xmm7, -8232(%rbp)
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmovd	%xmm5, %r11d
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8220(%rbp)
	vmovss	-80(%rax), %xmm1
	vaddss	-76(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8216(%rbp)
	vmovss	-72(%rax), %xmm1
	vaddss	-68(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8212(%rbp)
	vmovss	-128(%rdx), %xmm1
	vaddss	-60(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8208(%rbp)
	vmovss	-56(%rax), %xmm1
	vaddss	-52(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8204(%rbp)
	vmovss	%xmm5, -8200(%rbp)
	vmovss	-40(%rax), %xmm3
	vaddss	-36(%rax), %xmm3, %xmm3
	vmovss	-32(%rax), %xmm2
	vaddss	-28(%rax), %xmm2, %xmm2
	vmulss	%xmm0, %xmm3, %xmm5
	vmovss	-24(%rax), %xmm3
	vaddss	-20(%rax), %xmm3, %xmm3
	vmovss	-16(%rax), %xmm1
	vaddss	-12(%rax), %xmm1, %xmm1
	vmovd	%xmm5, %r10d
	vmovups	-88(%rax), %ymm12
	vmovss	%xmm5, -8196(%rbp)
	vmulss	%xmm0, %xmm2, %xmm5
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vmulsd	.LC35(%rip), %xmm6, %xmm10
	vmulsd	.LC36(%rip), %xmm6, %xmm6
	vmovd	%xmm5, %r8d
	vmovss	%xmm5, -8192(%rbp)
	vmulss	%xmm0, %xmm3, %xmm5
	vaddsd	%xmm9, %xmm10, %xmm10
	vsubsd	%xmm9, %xmm6, %xmm6
	vmovaps	-8240(%rbp), %ymm9
	vmovd	%xmm5, %edi
	vmovss	%xmm5, -8188(%rbp)
	vmulss	%xmm0, %xmm1, %xmm5
	vcvtps2pd	%xmm9, %ymm3
	vextractf128	$0x1, %ymm9, %xmm9
	vmovd	%xmm5, %ecx
	vmovss	%xmm5, -8184(%rbp)
	vmovss	-8(%rax), %xmm5
	vaddss	-4(%rax), %xmm5, %xmm1
	vmulss	%xmm0, %xmm1, %xmm5
	vmovsd	.LC34(%rip), %xmm1
	vmulsd	%xmm1, %xmm7, %xmm7
	vmovss	%xmm5, -8180(%rbp)
	vsubsd	%xmm7, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovups	-120(%rax), %ymm7
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm10, %xmm2
	vmovups	-8232(%rbp), %ymm10
	vmovups	-8236(%rbp), %ymm6
	vsubss	%xmm2, %xmm8, %xmm8
	vcvtps2pd	%xmm6, %ymm11
	vcvtps2pd	%xmm9, %ymm2
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm10, %ymm9
	vcvtps2pd	%xmm6, %ymm6
	vextractf128	$0x1, %ymm10, %xmm10
	vcvtps2pd	%xmm10, %ymm10
	vmulpd	%ymm14, %ymm10, %ymm4
	vshufps	$221, %ymm12, %ymm7, %ymm10
	vshufps	$136, %ymm12, %ymm7, %ymm7
	vperm2f128	$3, %ymm7, %ymm7, %ymm12
	vmulss	%xmm0, %xmm8, %xmm8
	vmulpd	%ymm14, %ymm9, %ymm9
	vmovss	%xmm8, -4144(%rbp)
	vperm2f128	$3, %ymm10, %ymm10, %ymm8
	vshufps	$68, %ymm8, %ymm10, %ymm13
	vshufps	$238, %ymm8, %ymm10, %ymm8
	vshufps	$68, %ymm12, %ymm7, %ymm10
	vinsertf128	$1, %xmm8, %ymm13, %ymm8
	vshufps	$238, %ymm12, %ymm7, %ymm12
	vmulpd	%ymm15, %ymm2, %ymm7
	vinsertf128	$1, %xmm12, %ymm10, %ymm10
	vaddpd	%ymm6, %ymm7, %ymm7
	vsubps	%ymm10, %ymm8, %ymm8
	vmulpd	%ymm15, %ymm3, %ymm10
	vaddpd	%ymm11, %ymm10, %ymm10
	vaddpd	%ymm4, %ymm7, %ymm7
	vaddpd	%ymm9, %ymm10, %ymm10
	vcvtpd2psy	%ymm7, %xmm7
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm7, %ymm10, %ymm7
	vmulpd	%ymm14, %ymm3, %ymm10
	vaddpd	%ymm10, %ymm11, %ymm11
	vmovd	%r11d, %xmm3
	vsubpd	%ymm9, %ymm11, %ymm11
	vmulpd	%ymm14, %ymm2, %ymm9
	vaddpd	%ymm9, %ymm6, %ymm6
	vcvtpd2psy	%ymm11, %xmm11
	vsubpd	%ymm4, %ymm6, %ymm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm4, %xmm9
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-8204(%rbp), %xmm6, %xmm6
	vmulsd	%xmm1, %xmm6, %xmm12
	vmovd	%r10d, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm11, %ymm11
	vsubps	%ymm11, %ymm7, %ymm11
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8208(%rbp), %xmm7, %xmm7
	vsubps	%ymm11, %ymm8, %ymm11
	vmovss	-52(%rax), %xmm8
	vmulps	.LC32(%rip), %ymm11, %ymm11
	vmovups	%ymm11, -4140(%rbp)
	vsubss	-56(%rax), %xmm8, %xmm2
	vmulsd	%xmm1, %xmm7, %xmm11
	vmovsd	.LC40(%rip), %xmm8
	vaddsd	%xmm4, %xmm12, %xmm12
	vmulsd	%xmm8, %xmm7, %xmm10
	vaddsd	%xmm11, %xmm6, %xmm11
	vaddsd	%xmm6, %xmm10, %xmm10
	vsubsd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm9, %xmm10, %xmm10
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm9, %xmm3, %xmm9
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm10, %xmm2, %xmm7
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vmovss	-44(%rax), %xmm10
	vmulsd	%xmm1, %xmm3, %xmm7
	vsubss	-48(%rax), %xmm10, %xmm2
	vmulsd	%xmm8, %xmm6, %xmm10
	vmovd	%r8d, %xmm6
	vsubsd	%xmm7, %xmm12, %xmm12
	vaddsd	%xmm4, %xmm10, %xmm10
	vmulsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vaddsd	%xmm7, %xmm10, %xmm10
	vaddsd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm8, %xmm3, %xmm3
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm12, %xmm10, %xmm11
	vsubss	%xmm11, %xmm2, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vmulsd	%xmm1, %xmm2, %xmm6
	vaddsd	%xmm7, %xmm2, %xmm11
	vaddsd	%xmm2, %xmm3, %xmm3
	vmulsd	%xmm8, %xmm2, %xmm2
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm10, -4104(%rbp)
	vmovss	-36(%rax), %xmm10
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	-40(%rax), %xmm10, %xmm10
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm4, %xmm10, %xmm10
	vmovd	%edi, %xmm4
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm4, %xmm9
	vaddsd	%xmm6, %xmm4, %xmm6
	vaddsd	%xmm4, %xmm2, %xmm2
	vmulsd	%xmm8, %xmm4, %xmm4
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm9, %xmm3, %xmm3
	vsubsd	%xmm9, %xmm11, %xmm11
	vmovss	%xmm10, -4100(%rbp)
	vmovss	-28(%rax), %xmm10
	vsubss	-32(%rax), %xmm10, %xmm10
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm11, %xmm3, %xmm7
	vmovd	%ecx, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm9, %xmm3, %xmm9
	vsubss	%xmm7, %xmm10, %xmm10
	vmovss	-20(%rax), %xmm7
	vsubss	-24(%rax), %xmm7, %xmm7
	vmulss	%xmm0, %xmm10, %xmm10
	vmovss	%xmm10, -4096(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm10
	vmovd	%edi, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm10, %xmm2, %xmm2
	vsubsd	%xmm10, %xmm6, %xmm10
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	%xmm5, %xmm6, %xmm6
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vsubss	%xmm2, %xmm7, %xmm7
	vmovss	-12(%rax), %xmm2
	vsubss	-16(%rax), %xmm2, %xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm7, -4092(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm1, %xmm7, %xmm7
	vmovss	-4(%rax), %xmm5
	vsubss	-8(%rax), %xmm5, %xmm5
	vaddsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm7, %xmm9, %xmm9
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm2, %xmm4, %xmm2
	vmovd	%ecx, %xmm4
	movq	-8240(%rbp), %rcx
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmulsd	.LC33(%rip), %xmm4, %xmm4
	vsubss	%xmm2, %xmm10, %xmm2
	movq	%rcx, -128(%rax)
	movq	-8232(%rbp), %rcx
	vmulss	%xmm0, %xmm2, %xmm2
	movq	%rcx, -120(%rax)
	movq	-8224(%rbp), %rcx
	vmovss	%xmm2, -4088(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm2
	vmulsd	.LC36(%rip), %xmm6, %xmm1
	movq	%rcx, -112(%rax)
	movq	-8216(%rbp), %rcx
	vsubsd	%xmm4, %xmm2, %xmm2
	movq	%rcx, -104(%rax)
	movq	-8208(%rbp), %rcx
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm8, %xmm3, %xmm1
	movq	%rcx, -96(%rax)
	movq	-8200(%rbp), %rcx
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm4, %xmm4
	vmulsd	.LC35(%rip), %xmm6, %xmm1
	movq	%rcx, -88(%rax)
	vaddsd	%xmm1, %xmm4, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -4084(%rbp)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-4144(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	je	.L1179
.L1057:
	cmpl	$1, %r14d
	ja	.L1180
	vmovss	(%rax), %xmm7
	vxorpd	%xmm12, %xmm12, %xmm12
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovss	8(%rax), %xmm6
	vcvtss2sd	16(%rax), %xmm10, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	24(%rax), %xmm9, %xmm9
	vcvtss2sd	%xmm7, %xmm12, %xmm12
	vmovups	40(%rax), %ymm8
	vcvtss2sd	%xmm6, %xmm11, %xmm11
	vmovups	8(%rax), %ymm4
	vmulsd	.LC21(%rip), %xmm11, %xmm1
	vmulsd	.LC20(%rip), %xmm12, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm10, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm9, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vshufps	$136, %ymm8, %ymm4, %ymm1
	vshufps	$221, %ymm8, %ymm4, %ymm4
	vmovss	%xmm0, -8240(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm3
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm3
	vmovups	24(%rax), %ymm0
	vshufps	$136, 56(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	16(%rax), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 48(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm5
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vmovups	(%rax), %ymm5
	vshufps	$136, 32(%rax), %ymm5, %ymm5
	vperm2f128	$3, %ymm5, %ymm5, %ymm0
	vshufps	$68, %ymm0, %ymm5, %ymm13
	vshufps	$238, %ymm0, %ymm5, %ymm0
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vinsertf128	$1, %xmm0, %ymm13, %ymm0
	vshufps	$68, %ymm5, %ymm4, %ymm8
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm0, %ymm4
	vinsertf128	$1, %xmm5, %ymm8, %ymm8
	vmulpd	.LC23(%rip), %ymm4, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm3, %ymm4
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm4, %ymm5, %ymm4
	vmulpd	.LC23(%rip), %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vaddpd	%ymm3, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	72(%rax), %xmm3, %xmm3
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm1, %ymm4
	vmulpd	.LC25(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vaddpd	%ymm2, %ymm0, %ymm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	80(%rax), %xmm2, %xmm2
	vsubpd	%ymm4, %ymm5, %ymm4
	vmulsd	.LC26(%rip), %xmm2, %xmm5
	vsubpd	%ymm1, %ymm0, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	88(%rax), %xmm1, %xmm1
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vsubps	%ymm0, %ymm8, %ymm0
	vmovsd	.LC27(%rip), %xmm4
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovups	%ymm0, -8236(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rdx), %xmm0, %xmm0
	vmulsd	.LC26(%rip), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm1, %xmm13
	vmulsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm0, %xmm0
	vmovss	76(%rax), %xmm8
	vaddsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	84(%rax), %xmm5
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm8, %xmm0
	vmulsd	.LC26(%rip), %xmm1, %xmm8
	vmulsd	%xmm4, %xmm1, %xmm1
	vmovss	%xmm0, -8204(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	96(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm13
	vaddsd	%xmm8, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm2, %xmm2
	vmovss	92(%rax), %xmm8
	vsubsd	%xmm13, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vmulsd	.LC26(%rip), %xmm0, %xmm5
	vmulsd	%xmm4, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	96(%rax), %xmm4, %xmm4
	vmovss	%xmm3, -8200(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm13
	vmulsd	.LC26(%rip), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm0, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm8, %xmm2
	vmulsd	.LC20(%rip), %xmm3, %xmm8
	vmovss	%xmm2, -8196(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm1
	vmulsd	.LC26(%rip), %xmm2, %xmm2
	vsubsd	%xmm1, %xmm5, %xmm1
	vmovss	100(%rax), %xmm5
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovss	%xmm1, -8192(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	108(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmovss	%xmm0, -8188(%rbp)
	vmulsd	.LC22(%rip), %xmm4, %xmm0
	vsubsd	%xmm8, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmulsd	.LC20(%rip), %xmm1, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	116(%rax), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -8184(%rbp)
	vmovd	%xmm5, %ecx
	movl	32(%rax), %r11d
	vmovss	124(%rax), %xmm5
	vmovss	%xmm6, -4140(%rbp)
	vmovss	96(%rax), %xmm6
	vmovss	%xmm7, -4144(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm13
	vmulsd	.LC28(%rip), %xmm4, %xmm8
	vmovss	%xmm6, -4096(%rbp)
	movl	40(%rax), %r10d
	movl	%r11d, -4128(%rbp)
	vmovss	104(%rax), %xmm6
	movl	48(%rax), %r9d
	movl	56(%rax), %r8d
	vmovss	%xmm6, -4092(%rbp)
	vaddsd	%xmm13, %xmm8, %xmm8
	movl	(%rdx), %edi
	movl	%r10d, -4124(%rbp)
	vmovsd	.LC30(%rip), %xmm13
	vmovss	112(%rax), %xmm6
	movl	%r9d, -4120(%rbp)
	vmulsd	%xmm13, %xmm2, %xmm0
	movl	%r8d, -4116(%rbp)
	vmulsd	%xmm13, %xmm1, %xmm13
	movl	%edi, -4112(%rbp)
	vmovss	%xmm6, -4088(%rbp)
	vsubsd	%xmm0, %xmm8, %xmm8
	vmovss	88(%rax), %xmm0
	vmovss	%xmm0, -4100(%rbp)
	vaddsd	%xmm13, %xmm8, %xmm8
	vmovss	72(%rax), %xmm13
	vmovss	%xmm13, -4108(%rbp)
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm5, %xmm8
	vmovss	16(%rax), %xmm5
	vmovss	%xmm5, -4136(%rbp)
	vmovss	24(%rax), %xmm5
	vmovss	%xmm8, -8180(%rbp)
	vmovss	%xmm5, -4132(%rbp)
	vmovss	80(%rax), %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vmovss	120(%rax), %xmm6
	vmovss	%xmm6, -4084(%rbp)
	testb	%r12b, %r12b
	je	.L1056
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8240(%rbp), %xmm7, %xmm7
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm8, %xmm8, %xmm8
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm12, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4144(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8236(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm11, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8232(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm10, %xmm10
	vxorps	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8228(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4136(%rbp)
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8224(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4132(%rbp)
	vmovd	%r11d, %xmm7
	vcvtss2sd	%xmm7, %xmm9, %xmm9
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8220(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm7, -4128(%rbp)
	vmovd	%r10d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8216(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4124(%rbp)
	vmovd	%r9d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8212(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4120(%rbp)
	vmovd	%r8d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8208(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4116(%rbp)
	vmovd	%edi, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4112(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8204(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm13, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8200(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8196(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8192(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmovss	%xmm5, -4100(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8188(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm5, %xmm5
	vmovss	%xmm5, -4096(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmovss	%xmm5, -4092(%rbp)
	vmovd	%ecx, %xmm5
	vcvtss2sd	%xmm5, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmulsd	%xmm6, %xmm8, %xmm6
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm6
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -4088(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -4084(%rbp)
.L1056:
	movq	-4144(%rbp), %rcx
	subq	$-128, %rax
	subq	$-128, %rdx
	movq	%rcx, -128(%rax)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rax)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rax)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rax)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rax)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-8240(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-8232(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-8224(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-8216(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-8208(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	jne	.L1057
.L1179:
	leaq	128(%r13), %r8
	xorl	%edi, %edi
	movq	%r13, %rsi
	addl	$1, %edi
	cmpl	$32, %edi
	je	.L1125
	.p2align 4,,10
	.p2align 3
.L1181:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L1059:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	subq	$-128, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -128(%rdx)
	cmpl	$32, %eax
	jne	.L1059
	addl	$1, %edi
	addq	$132, %r8
	subq	$-128, %rsi
	cmpl	$32, %edi
	jne	.L1181
.L1125:
	movq	%r15, %rdx
	movq	%r13, %rax
	movl	$32, %esi
	vmovapd	.LC38(%rip), %ymm12
	jmp	.L1058
	.p2align 4,,10
	.p2align 3
.L1183:
	vmovss	24(%rax), %xmm1
	subq	$-128, %rdx
	subq	$-128, %rax
	vaddss	-100(%rax), %xmm1, %xmm1
	vmovss	.LC37(%rip), %xmm0
	vmovss	-124(%rax), %xmm8
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	-128(%rax), %xmm9
	vmovss	-120(%rax), %xmm7
	vaddss	%xmm8, %xmm9, %xmm5
	vmovss	-112(%rax), %xmm6
	vaddss	-116(%rax), %xmm7, %xmm7
	vmovss	%xmm1, -8228(%rbp)
	vmovss	-96(%rax), %xmm1
	vsubss	%xmm9, %xmm8, %xmm9
	vaddss	-92(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm5, %xmm5
	vaddss	-108(%rax), %xmm6, %xmm6
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	-48(%rax), %xmm4
	vmulss	%xmm0, %xmm1, %xmm1
	vaddss	-44(%rax), %xmm4, %xmm4
	vmulss	%xmm0, %xmm6, %xmm6
	vmovss	%xmm5, -8240(%rbp)
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmovss	%xmm7, -8236(%rbp)
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmulss	%xmm0, %xmm4, %xmm3
	vmovss	%xmm1, -8224(%rbp)
	vmovss	-88(%rax), %xmm1
	vaddss	-84(%rax), %xmm1, %xmm1
	vmovss	%xmm6, -8232(%rbp)
	vcvtss2sd	%xmm6, %xmm6, %xmm6
	vmovd	%xmm3, %r11d
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8220(%rbp)
	vmovss	-80(%rax), %xmm1
	vaddss	-76(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8216(%rbp)
	vmovss	-72(%rax), %xmm1
	vaddss	-68(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8212(%rbp)
	vmovss	-128(%rdx), %xmm1
	vaddss	-60(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8208(%rbp)
	vmovss	-56(%rax), %xmm1
	vaddss	-52(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8204(%rbp)
	vmovss	%xmm3, -8200(%rbp)
	vmovss	-40(%rax), %xmm3
	vaddss	-36(%rax), %xmm3, %xmm3
	vmovss	-32(%rax), %xmm1
	vaddss	-28(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm3, %xmm3
	vmovups	-88(%rax), %ymm11
	vmulsd	.LC33(%rip), %xmm7, %xmm7
	vmulsd	.LC35(%rip), %xmm5, %xmm8
	vmulsd	.LC36(%rip), %xmm5, %xmm5
	vmovd	%xmm3, %r10d
	vmovss	%xmm3, -8196(%rbp)
	vmovsd	.LC34(%rip), %xmm2
	vmulss	%xmm0, %xmm1, %xmm3
	vmovss	-24(%rax), %xmm1
	vaddss	-20(%rax), %xmm1, %xmm1
	vmulsd	%xmm2, %xmm6, %xmm6
	vaddsd	%xmm7, %xmm8, %xmm8
	vmovups	-8232(%rbp), %ymm10
	vsubsd	%xmm7, %xmm5, %xmm7
	vmovups	-8236(%rbp), %ymm5
	vmovd	%xmm3, %r9d
	vmovss	%xmm3, -8192(%rbp)
	vmulss	%xmm0, %xmm1, %xmm3
	vmovss	-16(%rax), %xmm1
	vcvtps2pd	%xmm5, %ymm4
	vextractf128	$0x1, %ymm5, %xmm5
	vaddss	-12(%rax), %xmm1, %xmm1
	vcvtps2pd	%xmm5, %ymm5
	vsubsd	%xmm6, %xmm8, %xmm8
	vaddsd	%xmm7, %xmm6, %xmm7
	vmovups	-120(%rax), %ymm6
	vmovd	%xmm3, %edi
	vmovss	%xmm3, -8188(%rbp)
	vmulss	%xmm0, %xmm1, %xmm3
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovd	%xmm3, %ecx
	vmovss	%xmm3, -8184(%rbp)
	vmovss	-8(%rax), %xmm3
	vaddss	-4(%rax), %xmm3, %xmm1
	vmulss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm7, %xmm8, %xmm1
	vsubss	%xmm1, %xmm9, %xmm8
	vmovd	%xmm3, %r8d
	vmovss	%xmm3, -8180(%rbp)
	vmulss	%xmm0, %xmm8, %xmm8
	vmovss	%xmm8, -4144(%rbp)
	vmovaps	-8240(%rbp), %ymm8
	vcvtps2pd	%xmm8, %ymm9
	vextractf128	$0x1, %ymm8, %xmm8
	vcvtps2pd	%xmm8, %ymm1
	vcvtps2pd	%xmm10, %ymm8
	vextractf128	$0x1, %ymm10, %xmm10
	vcvtps2pd	%xmm10, %ymm10
	vmulpd	%ymm12, %ymm10, %ymm3
	vshufps	$221, %ymm11, %ymm6, %ymm10
	vshufps	$136, %ymm11, %ymm6, %ymm6
	vperm2f128	$3, %ymm10, %ymm10, %ymm7
	vperm2f128	$3, %ymm6, %ymm6, %ymm11
	vshufps	$68, %ymm7, %ymm10, %ymm13
	vshufps	$238, %ymm7, %ymm10, %ymm7
	vshufps	$68, %ymm11, %ymm6, %ymm10
	vinsertf128	$1, %xmm7, %ymm13, %ymm7
	vshufps	$238, %ymm11, %ymm6, %ymm11
	vinsertf128	$1, %xmm11, %ymm10, %ymm10
	vsubps	%ymm10, %ymm7, %ymm7
	vmulpd	.LC39(%rip), %ymm9, %ymm10
	vmulpd	%ymm12, %ymm9, %ymm9
	vaddpd	%ymm4, %ymm10, %ymm10
	vmulpd	%ymm12, %ymm8, %ymm8
	vmulpd	.LC39(%rip), %ymm1, %ymm6
	vaddpd	%ymm9, %ymm4, %ymm9
	vmovd	%r11d, %xmm4
	vaddpd	%ymm5, %ymm6, %ymm6
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vaddpd	%ymm8, %ymm10, %ymm10
	vsubpd	%ymm8, %ymm9, %ymm9
	vmulpd	%ymm12, %ymm1, %ymm8
	vaddpd	%ymm8, %ymm5, %ymm5
	vmulsd	%xmm2, %xmm4, %xmm8
	vaddpd	%ymm3, %ymm6, %ymm6
	vcvtpd2psy	%ymm10, %xmm10
	vcvtpd2psy	%ymm9, %xmm9
	vsubpd	%ymm3, %ymm5, %ymm5
	vmovd	%r10d, %xmm3
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm10, %ymm6
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm9, %ymm9
	vsubps	%ymm9, %ymm6, %ymm9
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-8204(%rbp), %xmm5, %xmm5
	vcvtss2sd	-8208(%rbp), %xmm6, %xmm6
	vmulsd	%xmm2, %xmm6, %xmm11
	vsubps	%ymm9, %ymm7, %ymm9
	vmulps	.LC32(%rip), %ymm9, %ymm9
	vmovups	%ymm9, -4140(%rbp)
	vmovss	-52(%rax), %xmm9
	vsubss	-56(%rax), %xmm9, %xmm1
	vmovsd	.LC40(%rip), %xmm7
	vaddsd	%xmm11, %xmm5, %xmm11
	vmulsd	%xmm7, %xmm6, %xmm9
	vsubsd	%xmm8, %xmm11, %xmm11
	vaddsd	%xmm5, %xmm9, %xmm9
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm8, %xmm9, %xmm9
	vaddsd	%xmm8, %xmm3, %xmm8
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm11, %xmm9, %xmm10
	vmovss	-44(%rax), %xmm9
	vsubss	%xmm10, %xmm1, %xmm6
	vmulsd	%xmm2, %xmm5, %xmm10
	vsubss	-48(%rax), %xmm9, %xmm1
	vmulsd	%xmm7, %xmm5, %xmm9
	vmulss	%xmm0, %xmm6, %xmm6
	vaddsd	%xmm4, %xmm10, %xmm10
	vaddsd	%xmm4, %xmm9, %xmm9
	vmulsd	%xmm7, %xmm4, %xmm4
	vmovss	%xmm6, -4108(%rbp)
	vmulsd	%xmm2, %xmm3, %xmm6
	vaddsd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm7, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm9, %xmm9
	vsubsd	%xmm6, %xmm10, %xmm10
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm10, %xmm9, %xmm5
	vsubss	%xmm5, %xmm1, %xmm9
	vmovd	%r9d, %xmm5
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm2, %xmm1, %xmm5
	vaddsd	%xmm1, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm7, %xmm1, %xmm1
	vmulss	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm5, %xmm4, %xmm4
	vsubsd	%xmm5, %xmm8, %xmm8
	vmovss	%xmm9, -4104(%rbp)
	vmovss	-36(%rax), %xmm9
	vsubss	-40(%rax), %xmm9, %xmm9
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm4, %xmm9, %xmm9
	vmovd	%edi, %xmm4
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmulsd	%xmm2, %xmm4, %xmm8
	vaddsd	%xmm5, %xmm4, %xmm5
	vaddsd	%xmm4, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm4, %xmm4
	vmulss	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm8, %xmm6, %xmm6
	vmovss	%xmm9, -4100(%rbp)
	vmovss	-28(%rax), %xmm9
	vsubss	-32(%rax), %xmm9, %xmm9
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vmovd	%ecx, %xmm6
	vsubss	%xmm3, %xmm9, %xmm9
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	%xmm6, %xmm3, %xmm3
	vmovss	-20(%rax), %xmm6
	vaddsd	%xmm3, %xmm4, %xmm4
	vsubss	-24(%rax), %xmm6, %xmm6
	vmulss	%xmm0, %xmm9, %xmm9
	vmovss	%xmm9, -4096(%rbp)
	vmulsd	%xmm2, %xmm3, %xmm9
	vaddsd	%xmm8, %xmm3, %xmm3
	vaddsd	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm9, %xmm5, %xmm9
	vmovd	%r8d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmulsd	%xmm2, %xmm5, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm5, %xmm4, %xmm4
	vsubsd	%xmm5, %xmm3, %xmm5
	vxorpd	%xmm3, %xmm3, %xmm3
	vsubss	%xmm1, %xmm6, %xmm6
	vmovss	-12(%rax), %xmm1
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	-16(%rax), %xmm1, %xmm1
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm4, %xmm4
	vmovd	%edi, %xmm5
	vcvtss2sd	%xmm5, %xmm3, %xmm3
	vmulss	%xmm0, %xmm6, %xmm6
	vmovd	%ecx, %xmm5
	movq	-8240(%rbp), %rcx
	vsubss	%xmm4, %xmm1, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm5, %xmm4, %xmm4
	vmovd	%r8d, %xmm5
	vmulsd	.LC33(%rip), %xmm4, %xmm4
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	movq	%rcx, -128(%rax)
	movq	-8232(%rbp), %rcx
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm6, -4092(%rbp)
	vmovss	-4(%rax), %xmm6
	vsubss	-8(%rax), %xmm6, %xmm6
	vsubsd	%xmm4, %xmm8, %xmm2
	movq	%rcx, -120(%rax)
	movq	-8224(%rbp), %rcx
	vmovss	%xmm1, -4088(%rbp)
	vmulsd	.LC36(%rip), %xmm5, %xmm1
	movq	%rcx, -112(%rax)
	movq	-8216(%rbp), %rcx
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm3, %xmm1
	movq	%rcx, -104(%rax)
	movq	-8208(%rbp), %rcx
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm4, %xmm4
	vmulsd	.LC35(%rip), %xmm5, %xmm1
	movq	%rcx, -96(%rax)
	vaddsd	%xmm1, %xmm4, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm1, %xmm6, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -4084(%rbp)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-4144(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	je	.L1182
.L1058:
	cmpl	$1, %r14d
	ja	.L1183
	vmovss	(%rax), %xmm6
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	8(%rax), %xmm10, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	16(%rax), %xmm9, %xmm9
	vxorpd	%xmm8, %xmm8, %xmm8
	vmulsd	.LC21(%rip), %xmm10, %xmm1
	vcvtss2sd	%xmm6, %xmm11, %xmm11
	vcvtss2sd	24(%rax), %xmm8, %xmm8
	vmovups	40(%rax), %ymm7
	vmulsd	.LC20(%rip), %xmm11, %xmm0
	vmovups	8(%rax), %ymm4
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm9, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm8, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vshufps	$136, %ymm7, %ymm4, %ymm1
	vshufps	$221, %ymm7, %ymm4, %ymm4
	vmovss	%xmm0, -8240(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	24(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vshufps	$136, 56(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	16(%rax), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vshufps	$136, 48(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm3
	vshufps	$68, %ymm3, %ymm1, %ymm5
	vshufps	$238, %ymm3, %ymm1, %ymm3
	vinsertf128	$1, %xmm3, %ymm5, %ymm3
	vmovups	(%rax), %ymm5
	vshufps	$136, 32(%rax), %ymm5, %ymm5
	vperm2f128	$3, %ymm5, %ymm5, %ymm1
	vshufps	$68, %ymm1, %ymm5, %ymm13
	vshufps	$238, %ymm1, %ymm5, %ymm1
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vinsertf128	$1, %xmm1, %ymm13, %ymm1
	vshufps	$68, %ymm5, %ymm4, %ymm7
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm1, %ymm4
	vinsertf128	$1, %xmm5, %ymm7, %ymm7
	vmulpd	.LC23(%rip), %ymm4, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm0, %ymm4
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm4, %ymm5, %ymm4
	vmulpd	.LC23(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vaddpd	%ymm0, %ymm1, %ymm0
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vextractf128	$0x1, %ymm3, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm2, %ymm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	72(%rax), %xmm3, %xmm3
	vmulpd	.LC25(%rip), %ymm4, %ymm4
	vaddpd	%ymm1, %ymm0, %ymm1
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	80(%rax), %xmm2, %xmm2
	vsubpd	%ymm4, %ymm5, %ymm4
	vmulsd	.LC26(%rip), %xmm2, %xmm5
	vsubpd	%ymm0, %ymm1, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	88(%rax), %xmm1, %xmm1
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vsubps	%ymm0, %ymm7, %ymm0
	vmovsd	.LC27(%rip), %xmm4
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovups	%ymm0, -8236(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rdx), %xmm0, %xmm0
	vmulsd	.LC26(%rip), %xmm3, %xmm7
	vmulsd	%xmm4, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm1, %xmm13
	vmulsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm7, %xmm0, %xmm0
	vmovss	76(%rax), %xmm7
	vaddsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	84(%rax), %xmm5
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	.LC26(%rip), %xmm1, %xmm7
	vmulsd	%xmm4, %xmm1, %xmm1
	vmovss	%xmm0, -8204(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	96(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm13
	vaddsd	%xmm7, %xmm3, %xmm3
	vaddsd	%xmm7, %xmm2, %xmm2
	vmovss	92(%rax), %xmm7
	vsubsd	%xmm13, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vmulsd	.LC26(%rip), %xmm0, %xmm5
	vmulsd	%xmm4, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	96(%rax), %xmm4, %xmm4
	vmovss	%xmm3, -8200(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm13
	vmulsd	.LC26(%rip), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm5, %xmm1
	vmovss	100(%rax), %xmm5
	vaddsd	%xmm3, %xmm0, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm7, %xmm2
	vmovss	%xmm2, -8196(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm7
	vmulsd	.LC26(%rip), %xmm2, %xmm0
	vsubsd	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	120(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vsubsd	%xmm0, %xmm3, %xmm0
	vmovss	%xmm1, -8192(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovss	108(%rax), %xmm1
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vmulsd	.LC20(%rip), %xmm3, %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmovss	%xmm0, -8188(%rbp)
	vmulsd	.LC22(%rip), %xmm4, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmulsd	.LC20(%rip), %xmm1, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	116(%rax), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -8184(%rbp)
	vmovd	%xmm5, %ecx
	vmulsd	.LC28(%rip), %xmm4, %xmm7
	vmovss	%xmm6, -4144(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm13
	vmovss	124(%rax), %xmm5
	movl	32(%rax), %ebx
	movl	40(%rax), %r11d
	movl	48(%rax), %r10d
	vaddsd	%xmm13, %xmm7, %xmm7
	movl	56(%rax), %r9d
	vmovsd	.LC30(%rip), %xmm13
	movl	%ebx, -4128(%rbp)
	movl	(%rdx), %r8d
	movl	%r11d, -4124(%rbp)
	vmulsd	%xmm13, %xmm2, %xmm0
	movl	72(%rax), %edi
	movl	%r10d, -4120(%rbp)
	vmulsd	%xmm13, %xmm1, %xmm13
	movl	%r9d, -4116(%rbp)
	movl	%r8d, -4112(%rbp)
	movl	%edi, -4108(%rbp)
	vsubsd	%xmm0, %xmm7, %xmm7
	vmovss	88(%rax), %xmm0
	vmovss	%xmm0, -4100(%rbp)
	vaddsd	%xmm13, %xmm7, %xmm7
	vmovss	80(%rax), %xmm13
	vmovss	%xmm13, -4104(%rbp)
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm7, %xmm5, %xmm7
	vmovss	8(%rax), %xmm5
	vmovss	%xmm5, -4140(%rbp)
	vmovss	16(%rax), %xmm5
	vmovss	%xmm7, -8180(%rbp)
	vmovss	%xmm5, -4136(%rbp)
	vmovss	24(%rax), %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vmovss	96(%rax), %xmm5
	vmovss	%xmm5, -4096(%rbp)
	vmovss	104(%rax), %xmm5
	vmovss	%xmm5, -4092(%rbp)
	vmovss	112(%rax), %xmm5
	vmovss	%xmm5, -4088(%rbp)
	vmovss	120(%rax), %xmm5
	vmovss	%xmm5, -4084(%rbp)
	testb	%r12b, %r12b
	je	.L1063
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8240(%rbp), %xmm5, %xmm5
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm11, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4144(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8236(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm10, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4140(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8232(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm9, %xmm10
	vxorps	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm5, %xmm5
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8228(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm5, -4136(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm10, %xmm8, %xmm10
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtsd2ss	%xmm10, %xmm5, %xmm5
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8224(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm5, -4132(%rbp)
	vmovd	%ebx, %xmm5
	vcvtss2sd	%xmm5, %xmm9, %xmm9
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm5, %xmm5
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8220(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm5, -4128(%rbp)
	vmovd	%r11d, %xmm5
	vcvtss2sd	%xmm5, %xmm8, %xmm8
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8216(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vmovss	%xmm5, -4124(%rbp)
	vmovd	%r10d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8212(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4120(%rbp)
	vmovd	%r9d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8208(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4116(%rbp)
	vmovd	%r8d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8204(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4112(%rbp)
	vmovd	%edi, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4108(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8200(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm13, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8196(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8192(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmovss	%xmm5, -4100(%rbp)
	vaddsd	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8188(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm4, -4096(%rbp)
	vxorps	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm4, %xmm4
	vmovss	%xmm4, -4092(%rbp)
	vmovd	%ecx, %xmm4
	vxorps	%xmm3, %xmm3, %xmm3
	vcvtss2sd	%xmm4, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm7, %xmm6
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm6
	vcvtsd2ss	%xmm0, %xmm3, %xmm3
	vmovss	%xmm3, -4088(%rbp)
	vcvtsd2ss	%xmm6, %xmm4, %xmm4
	vmovss	%xmm4, -4084(%rbp)
.L1063:
	movq	-4144(%rbp), %rcx
	subq	$-128, %rax
	subq	$-128, %rdx
	movq	%rcx, -128(%rax)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rax)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rax)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rax)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rax)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-8240(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-8232(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-8224(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-8216(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-8208(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	jne	.L1058
.L1182:
	addq	$4096, %r13
	addq	$4096, %r15
	cmpq	-8272(%rbp), %r13
	jne	.L1064
	movq	-8264(%rbp), %rax
	xorl	%r11d, %r11d
	leaq	4116(%rax), %r10
.L1070:
	xorl	%r8d, %r8d
	movslq	%r11d, %rdi
	movq	-8248(%rbp), %rsi
	movq	%r10, %r9
	addl	$1, %r8d
	salq	$5, %rdi
	cmpl	$32, %r8d
	je	.L1066
	.p2align 4,,10
	.p2align 3
.L1184:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L1067:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$4096, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -4096(%rcx)
	cmpl	$32, %edx
	jne	.L1067
	addl	$1, %r8d
	addq	$4100, %r9
	addq	$4096, %rsi
	cmpl	$32, %r8d
	jne	.L1184
.L1066:
	addl	$1, %r11d
	subq	$-128, %r10
	cmpl	$32, %r11d
	jne	.L1070
	movq	-8248(%rbp), %r11
	movzbl	-8256(%rbp), %r12d
	vmovapd	.LC38(%rip), %ymm15
	vmovapd	.LC39(%rip), %ymm14
	movq	%r11, %rbx
.L1071:
	leaq	64(%rbx), %rdx
	movq	%rbx, %rax
	movl	$32, %r15d
	jmp	.L1075
	.p2align 4,,10
	.p2align 3
.L1186:
	vmovss	24(%rax), %xmm1
	subq	$-128, %rdx
	subq	$-128, %rax
	vaddss	-100(%rax), %xmm1, %xmm1
	vmovss	.LC37(%rip), %xmm0
	vmovss	-124(%rax), %xmm8
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	-128(%rax), %xmm10
	vmovss	-120(%rax), %xmm9
	vaddss	%xmm8, %xmm10, %xmm6
	vmovss	-112(%rax), %xmm7
	vaddss	-116(%rax), %xmm9, %xmm9
	vmovss	%xmm1, -8228(%rbp)
	vmovss	-96(%rax), %xmm1
	vsubss	%xmm10, %xmm8, %xmm10
	vaddss	-92(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm6, %xmm6
	vaddss	-108(%rax), %xmm7, %xmm7
	vmulss	%xmm0, %xmm9, %xmm9
	vmovss	-48(%rax), %xmm4
	vmulss	%xmm0, %xmm1, %xmm1
	vaddss	-44(%rax), %xmm4, %xmm4
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm6, -8240(%rbp)
	vcvtss2sd	%xmm6, %xmm6, %xmm6
	vmovss	%xmm9, -8236(%rbp)
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vmulss	%xmm0, %xmm4, %xmm5
	vmovss	%xmm1, -8224(%rbp)
	vmovss	-88(%rax), %xmm1
	vaddss	-84(%rax), %xmm1, %xmm1
	vmovss	%xmm7, -8232(%rbp)
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmovd	%xmm5, %r10d
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8220(%rbp)
	vmovss	-80(%rax), %xmm1
	vaddss	-76(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8216(%rbp)
	vmovss	-72(%rax), %xmm1
	vaddss	-68(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8212(%rbp)
	vmovss	-128(%rdx), %xmm1
	vaddss	-60(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8208(%rbp)
	vmovss	-56(%rax), %xmm1
	vaddss	-52(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8204(%rbp)
	vmovss	%xmm5, -8200(%rbp)
	vmovss	-40(%rax), %xmm3
	vaddss	-36(%rax), %xmm3, %xmm3
	vmovss	-32(%rax), %xmm2
	vaddss	-28(%rax), %xmm2, %xmm2
	vmulss	%xmm0, %xmm3, %xmm5
	vmovss	-24(%rax), %xmm3
	vaddss	-20(%rax), %xmm3, %xmm3
	vmovss	-16(%rax), %xmm1
	vaddss	-12(%rax), %xmm1, %xmm1
	vmovd	%xmm5, %r8d
	vmovss	%xmm5, -8196(%rbp)
	vmovups	-88(%rax), %ymm12
	vmulss	%xmm0, %xmm2, %xmm5
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vmulsd	.LC35(%rip), %xmm6, %xmm2
	vmulsd	.LC36(%rip), %xmm6, %xmm6
	vmovd	%xmm5, %edi
	vmovss	%xmm5, -8192(%rbp)
	vmulss	%xmm0, %xmm3, %xmm5
	vaddsd	%xmm9, %xmm2, %xmm2
	vsubsd	%xmm9, %xmm6, %xmm9
	vmovups	-8236(%rbp), %ymm6
	vmovd	%xmm5, %esi
	vmovss	%xmm5, -8188(%rbp)
	vmulss	%xmm0, %xmm1, %xmm5
	vcvtps2pd	%xmm6, %ymm11
	vextractf128	$0x1, %ymm6, %xmm6
	vmovsd	.LC34(%rip), %xmm1
	vcvtps2pd	%xmm6, %ymm6
	vmulsd	%xmm1, %xmm7, %xmm7
	vmovd	%xmm5, %ecx
	vmovss	%xmm5, -8184(%rbp)
	vmovss	-8(%rax), %xmm5
	vaddss	-4(%rax), %xmm5, %xmm4
	vsubsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm9, %xmm7, %xmm9
	vmovups	-120(%rax), %ymm7
	vmulss	%xmm0, %xmm4, %xmm5
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm2, %xmm2
	vmovaps	-8240(%rbp), %ymm9
	vmovss	%xmm5, -8180(%rbp)
	vcvtps2pd	%xmm9, %ymm3
	vextractf128	$0x1, %ymm9, %xmm9
	vsubss	%xmm2, %xmm10, %xmm8
	vmovups	-8232(%rbp), %ymm10
	vcvtps2pd	%xmm9, %ymm2
	vcvtps2pd	%xmm10, %ymm9
	vextractf128	$0x1, %ymm10, %xmm10
	vcvtps2pd	%xmm10, %ymm10
	vmulpd	%ymm15, %ymm10, %ymm4
	vmulss	%xmm0, %xmm8, %xmm8
	vshufps	$221, %ymm12, %ymm7, %ymm10
	vshufps	$136, %ymm12, %ymm7, %ymm7
	vperm2f128	$3, %ymm7, %ymm7, %ymm12
	vmulpd	%ymm15, %ymm9, %ymm9
	vmovss	%xmm8, -4144(%rbp)
	vperm2f128	$3, %ymm10, %ymm10, %ymm8
	vshufps	$68, %ymm8, %ymm10, %ymm13
	vshufps	$238, %ymm8, %ymm10, %ymm8
	vshufps	$68, %ymm12, %ymm7, %ymm10
	vinsertf128	$1, %xmm8, %ymm13, %ymm8
	vshufps	$238, %ymm12, %ymm7, %ymm12
	vmulpd	%ymm14, %ymm2, %ymm7
	vinsertf128	$1, %xmm12, %ymm10, %ymm10
	vaddpd	%ymm6, %ymm7, %ymm7
	vmulpd	%ymm15, %ymm2, %ymm12
	vsubps	%ymm10, %ymm8, %ymm8
	vmulpd	%ymm14, %ymm3, %ymm10
	vaddpd	%ymm11, %ymm10, %ymm10
	vaddpd	%ymm4, %ymm7, %ymm7
	vaddpd	%ymm12, %ymm6, %ymm6
	vaddpd	%ymm9, %ymm10, %ymm10
	vcvtpd2psy	%ymm7, %xmm7
	vsubpd	%ymm4, %ymm6, %ymm6
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm7, %ymm10, %ymm7
	vmulpd	%ymm15, %ymm3, %ymm10
	vaddpd	%ymm10, %ymm11, %ymm11
	vmovd	%r10d, %xmm3
	vcvtpd2psy	%ymm6, %xmm6
	vcvtss2sd	%xmm3, %xmm4, %xmm4
	vmovd	%r8d, %xmm3
	vsubpd	%ymm9, %ymm11, %ymm9
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vcvtpd2psy	%ymm9, %xmm9
	vinsertf128	$0x1, %xmm6, %ymm9, %ymm9
	vsubps	%ymm9, %ymm7, %ymm9
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8208(%rbp), %xmm7, %xmm7
	vmulsd	%xmm1, %xmm7, %xmm11
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-8204(%rbp), %xmm6, %xmm6
	vsubps	%ymm9, %ymm8, %ymm9
	vmovss	-52(%rax), %xmm8
	vmulps	.LC32(%rip), %ymm9, %ymm9
	vmovups	%ymm9, -4140(%rbp)
	vsubss	-56(%rax), %xmm8, %xmm2
	vmulsd	%xmm1, %xmm4, %xmm9
	vmovsd	.LC40(%rip), %xmm8
	vaddsd	%xmm11, %xmm6, %xmm11
	vmulsd	%xmm8, %xmm7, %xmm10
	vsubsd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm6, %xmm10, %xmm10
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm9, %xmm3, %xmm9
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm11, %xmm10, %xmm10
	vmulsd	%xmm1, %xmm6, %xmm11
	vsubss	%xmm10, %xmm2, %xmm7
	vaddsd	%xmm4, %xmm11, %xmm11
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vmovss	-44(%rax), %xmm10
	vmulsd	%xmm1, %xmm3, %xmm7
	vsubss	-48(%rax), %xmm10, %xmm2
	vmulsd	%xmm8, %xmm6, %xmm10
	vsubsd	%xmm7, %xmm11, %xmm11
	vaddsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm7, %xmm10, %xmm10
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm11, %xmm10, %xmm6
	vmulsd	%xmm8, %xmm4, %xmm11
	vmovd	%esi, %xmm4
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vsubss	%xmm6, %xmm2, %xmm10
	vmovd	%edi, %xmm6
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vmulsd	%xmm1, %xmm2, %xmm6
	vaddsd	%xmm7, %xmm2, %xmm7
	vaddsd	%xmm11, %xmm3, %xmm11
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm11, %xmm11
	vsubsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm10, -4104(%rbp)
	vmovss	-36(%rax), %xmm10
	vsubss	-40(%rax), %xmm10, %xmm10
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm6, %xmm4, %xmm6
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm11, %xmm11
	vmulsd	%xmm1, %xmm4, %xmm9
	vsubss	%xmm11, %xmm10, %xmm10
	vmulsd	%xmm8, %xmm3, %xmm11
	vmovd	%ecx, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vsubsd	%xmm9, %xmm7, %xmm7
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm2, %xmm11, %xmm11
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmulsd	%xmm8, %xmm2, %xmm2
	vmovss	%xmm10, -4100(%rbp)
	vmovss	-28(%rax), %xmm10
	vaddsd	%xmm9, %xmm11, %xmm11
	vsubss	-32(%rax), %xmm10, %xmm10
	vaddsd	%xmm4, %xmm2, %xmm2
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm7, %xmm11, %xmm11
	vmovss	-20(%rax), %xmm7
	vsubss	-24(%rax), %xmm7, %xmm7
	vaddsd	%xmm9, %xmm3, %xmm9
	vsubss	%xmm11, %xmm10, %xmm10
	vmulss	%xmm0, %xmm10, %xmm10
	vmovss	%xmm10, -4096(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm10
	vaddsd	%xmm10, %xmm2, %xmm2
	vsubsd	%xmm10, %xmm6, %xmm10
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	%xmm5, %xmm6, %xmm6
	vmulsd	%xmm1, %xmm6, %xmm6
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	vsubsd	%xmm6, %xmm9, %xmm9
	vsubss	%xmm2, %xmm7, %xmm7
	vmovss	-12(%rax), %xmm2
	vsubss	-16(%rax), %xmm2, %xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm9, %xmm2, %xmm2
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm7, -4092(%rbp)
	vmulsd	%xmm8, %xmm4, %xmm7
	vmovd	%esi, %xmm4
	vaddsd	%xmm3, %xmm7, %xmm7
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	%xmm4, %xmm3, %xmm3
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm5, %xmm4, %xmm4
	vmovss	-4(%rax), %xmm5
	vsubss	-8(%rax), %xmm5, %xmm5
	vaddsd	%xmm6, %xmm7, %xmm7
	vmovd	%ecx, %xmm6
	movq	-8240(%rbp), %rcx
	vcvtss2sd	%xmm6, %xmm6, %xmm6
	vmulsd	.LC33(%rip), %xmm6, %xmm6
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm2, %xmm7, %xmm2
	movq	%rcx, -128(%rax)
	movq	-8232(%rbp), %rcx
	vsubss	%xmm2, %xmm10, %xmm2
	movq	%rcx, -120(%rax)
	movq	-8224(%rbp), %rcx
	vmulss	%xmm0, %xmm2, %xmm2
	movq	%rcx, -112(%rax)
	movq	-8216(%rbp), %rcx
	vmovss	%xmm2, -4088(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm2
	vmulsd	.LC36(%rip), %xmm4, %xmm1
	movq	%rcx, -104(%rax)
	movq	-8208(%rbp), %rcx
	vsubsd	%xmm6, %xmm2, %xmm2
	movq	%rcx, -96(%rax)
	movq	-8200(%rbp), %rcx
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm8, %xmm3, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	movq	%rcx, -88(%rax)
	vaddsd	%xmm1, %xmm6, %xmm6
	vmulsd	.LC35(%rip), %xmm4, %xmm1
	vaddsd	%xmm1, %xmm6, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -4084(%rbp)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-4144(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %r15d
	je	.L1185
.L1075:
	cmpl	$1, %r14d
	ja	.L1186
	vmovss	(%rax), %xmm7
	vxorpd	%xmm12, %xmm12, %xmm12
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovss	8(%rax), %xmm6
	vcvtss2sd	16(%rax), %xmm10, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	24(%rax), %xmm9, %xmm9
	vcvtss2sd	%xmm7, %xmm12, %xmm12
	vmovups	40(%rax), %ymm8
	vcvtss2sd	%xmm6, %xmm11, %xmm11
	vmovups	8(%rax), %ymm4
	vmulsd	.LC21(%rip), %xmm11, %xmm1
	vmulsd	.LC20(%rip), %xmm12, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm10, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm9, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vshufps	$136, %ymm8, %ymm4, %ymm1
	vshufps	$221, %ymm8, %ymm4, %ymm4
	vmovss	%xmm0, -8240(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm3
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm3
	vmovups	24(%rax), %ymm0
	vshufps	$136, 56(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	16(%rax), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 48(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm5
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vmovups	(%rax), %ymm5
	vshufps	$136, 32(%rax), %ymm5, %ymm5
	vperm2f128	$3, %ymm5, %ymm5, %ymm0
	vshufps	$68, %ymm0, %ymm5, %ymm13
	vshufps	$238, %ymm0, %ymm5, %ymm0
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vinsertf128	$1, %xmm0, %ymm13, %ymm0
	vshufps	$68, %ymm5, %ymm4, %ymm8
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm0, %ymm4
	vinsertf128	$1, %xmm5, %ymm8, %ymm8
	vmulpd	.LC23(%rip), %ymm4, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm3, %ymm4
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm4, %ymm5, %ymm4
	vmulpd	.LC23(%rip), %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vaddpd	%ymm3, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmovsd	.LC27(%rip), %xmm3
	vaddpd	%ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm1, %ymm4
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm4, %ymm4
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vaddpd	%ymm2, %ymm0, %ymm0
	vsubpd	%ymm4, %ymm5, %ymm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	80(%rax), %xmm5, %xmm5
	vsubpd	%ymm1, %ymm0, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	72(%rax), %xmm1, %xmm1
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vsubps	%ymm0, %ymm8, %ymm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vmulsd	.LC26(%rip), %xmm5, %xmm8
	vcvtss2sd	88(%rax), %xmm4, %xmm4
	vmulsd	%xmm3, %xmm5, %xmm5
	vmovups	%ymm0, -8236(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rdx), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vmulsd	.LC26(%rip), %xmm1, %xmm2
	vmulsd	.LC22(%rip), %xmm4, %xmm13
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm0, %xmm0
	vmovss	76(%rax), %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm0, %xmm0
	vmovss	84(%rax), %xmm8
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulsd	.LC26(%rip), %xmm4, %xmm2
	vmulsd	%xmm3, %xmm4, %xmm4
	vmovss	%xmm0, -8204(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	96(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm13
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm5, %xmm2, %xmm5
	vsubsd	%xmm13, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	104(%rax), %xmm8, %xmm8
	vmulsd	.LC22(%rip), %xmm8, %xmm13
	vmulsd	.LC26(%rip), %xmm8, %xmm8
	vmovss	%xmm1, -8200(%rbp)
	vmulsd	.LC26(%rip), %xmm0, %xmm1
	vmulsd	%xmm3, %xmm0, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vaddsd	%xmm1, %xmm5, %xmm2
	vmovss	92(%rax), %xmm5
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm0, %xmm0
	vsubsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm3, %xmm8
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm5, %xmm2
	vmovss	%xmm2, -8196(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm4
	vmulsd	.LC26(%rip), %xmm2, %xmm2
	vsubsd	%xmm4, %xmm1, %xmm1
	vmovss	100(%rax), %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm4, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	96(%rax), %xmm4, %xmm4
	vmovss	%xmm1, -8192(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	108(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmovss	%xmm0, -8188(%rbp)
	vmulsd	.LC22(%rip), %xmm4, %xmm0
	vsubsd	%xmm8, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmulsd	.LC20(%rip), %xmm1, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	116(%rax), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -8184(%rbp)
	vmovd	%xmm5, %ecx
	movl	32(%rax), %r10d
	vmovss	124(%rax), %xmm5
	vmovss	%xmm6, -4140(%rbp)
	movl	40(%rax), %r9d
	vmovss	%xmm7, -4144(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm13
	vmulsd	.LC28(%rip), %xmm4, %xmm8
	movl	%r10d, -4128(%rbp)
	movl	48(%rax), %r8d
	movl	56(%rax), %edi
	movl	%r9d, -4124(%rbp)
	vmovss	96(%rax), %xmm6
	vaddsd	%xmm13, %xmm8, %xmm8
	movl	%r8d, -4120(%rbp)
	vmovsd	.LC30(%rip), %xmm13
	movl	%edi, -4116(%rbp)
	vmulsd	%xmm13, %xmm2, %xmm0
	vmulsd	%xmm13, %xmm1, %xmm13
	vsubsd	%xmm0, %xmm8, %xmm8
	vmovss	88(%rax), %xmm0
	vaddsd	%xmm13, %xmm8, %xmm8
	vmovss	72(%rax), %xmm13
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm5, %xmm8
	vmovss	16(%rax), %xmm5
	vmovss	%xmm5, -4136(%rbp)
	vmovss	24(%rax), %xmm5
	vmovss	%xmm8, -8180(%rbp)
	vmovss	%xmm5, -4132(%rbp)
	movl	(%rdx), %esi
	vmovss	%xmm6, -4096(%rbp)
	vmovss	104(%rax), %xmm6
	vmovss	80(%rax), %xmm5
	vmovss	%xmm13, -4108(%rbp)
	vmovss	%xmm6, -4092(%rbp)
	vmovss	112(%rax), %xmm6
	movl	%esi, -4112(%rbp)
	vmovss	%xmm5, -4104(%rbp)
	vmovss	%xmm0, -4100(%rbp)
	vmovss	%xmm6, -4088(%rbp)
	vmovss	120(%rax), %xmm6
	vmovss	%xmm6, -4084(%rbp)
	testb	%r12b, %r12b
	je	.L1074
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8240(%rbp), %xmm7, %xmm7
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm8, %xmm8, %xmm8
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm12, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4144(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8236(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm11, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8232(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm10, %xmm10
	vxorps	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8228(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4136(%rbp)
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8224(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4132(%rbp)
	vmovd	%r10d, %xmm7
	vcvtss2sd	%xmm7, %xmm9, %xmm9
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8220(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm7, -4128(%rbp)
	vmovd	%r9d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8216(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4124(%rbp)
	vmovd	%r8d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8212(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4120(%rbp)
	vmovd	%edi, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8208(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4116(%rbp)
	vmovd	%esi, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4112(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8204(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm13, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8200(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8196(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8192(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmovss	%xmm5, -4100(%rbp)
	vaddsd	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8188(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm4, -4096(%rbp)
	vxorps	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -4092(%rbp)
	vmovd	%ecx, %xmm3
	vcvtss2sd	%xmm3, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vxorps	%xmm3, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm8, %xmm6
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm6
	vcvtsd2ss	%xmm0, %xmm4, %xmm4
	vmovss	%xmm4, -4088(%rbp)
	vcvtsd2ss	%xmm6, %xmm3, %xmm3
	vmovss	%xmm3, -4084(%rbp)
.L1074:
	movq	-4144(%rbp), %rcx
	subq	$-128, %rax
	subq	$-128, %rdx
	movq	%rcx, -128(%rax)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rax)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rax)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rax)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rax)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-8240(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-8232(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-8224(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-8216(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-8208(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %r15d
	jne	.L1075
.L1185:
	addq	$4096, %rbx
	cmpq	%rbx, -8272(%rbp)
	jne	.L1071
	movq	-8264(%rbp), %rax
	movl	%r15d, -8332(%rbp)
	leaq	2100(%rax), %rbx
	leaq	52(%rax), %rdi
	addq	$67636, %rax
	movq	%rbx, -8312(%rbp)
	movq	%rax, -8304(%rbp)
	movq	-8248(%rbp), %rax
	movq	%rdi, -8280(%rbp)
	movq	%rbx, -8272(%rbp)
	movq	%rax, -8296(%rbp)
.L1086:
	movq	-8280(%rbp), %rax
	vmovss	.LC37(%rip), %xmm12
	movq	-8272(%rbp), %rdx
	movzbl	-8256(%rbp), %ecx
	jmp	.L1080
.L1188:
	vmovss	-8(%rax), %xmm3
	subq	$-128, %rax
	vaddss	-132(%rax), %xmm3, %xmm3
	vmovss	-144(%rax), %xmm4
	vaddss	-140(%rax), %xmm4, %xmm0
	vmulss	%xmm12, %xmm3, %xmm9
	vmovss	-128(%rax), %xmm4
	vaddss	-124(%rax), %xmm4, %xmm3
	vmovss	-156(%rax), %xmm13
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	-160(%rax), %xmm15
	vmulss	%xmm12, %xmm3, %xmm8
	vmovss	-120(%rax), %xmm3
	vmovss	%xmm9, -8228(%rbp)
	vmovss	-148(%rax), %xmm11
	vaddss	%xmm13, %xmm15, %xmm2
	vaddss	-116(%rax), %xmm3, %xmm4
	vmovss	%xmm0, -8232(%rbp)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vaddss	-152(%rax), %xmm11, %xmm1
	vmulss	%xmm12, %xmm2, %xmm2
	vsubss	%xmm15, %xmm13, %xmm15
	vmovss	%xmm8, -8224(%rbp)
	vmulss	%xmm12, %xmm4, %xmm7
	vmovss	-112(%rax), %xmm4
	vaddss	-108(%rax), %xmm4, %xmm3
	vmulss	%xmm12, %xmm1, %xmm1
	vsubss	-152(%rax), %xmm11, %xmm11
	vmovss	%xmm2, -8240(%rbp)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm2, %xmm13
	vmulss	%xmm12, %xmm3, %xmm6
	vmovss	-104(%rax), %xmm3
	vmovss	%xmm7, -8220(%rbp)
	vaddss	-100(%rax), %xmm3, %xmm3
	vmovss	%xmm1, -8236(%rbp)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmulsd	.LC36(%rip), %xmm2, %xmm10
	vmulsd	.LC33(%rip), %xmm1, %xmm14
	vmulss	%xmm12, %xmm3, %xmm5
	vmovss	%xmm6, -8216(%rbp)
	vmovsd	.LC34(%rip), %xmm3
	vmulsd	%xmm3, %xmm0, %xmm4
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm5, -8212(%rbp)
	vsubsd	%xmm14, %xmm10, %xmm14
	vsubsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm13, %xmm14
	vsubss	%xmm14, %xmm15, %xmm13
	vmulsd	%xmm3, %xmm2, %xmm15
	vmulss	%xmm12, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm1, %xmm15
	vmovss	%xmm13, -4144(%rbp)
	vmovsd	.LC40(%rip), %xmm13
	vsubsd	%xmm4, %xmm15, %xmm15
	vmulsd	%xmm13, %xmm2, %xmm14
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm9, %xmm2, %xmm2
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm15, %xmm14, %xmm15
	vmulsd	%xmm13, %xmm1, %xmm14
	vsubss	%xmm15, %xmm11, %xmm11
	vmovss	-140(%rax), %xmm15
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulss	%xmm12, %xmm11, %xmm11
	vmovss	%xmm11, -4140(%rbp)
	vsubss	-144(%rax), %xmm15, %xmm10
	vmulsd	%xmm3, %xmm1, %xmm15
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm11
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	movq	-8240(%rbp), %rsi
	vaddsd	%xmm15, %xmm0, %xmm15
	vmulsd	%xmm13, %xmm0, %xmm0
	movq	%rsi, -160(%rax)
	movq	-8232(%rbp), %rsi
	vaddsd	%xmm11, %xmm14, %xmm14
	vsubsd	%xmm11, %xmm15, %xmm15
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	movq	%rsi, -152(%rax)
	movq	-8224(%rbp), %rsi
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vsubss	%xmm15, %xmm14, %xmm15
	vxorps	%xmm14, %xmm14, %xmm14
	movq	%rsi, -144(%rax)
	movq	-8216(%rbp), %rsi
	vsubss	%xmm15, %xmm10, %xmm10
	vmovss	-132(%rax), %xmm15
	vsubss	-136(%rax), %xmm15, %xmm9
	vaddsd	%xmm2, %xmm4, %xmm15
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm7, %xmm4, %xmm4
	vmovss	-124(%rax), %xmm7
	vmulss	%xmm12, %xmm10, %xmm10
	vsubss	-128(%rax), %xmm7, %xmm8
	movq	%rsi, -136(%rax)
	movq	-4144(%rbp), %rsi
	vmovss	%xmm10, -4136(%rbp)
	vmulsd	%xmm3, %xmm1, %xmm10
	movq	%rsi, -128(%rax)
	vaddsd	%xmm10, %xmm0, %xmm0
	vsubsd	%xmm10, %xmm15, %xmm15
	vaddsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vsubss	%xmm14, %xmm9, %xmm0
	vmulsd	%xmm13, %xmm2, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm11, %xmm14
	vxorps	%xmm11, %xmm11, %xmm11
	vmovss	-116(%rax), %xmm6
	vsubss	-120(%rax), %xmm6, %xmm7
	vmulss	%xmm12, %xmm0, %xmm0
	vaddsd	%xmm9, %xmm1, %xmm9
	vmovss	%xmm0, -4132(%rbp)
	vmulsd	%xmm3, %xmm4, %xmm0
	movq	-4136(%rbp), %rsi
	movq	%rsi, -120(%rax)
	vaddsd	%xmm0, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm14, %xmm14
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm11, %xmm9, %xmm11
	vmulsd	%xmm13, %xmm1, %xmm9
	vxorps	%xmm1, %xmm1, %xmm1
	vsubss	%xmm11, %xmm8, %xmm8
	vaddsd	%xmm9, %xmm4, %xmm9
	vmulss	%xmm12, %xmm8, %xmm8
	vmovss	%xmm8, -4128(%rbp)
	vmulsd	%xmm3, %xmm2, %xmm8
	vaddsd	%xmm8, %xmm9, %xmm9
	vsubsd	%xmm8, %xmm10, %xmm8
	vcvtsd2ss	%xmm9, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm3, %xmm7, %xmm8
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm12, %xmm1, %xmm1
	vmovss	%xmm1, -4124(%rbp)
	vmulsd	%xmm13, %xmm4, %xmm1
	vmovss	-108(%rax), %xmm4
	vsubss	-112(%rax), %xmm4, %xmm6
	movq	-4128(%rbp), %rsi
	vaddsd	%xmm1, %xmm2, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vsubsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vmulss	%xmm12, %xmm3, %xmm3
	vmovss	%xmm3, -4120(%rbp)
	vmovss	-100(%rax), %xmm3
	vsubss	-104(%rax), %xmm3, %xmm5
	vsubsd	%xmm2, %xmm0, %xmm3
	vmulsd	.LC36(%rip), %xmm7, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm7, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	%xmm0, -4116(%rbp)
	movq	%rsi, -112(%rax)
	movq	-4120(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	je	.L1187
.L1080:
	leaq	-32(%rax), %rsi
	cmpl	$1, %r14d
	ja	.L1188
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-32(%rax), %xmm8, %xmm8
	vcvtss2sd	-24(%rax), %xmm7, %xmm7
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-16(%rax), %xmm6, %xmm6
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8(%rax), %xmm5, %xmm5
	vmulsd	.LC21(%rip), %xmm7, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	(%rax), %xmm4, %xmm4
	vmulsd	.LC20(%rip), %xmm8, %xmm0
	vmulsd	.LC22(%rip), %xmm5, %xmm3
	vmulsd	.LC27(%rip), %xmm8, %xmm2
	vmulsd	.LC22(%rip), %xmm4, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm6, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	-28(%rax), %xmm1
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm9
	vmulsd	.LC26(%rip), %xmm7, %xmm1
	vmulsd	.LC26(%rip), %xmm6, %xmm0
	vmovss	%xmm9, -8240(%rbp)
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm1
	vmovss	-20(%rax), %xmm2
	vsubsd	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm3
	vmovss	-12(%rax), %xmm1
	vmulsd	.LC26(%rip), %xmm5, %xmm2
	vmovss	%xmm3, -8236(%rbp)
	vmovd	%xmm3, %r8d
	vmulsd	.LC27(%rip), %xmm7, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm3, %xmm3
	vsubsd	%xmm14, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm3
	vmulsd	.LC27(%rip), %xmm6, %xmm1
	vmulsd	.LC26(%rip), %xmm4, %xmm0
	vmovss	%xmm3, -8232(%rbp)
	vmovd	%xmm3, %ebx
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	8(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm15
	vaddsd	%xmm1, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vsubsd	%xmm15, %xmm2, %xmm2
	vmulsd	.LC27(%rip), %xmm5, %xmm15
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vmovss	-4(%rax), %xmm2
	vsubss	%xmm1, %xmm2, %xmm13
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	16(%rax), %xmm2, %xmm2
	vmulsd	.LC26(%rip), %xmm3, %xmm1
	vaddsd	%xmm15, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm2, %xmm15
	vmovss	%xmm13, -8228(%rbp)
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm15, %xmm0, %xmm0
	vmovss	4(%rax), %xmm15
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm15, %xmm11
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovss	%xmm11, -8224(%rbp)
	vcvtss2sd	24(%rax), %xmm15, %xmm15
	vmulsd	.LC27(%rip), %xmm4, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC26(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm15, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	12(%rax), %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm1
	vmulsd	.LC20(%rip), %xmm3, %xmm0
	vmovss	%xmm1, -8220(%rbp)
	vsubsd	%xmm0, %xmm14, %xmm0
	vmulsd	.LC21(%rip), %xmm2, %xmm14
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulsd	.LC20(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	20(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm10
	vmulsd	.LC29(%rip), %xmm3, %xmm0
	vmulsd	.LC28(%rip), %xmm4, %xmm14
	vmovss	%xmm10, -8216(%rbp)
	vaddsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm2, %xmm0
	vsubsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	28(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vmovss	-32(%rax), %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovss	-24(%rax), %xmm0
	vmovss	%xmm14, -8212(%rbp)
	vmovss	%xmm0, -4140(%rbp)
	vmovss	-16(%rax), %xmm0
	vmovss	%xmm0, -4136(%rbp)
	vmovss	-8(%rax), %xmm0
	vmovss	%xmm0, -4132(%rbp)
	vmovss	(%rax), %xmm0
	vmovss	%xmm0, -4128(%rbp)
	vmovss	8(%rax), %xmm0
	vmovss	%xmm0, -4124(%rbp)
	vmovss	16(%rax), %xmm0
	vmovss	%xmm0, -4120(%rbp)
	vmovss	24(%rax), %xmm0
	vmovss	%xmm0, -4116(%rbp)
	testb	%cl, %cl
	je	.L1079
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovd	%r8d, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vmovd	%ebx, %xmm7
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4136(%rbp)
	vxorps	%xmm6, %xmm6, %xmm6
	vaddsd	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm11, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm4, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm4, %xmm6, %xmm6
	vxorps	%xmm4, %xmm4, %xmm4
	vmovss	%xmm6, -4128(%rbp)
	vxorps	%xmm6, %xmm6, %xmm6
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm10, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmovss	%xmm5, -4124(%rbp)
	vaddsd	%xmm2, %xmm0, %xmm2
	vmulsd	.LC33(%rip), %xmm14, %xmm0
	vcvtsd2ss	%xmm2, %xmm4, %xmm4
	vmovss	%xmm4, -4120(%rbp)
	vaddsd	%xmm15, %xmm0, %xmm15
	vcvtsd2ss	%xmm15, %xmm6, %xmm6
	vmovss	%xmm6, -4116(%rbp)
.L1079:
	movq	-4144(%rbp), %rdi
	subq	$-128, %rax
	movq	%rdi, (%rsi)
	movq	-4136(%rbp), %rdi
	movq	%rdi, 8(%rsi)
	movq	-4128(%rbp), %rdi
	movq	%rdi, 16(%rsi)
	movq	-4120(%rbp), %rdi
	movq	%rdi, 24(%rsi)
	movq	-8240(%rbp), %rsi
	movq	%rsi, -128(%rax)
	movq	-8232(%rbp), %rsi
	movq	%rsi, -120(%rax)
	movq	-8224(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-8216(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	jne	.L1080
.L1187:
	movq	-8296(%rbp), %rdx
	movl	$9, %r12d
	xorl	%ecx, %ecx
	movl	$8, %ebx
	movl	$7, %r11d
	movl	$3, %edi
	movl	$2, %esi
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$15, %r15d
	movq	%rdx, %rax
	jmp	.L1124
.L1189:
	vmovss	4(%rax), %xmm0
	vmovss	128(%rax), %xmm1
	vmovss	%xmm0, 128(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %esi
	ja	.L1082
	movslq	%esi, %r13
	vmovss	256(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 256(%rax)
	cmpl	$15, %edi
	ja	.L1082
	movslq	%edi, %r13
	vmovss	384(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 384(%rax)
	cmpl	$15, %r8d
	ja	.L1082
	movslq	%r8d, %r13
	vmovss	512(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 512(%rax)
	cmpl	$15, %r9d
	ja	.L1082
	movslq	%r9d, %r13
	vmovss	640(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 640(%rax)
	cmpl	$15, %r10d
	ja	.L1082
	movslq	%r10d, %r13
	vmovss	768(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 768(%rax)
	cmpl	$15, %r11d
	ja	.L1082
	movslq	%r11d, %r13
	vmovss	896(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 896(%rax)
	cmpl	$15, %ebx
	ja	.L1082
	movslq	%ebx, %r13
	vmovss	1024(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 1024(%rax)
	cmpl	$15, %r12d
	ja	.L1082
	movslq	%r12d, %r13
	vmovss	1152(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	9(%rcx), %r13d
	vmovss	%xmm0, 1152(%rax)
	cmpl	$15, %r13d
	ja	.L1082
	movslq	%r13d, %r13
	vmovss	1280(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	10(%rcx), %r13d
	vmovss	%xmm0, 1280(%rax)
	cmpl	$15, %r13d
	ja	.L1082
	movslq	%r13d, %r13
	vmovss	1408(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	11(%rcx), %r13d
	vmovss	%xmm0, 1408(%rax)
	cmpl	$15, %r13d
	ja	.L1082
	movslq	%r13d, %r13
	vmovss	1536(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	12(%rcx), %r13d
	vmovss	%xmm0, 1536(%rax)
	cmpl	$15, %r13d
	ja	.L1082
	movslq	%r13d, %r13
	vmovss	1664(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	13(%rcx), %r13d
	vmovss	%xmm0, 1664(%rax)
	cmpl	$15, %r13d
	ja	.L1082
	movslq	%r13d, %r13
	vmovss	1792(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 1792(%rax)
	cmpl	$15, %r15d
	jne	.L1082
	vmovss	60(%rdx), %xmm0
	vmovss	1920(%rax), %xmm1
	vmovss	%xmm1, 60(%rdx)
	vmovss	%xmm0, 1920(%rax)
.L1082:
	addl	$1, %r15d
	subq	$-128, %rdx
	addq	$132, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
.L1124:
	addl	$1, %ecx
	cmpl	$16, %ecx
	jne	.L1189
	movq	-8280(%rbp), %rax
	vmovss	.LC37(%rip), %xmm12
	movq	-8272(%rbp), %rdx
	movzbl	-8256(%rbp), %ecx
	jmp	.L1081
.L1191:
	vmovss	-8(%rax), %xmm3
	subq	$-128, %rax
	vaddss	-132(%rax), %xmm3, %xmm3
	vmovss	-144(%rax), %xmm4
	vaddss	-140(%rax), %xmm4, %xmm0
	vmulss	%xmm12, %xmm3, %xmm9
	vmovss	-128(%rax), %xmm4
	vaddss	-124(%rax), %xmm4, %xmm3
	vmovss	-156(%rax), %xmm13
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	-160(%rax), %xmm15
	vmulss	%xmm12, %xmm3, %xmm8
	vmovss	-120(%rax), %xmm3
	vmovss	%xmm9, -8228(%rbp)
	vmovss	-148(%rax), %xmm11
	vaddss	%xmm13, %xmm15, %xmm2
	vaddss	-116(%rax), %xmm3, %xmm4
	vmovss	%xmm0, -8232(%rbp)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vaddss	-152(%rax), %xmm11, %xmm1
	vmulss	%xmm12, %xmm2, %xmm2
	vsubss	%xmm15, %xmm13, %xmm15
	vmovss	%xmm8, -8224(%rbp)
	vmulss	%xmm12, %xmm4, %xmm7
	vmovss	-112(%rax), %xmm4
	vaddss	-108(%rax), %xmm4, %xmm3
	vmulss	%xmm12, %xmm1, %xmm1
	vsubss	-152(%rax), %xmm11, %xmm11
	vmovss	%xmm2, -8240(%rbp)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm2, %xmm13
	vmulss	%xmm12, %xmm3, %xmm6
	vmovss	-104(%rax), %xmm3
	vmovss	%xmm7, -8220(%rbp)
	vaddss	-100(%rax), %xmm3, %xmm3
	vmovss	%xmm1, -8236(%rbp)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmulsd	.LC36(%rip), %xmm2, %xmm10
	vmulsd	.LC33(%rip), %xmm1, %xmm14
	vmulss	%xmm12, %xmm3, %xmm5
	vmovss	%xmm6, -8216(%rbp)
	vmovsd	.LC34(%rip), %xmm3
	vmulsd	%xmm3, %xmm0, %xmm4
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm5, -8212(%rbp)
	vsubsd	%xmm14, %xmm10, %xmm14
	vsubsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm13, %xmm14
	vsubss	%xmm14, %xmm15, %xmm13
	vmulsd	%xmm3, %xmm2, %xmm15
	vmulss	%xmm12, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm1, %xmm15
	vmovss	%xmm13, -4144(%rbp)
	vmovsd	.LC40(%rip), %xmm13
	vsubsd	%xmm4, %xmm15, %xmm15
	vmulsd	%xmm13, %xmm2, %xmm14
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm9, %xmm2, %xmm2
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm15, %xmm14, %xmm15
	vmulsd	%xmm13, %xmm1, %xmm14
	vsubss	%xmm15, %xmm11, %xmm11
	vmovss	-140(%rax), %xmm15
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulss	%xmm12, %xmm11, %xmm11
	vmovss	%xmm11, -4140(%rbp)
	vsubss	-144(%rax), %xmm15, %xmm10
	vmulsd	%xmm3, %xmm1, %xmm15
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm11
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	movq	-8240(%rbp), %rsi
	vaddsd	%xmm15, %xmm0, %xmm15
	vmulsd	%xmm13, %xmm0, %xmm0
	movq	%rsi, -160(%rax)
	movq	-8232(%rbp), %rsi
	vaddsd	%xmm11, %xmm14, %xmm14
	vsubsd	%xmm11, %xmm15, %xmm15
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	movq	%rsi, -152(%rax)
	movq	-8224(%rbp), %rsi
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vsubss	%xmm15, %xmm14, %xmm15
	vxorps	%xmm14, %xmm14, %xmm14
	movq	%rsi, -144(%rax)
	movq	-8216(%rbp), %rsi
	vsubss	%xmm15, %xmm10, %xmm10
	vmovss	-132(%rax), %xmm15
	vsubss	-136(%rax), %xmm15, %xmm9
	vaddsd	%xmm2, %xmm4, %xmm15
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm7, %xmm4, %xmm4
	vmovss	-124(%rax), %xmm7
	vmulss	%xmm12, %xmm10, %xmm10
	vsubss	-128(%rax), %xmm7, %xmm8
	movq	%rsi, -136(%rax)
	movq	-4144(%rbp), %rsi
	vmovss	%xmm10, -4136(%rbp)
	vmulsd	%xmm3, %xmm1, %xmm10
	movq	%rsi, -128(%rax)
	vaddsd	%xmm10, %xmm0, %xmm0
	vsubsd	%xmm10, %xmm15, %xmm15
	vaddsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vsubss	%xmm14, %xmm9, %xmm0
	vmulsd	%xmm13, %xmm2, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm11, %xmm14
	vxorps	%xmm11, %xmm11, %xmm11
	vmovss	-116(%rax), %xmm6
	vsubss	-120(%rax), %xmm6, %xmm7
	vmulss	%xmm12, %xmm0, %xmm0
	vaddsd	%xmm9, %xmm1, %xmm9
	vmovss	%xmm0, -4132(%rbp)
	vmulsd	%xmm3, %xmm4, %xmm0
	movq	-4136(%rbp), %rsi
	movq	%rsi, -120(%rax)
	vaddsd	%xmm0, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm14, %xmm14
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm11, %xmm9, %xmm11
	vmulsd	%xmm13, %xmm1, %xmm9
	vxorps	%xmm1, %xmm1, %xmm1
	vsubss	%xmm11, %xmm8, %xmm8
	vaddsd	%xmm9, %xmm4, %xmm9
	vmulss	%xmm12, %xmm8, %xmm8
	vmovss	%xmm8, -4128(%rbp)
	vmulsd	%xmm3, %xmm2, %xmm8
	vaddsd	%xmm8, %xmm9, %xmm9
	vsubsd	%xmm8, %xmm10, %xmm8
	vcvtsd2ss	%xmm9, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm3, %xmm7, %xmm8
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm12, %xmm1, %xmm1
	vmovss	%xmm1, -4124(%rbp)
	vmulsd	%xmm13, %xmm4, %xmm1
	vmovss	-108(%rax), %xmm4
	vsubss	-112(%rax), %xmm4, %xmm6
	movq	-4128(%rbp), %rsi
	vaddsd	%xmm1, %xmm2, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vsubsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vmulss	%xmm12, %xmm3, %xmm3
	vmovss	%xmm3, -4120(%rbp)
	vmovss	-100(%rax), %xmm3
	vsubss	-104(%rax), %xmm3, %xmm5
	vsubsd	%xmm2, %xmm0, %xmm3
	vmulsd	.LC36(%rip), %xmm7, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm7, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	%xmm0, -4116(%rbp)
	movq	%rsi, -112(%rax)
	movq	-4120(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rdx, %rax
	je	.L1190
.L1081:
	leaq	-32(%rax), %rsi
	cmpl	$1, %r14d
	ja	.L1191
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-32(%rax), %xmm8, %xmm8
	vcvtss2sd	-24(%rax), %xmm7, %xmm7
	vmulsd	.LC20(%rip), %xmm8, %xmm0
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-16(%rax), %xmm6, %xmm6
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	.LC21(%rip), %xmm7, %xmm1
	vcvtss2sd	-8(%rax), %xmm5, %xmm5
	vmulsd	.LC22(%rip), %xmm5, %xmm3
	vmulsd	.LC27(%rip), %xmm8, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm6, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	-28(%rax), %xmm1
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm9
	vmulsd	.LC26(%rip), %xmm7, %xmm1
	vmulsd	.LC26(%rip), %xmm6, %xmm0
	vmovss	%xmm9, -8240(%rbp)
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm1
	vmovss	-20(%rax), %xmm2
	vsubsd	%xmm3, %xmm1, %xmm1
	vmulsd	.LC27(%rip), %xmm7, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm4
	vmovss	-12(%rax), %xmm1
	vmulsd	.LC26(%rip), %xmm5, %xmm2
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm4, -8236(%rbp)
	vmovd	%xmm4, %r8d
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	(%rax), %xmm4, %xmm4
	vmulsd	.LC22(%rip), %xmm4, %xmm14
	vaddsd	%xmm2, %xmm3, %xmm3
	vsubsd	%xmm14, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm3
	vmulsd	.LC27(%rip), %xmm6, %xmm1
	vmulsd	.LC26(%rip), %xmm4, %xmm0
	vmovss	%xmm3, -8232(%rbp)
	vmovd	%xmm3, %ebx
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	8(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm15
	vaddsd	%xmm1, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vsubsd	%xmm15, %xmm2, %xmm2
	vmulsd	.LC27(%rip), %xmm5, %xmm15
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vmovss	-4(%rax), %xmm2
	vsubss	%xmm1, %xmm2, %xmm13
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	16(%rax), %xmm2, %xmm2
	vmulsd	.LC26(%rip), %xmm3, %xmm1
	vaddsd	%xmm15, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm2, %xmm15
	vmovss	%xmm13, -8228(%rbp)
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm15, %xmm0, %xmm0
	vmovss	4(%rax), %xmm15
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm15, %xmm11
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovss	%xmm11, -8224(%rbp)
	vcvtss2sd	24(%rax), %xmm15, %xmm15
	vmulsd	.LC27(%rip), %xmm4, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC26(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm15, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	12(%rax), %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm1
	vmulsd	.LC20(%rip), %xmm3, %xmm0
	vmovss	%xmm1, -8220(%rbp)
	vsubsd	%xmm0, %xmm14, %xmm0
	vmulsd	.LC21(%rip), %xmm2, %xmm14
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulsd	.LC20(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	20(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm10
	vmulsd	.LC29(%rip), %xmm3, %xmm0
	vmulsd	.LC28(%rip), %xmm4, %xmm14
	vmovss	%xmm10, -8216(%rbp)
	vaddsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm2, %xmm0
	vsubsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	28(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vmovss	-32(%rax), %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovss	-24(%rax), %xmm0
	vmovss	%xmm14, -8212(%rbp)
	vmovss	%xmm0, -4140(%rbp)
	vmovss	-16(%rax), %xmm0
	vmovss	%xmm0, -4136(%rbp)
	vmovss	-8(%rax), %xmm0
	vmovss	%xmm0, -4132(%rbp)
	vmovss	(%rax), %xmm0
	vmovss	%xmm0, -4128(%rbp)
	vmovss	8(%rax), %xmm0
	vmovss	%xmm0, -4124(%rbp)
	vmovss	16(%rax), %xmm0
	vmovss	%xmm0, -4120(%rbp)
	vmovss	24(%rax), %xmm0
	vmovss	%xmm0, -4116(%rbp)
	testb	%cl, %cl
	je	.L1085
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovd	%r8d, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vmovd	%ebx, %xmm7
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4136(%rbp)
	vaddsd	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm11, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm4, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm4, -4128(%rbp)
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm10, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -4124(%rbp)
	vxorps	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm2
	vmulsd	.LC33(%rip), %xmm14, %xmm0
	vcvtsd2ss	%xmm2, %xmm5, %xmm5
	vmovss	%xmm5, -4120(%rbp)
	vaddsd	%xmm15, %xmm0, %xmm15
	vcvtsd2ss	%xmm15, %xmm3, %xmm3
	vmovss	%xmm3, -4116(%rbp)
.L1085:
	movq	-4144(%rbp), %rdi
	subq	$-128, %rax
	movq	%rdi, (%rsi)
	movq	-4136(%rbp), %rdi
	movq	%rdi, 8(%rsi)
	movq	-4128(%rbp), %rdi
	movq	%rdi, 16(%rsi)
	movq	-4120(%rbp), %rdi
	movq	%rdi, 24(%rsi)
	movq	-8240(%rbp), %rsi
	movq	%rsi, -128(%rax)
	movq	-8232(%rbp), %rsi
	movq	%rsi, -120(%rax)
	movq	-8224(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-8216(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rdx, %rax
	jne	.L1081
.L1190:
	addq	$4096, %rax
	movq	%rax, -8272(%rbp)
	addq	$4096, -8296(%rbp)
	addq	$4096, -8280(%rbp)
	cmpq	-8304(%rbp), %rax
	jne	.L1086
	movl	-8332(%rbp), %r15d
	movl	$0, -8272(%rbp)
	movq	-8248(%rbp), %rax
	movl	%r14d, -8332(%rbp)
	movl	%r15d, -8296(%rbp)
	movq	%rax, -8280(%rbp)
.L1087:
	movslq	-8272(%rbp), %r15
	movl	$8, %r13d
	xorl	%esi, %esi
	movl	$7, %r12d
	movq	-8280(%rbp), %rax
	movl	$6, %ebx
	movl	$5, %r11d
	movl	$4, %r10d
	movq	-8248(%rbp), %rdx
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$15, %r14d
	movq	%r15, %rcx
	salq	$7, %r15
	salq	$5, %rcx
	jmp	.L1090
.L1192:
	vmovss	4(%rax), %xmm0
	vmovss	4096(%rax), %xmm1
	vmovss	%xmm0, 4096(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %r8d
	ja	.L1089
	movslq	%r8d, %rdi
	vmovss	8192(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 8192(%rax)
	cmpl	$15, %r9d
	ja	.L1089
	movslq	%r9d, %rdi
	vmovss	12288(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 12288(%rax)
	cmpl	$15, %r10d
	ja	.L1089
	movslq	%r10d, %rdi
	vmovss	16384(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 16384(%rax)
	cmpl	$15, %r11d
	ja	.L1089
	movslq	%r11d, %rdi
	vmovss	20480(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 20480(%rax)
	cmpl	$15, %ebx
	ja	.L1089
	movslq	%ebx, %rdi
	vmovss	24576(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 24576(%rax)
	cmpl	$15, %r12d
	ja	.L1089
	movslq	%r12d, %rdi
	vmovss	28672(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 28672(%rax)
	cmpl	$15, %r13d
	ja	.L1089
	movslq	%r13d, %rdi
	vmovss	32768(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	8(%rsi), %edi
	vmovss	%xmm0, 32768(%rax)
	cmpl	$15, %edi
	ja	.L1089
	movslq	%edi, %rdi
	vmovss	36864(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	9(%rsi), %edi
	vmovss	%xmm0, 36864(%rax)
	cmpl	$15, %edi
	ja	.L1089
	movslq	%edi, %rdi
	vmovss	40960(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	10(%rsi), %edi
	vmovss	%xmm0, 40960(%rax)
	cmpl	$15, %edi
	ja	.L1089
	movslq	%edi, %rdi
	vmovss	45056(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	11(%rsi), %edi
	vmovss	%xmm0, 45056(%rax)
	cmpl	$15, %edi
	ja	.L1089
	movslq	%edi, %rdi
	vmovss	49152(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	12(%rsi), %edi
	vmovss	%xmm0, 49152(%rax)
	cmpl	$15, %edi
	ja	.L1089
	movslq	%edi, %rdi
	vmovss	53248(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	13(%rsi), %edi
	vmovss	%xmm0, 53248(%rax)
	cmpl	$15, %edi
	ja	.L1089
	movslq	%edi, %rdi
	vmovss	57344(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 57344(%rax)
	cmpl	$15, %r14d
	jne	.L1089
	leaq	(%rdx,%r15), %rdi
	vmovss	61440(%rax), %xmm1
	vmovss	60(%rdi), %xmm0
	vmovss	%xmm1, 60(%rdi)
	vmovss	%xmm0, 61440(%rax)
.L1089:
	addl	$1, %r14d
	addq	$4096, %rdx
	addq	$4100, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
	addl	$1, %r13d
.L1090:
	addl	$1, %esi
	cmpl	$16, %esi
	jne	.L1192
	addl	$1, -8272(%rbp)
	movl	-8272(%rbp), %eax
	subq	$-128, -8280(%rbp)
	cmpl	$16, %eax
	jne	.L1087
	movl	-8296(%rbp), %r15d
	movl	-8332(%rbp), %r14d
	movq	-8312(%rbp), %rdx
	movzbl	-8256(%rbp), %ecx
	vmovsd	.LC26(%rip), %xmm15
.L1091:
	vmovss	.LC37(%rip), %xmm11
	leaq	-2048(%rdx), %rax
	jmp	.L1095
.L1194:
	vmovss	-24(%rax), %xmm4
	subq	$-128, %rax
	vaddss	-148(%rax), %xmm4, %xmm1
	vmovss	-144(%rax), %xmm3
	vmovss	-136(%rax), %xmm4
	vaddss	-140(%rax), %xmm3, %xmm0
	vmulss	%xmm11, %xmm1, %xmm1
	vaddss	-132(%rax), %xmm4, %xmm3
	vmovss	-156(%rax), %xmm13
	vmovss	-160(%rax), %xmm10
	vmulss	%xmm11, %xmm0, %xmm0
	vmulss	%xmm11, %xmm3, %xmm9
	vmovss	-128(%rax), %xmm3
	vmovss	%xmm1, -8236(%rbp)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmulsd	.LC33(%rip), %xmm1, %xmm14
	vaddss	%xmm13, %xmm10, %xmm2
	vaddss	-124(%rax), %xmm3, %xmm3
	vmovss	-120(%rax), %xmm4
	vsubss	%xmm10, %xmm13, %xmm10
	vmovss	%xmm0, -8232(%rbp)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulss	%xmm11, %xmm2, %xmm2
	vmovss	%xmm9, -8228(%rbp)
	vmulss	%xmm11, %xmm3, %xmm8
	vaddss	-116(%rax), %xmm4, %xmm3
	vmovss	-104(%rax), %xmm4
	vmulss	%xmm11, %xmm3, %xmm7
	vmovss	-112(%rax), %xmm3
	vmovss	%xmm2, -8240(%rbp)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vaddss	-108(%rax), %xmm3, %xmm3
	vmovss	%xmm8, -8224(%rbp)
	vmulsd	.LC36(%rip), %xmm2, %xmm12
	vmulsd	.LC35(%rip), %xmm2, %xmm13
	vmulss	%xmm11, %xmm3, %xmm6
	vaddss	-100(%rax), %xmm4, %xmm3
	vmovss	%xmm7, -8220(%rbp)
	vmulss	%xmm11, %xmm3, %xmm5
	vmovsd	.LC34(%rip), %xmm3
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm6, -8216(%rbp)
	vmulsd	%xmm3, %xmm0, %xmm4
	vsubsd	%xmm14, %xmm12, %xmm14
	vmulsd	%xmm3, %xmm2, %xmm12
	vmovss	%xmm5, -8212(%rbp)
	vsubsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm13, %xmm14
	vsubss	%xmm14, %xmm10, %xmm13
	vmovss	-148(%rax), %xmm14
	vsubss	-152(%rax), %xmm14, %xmm10
	vmulss	%xmm11, %xmm13, %xmm13
	vmovss	%xmm13, -4144(%rbp)
	vmovsd	.LC40(%rip), %xmm13
	vmulsd	%xmm13, %xmm2, %xmm14
	vaddsd	%xmm12, %xmm1, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm12
	vxorps	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm14
	vcvtsd2ss	%xmm12, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm2, %xmm14, %xmm14
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm4, %xmm4
	vsubss	%xmm14, %xmm10, %xmm12
	vmulsd	%xmm3, %xmm1, %xmm10
	vmulss	%xmm11, %xmm12, %xmm12
	vmovss	%xmm12, -4140(%rbp)
	vmovss	-140(%rax), %xmm14
	vmulsd	%xmm3, %xmm2, %xmm12
	vsubss	-144(%rax), %xmm14, %xmm9
	vmulsd	%xmm13, %xmm1, %xmm14
	vaddsd	%xmm10, %xmm0, %xmm1
	movq	-8240(%rbp), %rsi
	vsubsd	%xmm12, %xmm1, %xmm10
	vxorps	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulsd	%xmm13, %xmm0, %xmm0
	movq	%rsi, -160(%rax)
	movq	-8232(%rbp), %rsi
	vcvtsd2ss	%xmm10, %xmm1, %xmm1
	vaddsd	%xmm12, %xmm14, %xmm14
	movq	%rsi, -152(%rax)
	movq	-8224(%rbp), %rsi
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm1, %xmm14, %xmm14
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	movq	%rsi, -144(%rax)
	movq	-8216(%rbp), %rsi
	vsubss	%xmm14, %xmm9, %xmm10
	vmovss	-132(%rax), %xmm14
	vsubss	-136(%rax), %xmm14, %xmm9
	vmulss	%xmm11, %xmm10, %xmm10
	movq	%rsi, -136(%rax)
	movq	-4144(%rbp), %rsi
	vmovss	%xmm10, -4136(%rbp)
	vmulsd	%xmm3, %xmm1, %xmm10
	vsubsd	%xmm10, %xmm4, %xmm14
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm7, %xmm4, %xmm4
	vmovss	-124(%rax), %xmm7
	vaddsd	%xmm10, %xmm0, %xmm0
	vsubss	-128(%rax), %xmm7, %xmm8
	movq	%rsi, -128(%rax)
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm14, %xmm0, %xmm14
	vsubss	%xmm14, %xmm9, %xmm0
	vmulsd	%xmm13, %xmm2, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm12, %xmm14
	vxorps	%xmm12, %xmm12, %xmm12
	vmovss	-116(%rax), %xmm6
	vsubss	-120(%rax), %xmm6, %xmm7
	vmulss	%xmm11, %xmm0, %xmm0
	vaddsd	%xmm9, %xmm1, %xmm9
	vmulsd	%xmm13, %xmm1, %xmm1
	vmovss	%xmm0, -4132(%rbp)
	vmulsd	%xmm3, %xmm4, %xmm0
	movq	-4136(%rbp), %rsi
	vaddsd	%xmm1, %xmm4, %xmm1
	movq	%rsi, -120(%rax)
	vaddsd	%xmm0, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm14, %xmm14
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm12, %xmm9, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vmulss	%xmm11, %xmm8, %xmm8
	vmovss	%xmm8, -4128(%rbp)
	vmulsd	%xmm3, %xmm2, %xmm8
	vaddsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm10, %xmm8
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm3, %xmm7, %xmm8
	vmovss	-108(%rax), %xmm3
	vsubss	-112(%rax), %xmm3, %xmm6
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm11, %xmm1, %xmm1
	vmovss	%xmm1, -4124(%rbp)
	vmulsd	%xmm13, %xmm4, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vsubsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vmovss	-100(%rax), %xmm4
	vsubss	-104(%rax), %xmm4, %xmm5
	vsubss	%xmm3, %xmm6, %xmm3
	vmulss	%xmm11, %xmm3, %xmm3
	vmovss	%xmm3, -4120(%rbp)
	vsubsd	%xmm2, %xmm0, %xmm3
	vmulsd	.LC36(%rip), %xmm7, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm7, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm11, %xmm0, %xmm0
	vmovss	%xmm0, -4116(%rbp)
	movq	-4128(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-4120(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	je	.L1193
.L1095:
	leaq	-32(%rax), %rsi
	cmpl	$1, %r14d
	ja	.L1194
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-32(%rax), %xmm8, %xmm8
	vcvtss2sd	-24(%rax), %xmm7, %xmm7
	vmulsd	.LC20(%rip), %xmm8, %xmm0
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-16(%rax), %xmm6, %xmm6
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	.LC21(%rip), %xmm7, %xmm1
	vcvtss2sd	-8(%rax), %xmm5, %xmm5
	vmulsd	.LC22(%rip), %xmm5, %xmm3
	vmulsd	%xmm15, %xmm6, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm1
	vmulsd	.LC20(%rip), %xmm6, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	-28(%rax), %xmm0
	vaddsd	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm9
	vmulsd	%xmm15, %xmm7, %xmm0
	vmulsd	.LC27(%rip), %xmm8, %xmm1
	vmovss	%xmm9, -8240(%rbp)
	vaddsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm0
	vmovss	-20(%rax), %xmm1
	vsubsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm4
	vmulsd	%xmm15, %xmm5, %xmm1
	vmulsd	.LC27(%rip), %xmm7, %xmm0
	vmovss	%xmm4, -8236(%rbp)
	vmovd	%xmm4, %edi
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	(%rax), %xmm4, %xmm4
	vmulsd	.LC22(%rip), %xmm4, %xmm13
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovss	-12(%rax), %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm3
	vmulsd	%xmm15, %xmm4, %xmm0
	vmulsd	.LC27(%rip), %xmm6, %xmm2
	vmovss	%xmm3, -8232(%rbp)
	vmovd	%xmm3, %r12d
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	8(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm14
	vaddsd	%xmm2, %xmm1, %xmm1
	vmovss	-4(%rax), %xmm2
	vaddsd	%xmm0, %xmm1, %xmm1
	vsubsd	%xmm14, %xmm1, %xmm1
	vmulsd	.LC27(%rip), %xmm5, %xmm14
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm15, %xmm3, %xmm1
	vaddsd	%xmm14, %xmm0, %xmm0
	vmovss	%xmm2, -8228(%rbp)
	vmovd	%xmm2, %r11d
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	16(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm14, %xmm0, %xmm0
	vmovss	4(%rax), %xmm14
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm14, %xmm0
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	24(%rax), %xmm14, %xmm14
	vmovss	%xmm0, -8224(%rbp)
	vmovd	%xmm0, %r10d
	vmulsd	.LC27(%rip), %xmm4, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm15, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm14, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	12(%rax), %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm12
	vmovss	%xmm12, -8220(%rbp)
	vmulsd	.LC20(%rip), %xmm3, %xmm0
	vmulsd	.LC30(%rip), %xmm2, %xmm1
	vsubsd	%xmm0, %xmm13, %xmm0
	vmulsd	.LC21(%rip), %xmm2, %xmm13
	vaddsd	%xmm13, %xmm0, %xmm13
	vmulsd	.LC20(%rip), %xmm14, %xmm0
	vaddsd	%xmm0, %xmm13, %xmm13
	vmovss	20(%rax), %xmm0
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vsubss	%xmm13, %xmm0, %xmm10
	vmulsd	.LC29(%rip), %xmm3, %xmm13
	vmulsd	.LC28(%rip), %xmm4, %xmm0
	vmovss	%xmm10, -8216(%rbp)
	vaddsd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC30(%rip), %xmm14, %xmm13
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	28(%rax), %xmm1
	vaddsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm13
	vmovss	-32(%rax), %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovss	-24(%rax), %xmm0
	vmovss	%xmm13, -8212(%rbp)
	vmovss	%xmm0, -4140(%rbp)
	vmovss	-16(%rax), %xmm0
	vmovss	%xmm0, -4136(%rbp)
	vmovss	-8(%rax), %xmm0
	vmovss	%xmm0, -4132(%rbp)
	vmovss	(%rax), %xmm0
	vmovss	%xmm0, -4128(%rbp)
	vmovss	8(%rax), %xmm0
	vmovss	%xmm0, -4124(%rbp)
	vmovss	16(%rax), %xmm0
	vmovss	%xmm0, -4120(%rbp)
	vmovss	24(%rax), %xmm0
	vmovss	%xmm0, -4116(%rbp)
	testb	%cl, %cl
	je	.L1094
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vxorps	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovd	%edi, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vmovd	%r12d, %xmm7
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4136(%rbp)
	vmovd	%r11d, %xmm6
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vmovd	%r10d, %xmm5
	vcvtss2sd	%xmm5, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm4, %xmm0, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm12, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm4, -4128(%rbp)
	vxorps	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm10, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -4124(%rbp)
	vxorps	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm2, %xmm4, %xmm4
	vmovss	%xmm4, -4120(%rbp)
	vaddsd	%xmm14, %xmm0, %xmm14
	vcvtsd2ss	%xmm14, %xmm3, %xmm3
	vmovss	%xmm3, -4116(%rbp)
.L1094:
	movq	-4144(%rbp), %rdi
	subq	$-128, %rax
	movq	%rdi, (%rsi)
	movq	-4136(%rbp), %rdi
	movq	%rdi, 8(%rsi)
	movq	-4128(%rbp), %rdi
	movq	%rdi, 16(%rsi)
	movq	-4120(%rbp), %rdi
	movq	%rdi, 24(%rsi)
	movq	-8240(%rbp), %rsi
	movq	%rsi, -128(%rax)
	movq	-8232(%rbp), %rsi
	movq	%rsi, -120(%rax)
	movq	-8224(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-8216(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	jne	.L1095
.L1193:
	addq	$4096, %rdx
	cmpq	%rdx, -8304(%rbp)
	jne	.L1091
	movq	-8264(%rbp), %rax
	movq	-8248(%rbp), %r12
	addq	$32788, %rax
	movq	%rax, -8256(%rbp)
.L1102:
	leaq	1024(%r12), %rbx
	movq	%r12, %r13
.L1097:
	movl	-8288(%rbp), %esi
	movq	%r13, %rdi
	subq	$-128, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%rbx, %r13
	jne	.L1097
	leaq	896(%r12), %rax
	movq	%r12, %rdx
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %edi
	movl	$2, %esi
	movl	$7, %ecx
.L1099:
	cmpl	$14, %ecx
	je	.L1101
	vmovss	-892(%rax), %xmm0
	vmovss	-768(%rax), %xmm1
	vmovss	%xmm0, -768(%rax)
	vmovss	%xmm1, -892(%rax)
	cmpl	$7, %esi
	ja	.L1101
	movslq	%esi, %r11
	vmovss	-640(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -640(%rax)
	cmpl	$7, %edi
	ja	.L1101
	movslq	%edi, %r11
	vmovss	-512(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -512(%rax)
	cmpl	$7, %r8d
	ja	.L1101
	movslq	%r8d, %r11
	vmovss	-384(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -384(%rax)
	cmpl	$7, %r9d
	ja	.L1101
	movslq	%r9d, %r11
	vmovss	-256(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -256(%rax)
	cmpl	$7, %r10d
	ja	.L1101
	movslq	%r10d, %r11
	vmovss	-128(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -128(%rax)
	cmpl	$7, %ecx
	jne	.L1101
	vmovss	28(%rdx), %xmm0
	vmovss	(%rax), %xmm1
	vmovss	%xmm1, 28(%rdx)
	vmovss	%xmm0, (%rax)
.L1101:
	addl	$1, %ecx
	subq	$-128, %rdx
	addq	$132, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	cmpl	$15, %ecx
	jne	.L1099
	movq	%r12, %r13
.L1100:
	movl	-8288(%rbp), %esi
	movq	%r13, %rdi
	subq	$-128, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%rbx, %r13
	jne	.L1100
	addq	$4096, %r12
	cmpq	-8256(%rbp), %r12
	jne	.L1102
	movq	-8264(%rbp), %rax
	xorl	%r14d, %r14d
	leaq	24(%rax), %r13
.L1106:
	movq	-8248(%rbp), %rdx
	movslq	%r14d, %rdi
	movq	%r13, %rax
	movl	$6, %ebx
	movq	%rdi, %rcx
	movl	$5, %r11d
	salq	$7, %rdi
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$7, %esi
	salq	$5, %rcx
.L1105:
	cmpl	$14, %esi
	je	.L1104
	vmovss	(%rax), %xmm0
	vmovss	4092(%rax), %xmm1
	vmovss	%xmm0, 4092(%rax)
	vmovss	%xmm1, (%rax)
	cmpl	$7, %r8d
	ja	.L1104
	movslq	%r8d, %r12
	vmovss	8188(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 8188(%rax)
	cmpl	$7, %r9d
	ja	.L1104
	movslq	%r9d, %r12
	vmovss	12284(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 12284(%rax)
	cmpl	$7, %r10d
	ja	.L1104
	movslq	%r10d, %r12
	vmovss	16380(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 16380(%rax)
	cmpl	$7, %r11d
	ja	.L1104
	movslq	%r11d, %r12
	vmovss	20476(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 20476(%rax)
	cmpl	$7, %ebx
	ja	.L1104
	movslq	%ebx, %r12
	vmovss	24572(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 24572(%rax)
	cmpl	$7, %esi
	jne	.L1104
	leaq	(%rdx,%rdi), %r12
	vmovss	28668(%rax), %xmm1
	vmovss	28(%r12), %xmm0
	vmovss	%xmm1, 28(%r12)
	vmovss	%xmm0, 28668(%rax)
.L1104:
	addl	$1, %esi
	addq	$4096, %rdx
	addq	$4100, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	cmpl	$15, %esi
	jne	.L1105
	addl	$1, %r14d
	subq	$-128, %r13
	cmpl	$8, %r14d
	jne	.L1106
	movq	-8320(%rbp), %rbx
.L1107:
	leaq	1024(%rbx), %r13
	movq	%rbx, %r12
.L1108:
	movl	-8288(%rbp), %esi
	movq	%r12, %rdi
	subq	$-128, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r12, %r13
	jne	.L1108
	addq	$4096, %rbx
	cmpq	-8256(%rbp), %rbx
	jne	.L1107
	movl	$4096, %edx
	xorl	%esi, %esi
	leaq	-8240(%rbp), %rdi
	movl	$7, %r13d
	movl	$6, %r12d
	call	memset
	movq	-8264(%rbp), %rax
	leaq	-8240(%rbp), %rsi
	movq	-8248(%rbp), %rcx
	vmovss	-8324(%rbp), %xmm0
	leaq	135200(%rax), %rbx
	leaq	16(%rax), %rdi
	movq	%rbx, %rdx
	movq	%rbx, -8256(%rbp)
	movq	%rax, %rbx
	call	_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf
	leaq	-8240(%rbp), %rsi
	movl	$4096, %edx
	leaq	-4144(%rbp), %rdi
	movl	%eax, -8248(%rbp)
	call	memcpy
	leaq	131104(%rbx), %r14
	xorl	%edi, %edi
	movl	$5, %ebx
	movl	$4, %r11d
	movl	$3, %r10d
	movl	$1, %esi
	movl	$2, %r9d
	movl	$1, %r8d
.L1110:
	movq	%r8, %rax
	movl	%r8d, %ecx
	movq	%rsi, %rdx
	shrq	$6, %rax
	salq	%cl, %rdx
	movl	%r9d, %ecx
	testq	%rdx, -4144(%rbp,%rax,8)
	setne	%al
	movzbl	%al, %eax
	leal	(%rax,%rax), %edx
	movq	%rsi, %rax
	salq	%cl, %rax
	movq	%rax, %rcx
	movq	%r9, %rax
	shrq	$6, %rax
	testq	%rcx, -4144(%rbp,%rax,8)
	movl	%edi, %ecx
	setne	%al
	movzbl	%al, %eax
	sall	$2, %eax
	orl	%edx, %eax
	movq	%rsi, %rdx
	salq	%cl, %rdx
	movq	%rdx, %rcx
	movq	%rdi, %rdx
	shrq	$6, %rdx
	testq	%rcx, -4144(%rbp,%rdx,8)
	movl	%r10d, %ecx
	setne	%dl
	movzbl	%dl, %edx
	orl	%edx, %eax
	movq	%rsi, %rdx
	salq	%cl, %rdx
	movq	%rdx, %rcx
	movq	%r10, %rdx
	shrq	$6, %rdx
	testq	%rcx, -4144(%rbp,%rdx,8)
	movl	%r11d, %ecx
	setne	%dl
	movzbl	%dl, %edx
	sall	$3, %edx
	orl	%edx, %eax
	movq	%rsi, %rdx
	salq	%cl, %rdx
	movq	%rdx, %rcx
	movq	%r11, %rdx
	shrq	$6, %rdx
	testq	%rcx, -4144(%rbp,%rdx,8)
	movl	%ebx, %ecx
	setne	%dl
	movzbl	%dl, %edx
	sall	$4, %edx
	orl	%eax, %edx
	movq	%rsi, %rax
	salq	%cl, %rax
	movq	%rax, %rcx
	movq	%rbx, %rax
	shrq	$6, %rax
	testq	%rcx, -4144(%rbp,%rax,8)
	movl	%r12d, %ecx
	setne	%al
	movzbl	%al, %eax
	sall	$5, %eax
	orl	%edx, %eax
	movq	%rsi, %rdx
	salq	%cl, %rdx
	movq	%rdx, %rcx
	movq	%r12, %rdx
	shrq	$6, %rdx
	testq	%rcx, -4144(%rbp,%rdx,8)
	movl	%r13d, %ecx
	setne	%dl
	movzbl	%dl, %edx
	sall	$6, %edx
	orl	%eax, %edx
	movq	%rsi, %rax
	salq	%cl, %rax
	movq	%rax, %rcx
	movq	%r13, %rax
	shrq	$6, %rax
	testq	%rcx, -4144(%rbp,%rax,8)
	setne	%al
	addq	$8, %rdi
	addq	$8, %r8
	movzbl	%al, %eax
	addq	$8, %r9
	addq	$8, %r10
	sall	$7, %eax
	addq	$8, %r11
	addq	$8, %rbx
	orl	%edx, %eax
	addq	$8, %r12
	addq	$8, %r13
	movb	%al, (%r14)
	addq	$1, %r14
	cmpq	$32768, %rdi
	jne	.L1110
	cmpb	$0, -8328(%rbp)
	jne	.L1195
	cmpb	$0, -8284(%rbp)
	je	.L1118
	movl	-8248(%rbp), %eax
	testl	%eax, %eax
	jle	.L1122
.L1119:
	movq	-8256(%rbp), %rcx
	xorl	%esi, %esi
	movl	$31, %r8d
	xorl	%edi, %edi
.L1121:
	movl	(%rcx,%rsi,4), %eax
	movl	%eax, %edx
	andl	$2147483647, %edx
	shrl	$23, %edx
	subl	$112, %edx
	cmpl	$31, %edx
	cmovg	%r8d, %edx
	movl	%edx, %r9d
	sall	$10, %r9d
	testl	%edx, %edx
	movl	%eax, %edx
	cmovle	%edi, %r9d
	andl	$-2147483648, %edx
	andl	$8388607, %eax
	shrl	$16, %edx
	orl	%r9d, %edx
	shrl	$13, %eax
	orl	%edx, %eax
	movw	%ax, (%rcx,%rsi,2)
	addq	$1, %rsi
	cmpl	%esi, -8248(%rbp)
	jg	.L1121
.L1122:
	movslq	-8248(%rbp), %rax
	leaq	4096(%rax,%rax), %rax
.L1177:
	addq	$8288, %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L1195:
	.cfi_restore_state
	movl	-8248(%rbp), %eax
	testl	%eax, %eax
	jle	.L1112
	leal	-32(%rax), %ecx
	subl	$1, %eax
	shrl	$5, %ecx
	addl	$1, %ecx
	movl	%ecx, %edx
	sall	$5, %edx
	cmpl	$30, %eax
	jbe	.L1127
	movq	-8256(%rbp), %rax
	vmovdqa	.LC54(%rip), %ymm7
	vmovdqa	.LC55(%rip), %ymm6
	vmovdqa	.LC56(%rip), %ymm5
	vmovdqa	.LC57(%rip), %ymm4
.L1114:
	vmovdqu	(%rax), %ymm1
	addl	$1, %r15d
	subq	$-128, %rax
	vmovdqu	-96(%rax), %ymm9
	vmovdqu	-64(%rax), %ymm0
	vpshufb	%ymm7, %ymm1, %ymm2
	vpshufb	%ymm5, %ymm1, %ymm1
	vmovdqu	-32(%rax), %ymm8
	vpshufb	%ymm6, %ymm9, %ymm3
	vpshufb	%ymm4, %ymm9, %ymm9
	vpor	%ymm3, %ymm2, %ymm3
	vpor	%ymm9, %ymm1, %ymm1
	vpermq	$216, %ymm3, %ymm3
	vpermq	$216, %ymm1, %ymm1
	vpshufb	%ymm7, %ymm0, %ymm2
	vpshufb	%ymm6, %ymm8, %ymm9
	vpshufb	%ymm5, %ymm0, %ymm0
	vpor	%ymm9, %ymm2, %ymm2
	vpshufb	%ymm4, %ymm8, %ymm8
	vpermq	$216, %ymm2, %ymm2
	vpor	%ymm8, %ymm0, %ymm0
	vpshufb	%ymm6, %ymm2, %ymm9
	vpermq	$216, %ymm0, %ymm0
	vpshufb	%ymm7, %ymm3, %ymm8
	vpshufb	%ymm4, %ymm2, %ymm2
	vpor	%ymm9, %ymm8, %ymm8
	vpshufb	%ymm5, %ymm3, %ymm3
	vpermq	$216, %ymm8, %ymm8
	vpor	%ymm2, %ymm3, %ymm2
	vpshufb	%ymm6, %ymm0, %ymm9
	vpermq	$216, %ymm2, %ymm2
	vpshufb	%ymm7, %ymm1, %ymm3
	vpshufb	%ymm4, %ymm0, %ymm0
	vpshufb	%ymm5, %ymm1, %ymm1
	vpor	%ymm9, %ymm3, %ymm3
	vpor	%ymm0, %ymm1, %ymm0
	vpermq	$216, %ymm3, %ymm3
	vpermq	$216, %ymm0, %ymm0
	vpunpcklbw	%ymm3, %ymm0, %ymm1
	vpunpckhbw	%ymm3, %ymm0, %ymm0
	vperm2i128	$32, %ymm0, %ymm1, %ymm3
	vperm2i128	$49, %ymm0, %ymm1, %ymm0
	vpunpcklbw	%ymm8, %ymm2, %ymm1
	vpunpckhbw	%ymm8, %ymm2, %ymm8
	vperm2i128	$32, %ymm8, %ymm1, %ymm2
	vperm2i128	$49, %ymm8, %ymm1, %ymm1
	vpunpcklbw	%ymm2, %ymm3, %ymm8
	vpunpckhbw	%ymm2, %ymm3, %ymm2
	vperm2i128	$32, %ymm2, %ymm8, %ymm3
	vperm2i128	$49, %ymm2, %ymm8, %ymm2
	vmovdqu	%ymm3, -128(%rax)
	vmovdqu	%ymm2, -96(%rax)
	vpunpcklbw	%ymm1, %ymm0, %ymm2
	vpunpckhbw	%ymm1, %ymm0, %ymm0
	vperm2i128	$32, %ymm0, %ymm2, %ymm1
	vperm2i128	$49, %ymm0, %ymm2, %ymm0
	vmovdqu	%ymm1, -64(%rax)
	vmovdqu	%ymm0, -32(%rax)
	cmpl	%ecx, %r15d
	jb	.L1114
	cmpl	-8248(%rbp), %edx
	je	.L1117
.L1113:
	leal	0(,%rdx,4), %eax
	cltq
	addq	-8256(%rbp), %rax
.L1116:
	movzbl	1(%rax), %edi
	addl	$1, %edx
	addq	$4, %rax
	movzbl	-2(%rax), %esi
	movzbl	-1(%rax), %ecx
	movzbl	-4(%rax), %r8d
	movb	%dil, -2(%rax)
	movb	%sil, -3(%rax)
	movb	%cl, -4(%rax)
	movb	%r8b, -1(%rax)
	cmpl	%edx, -8248(%rbp)
	jg	.L1116
.L1117:
	cmpb	$0, -8284(%rbp)
	jne	.L1119
.L1118:
	movslq	-8248(%rbp), %rax
	leaq	4096(,%rax,4), %rax
	jmp	.L1177
.L1127:
	xorl	%edx, %edx
	jmp	.L1113
.L1112:
	cmpb	$0, -8284(%rbp)
	jne	.L1122
	jmp	.L1118
	.cfi_endproc
.LFE1382:
	.size	_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi, .-_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,comdat
.LCOLDE64:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi,comdat
.LHOTE64:
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,comdat
	.align 2
.LCOLDB65:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,comdat
.LHOTB65:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi
	.type	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi, @function
_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi:
.LFB1389:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movzbl	%dl, %edx
	movzbl	%sil, %esi
	movq	%rdi, %rbx
	subq	$112, %rsp
	.cfi_def_cfa_offset 128
	call	_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi
	movq	%rsp, %rdi
	movl	$14, %ecx
	movl	$-1, %esi
	movq	%rax, %rdx
	xorl	%eax, %eax
	rep; stosq
	leaq	131104(%rbx), %rax
	movl	%edx, 8(%rsp)
	addq	$266280, %rbx
	movl	$.LC59, %edx
	movq	%rsp, %rdi
	movq	%rax, (%rsp)
	movl	$135168, 32(%rsp)
	movq	%rbx, 24(%rsp)
	movb	$112, %cl
	call	deflateInit_
	testl	%eax, %eax
	je	.L1197
.L1198:
	movl	$.LC60, %edi
	call	puts
	call	abort
	.p2align 4,,10
	.p2align 3
.L1197:
	movl	$4, %esi
	movq	%rsp, %rdi
	call	deflate
	cmpl	$1, %eax
	jne	.L1198
	movq	40(%rsp), %rbx
	movq	%rsp, %rdi
	call	deflateEnd
	addq	$112, %rsp
	.cfi_def_cfa_offset 16
	movslq	%ebx, %rax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1389:
	.size	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi, .-_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,comdat
.LCOLDE65:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi,comdat
.LHOTE65:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbi,comdat
	.align 2
.LCOLDB66:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbi,comdat
.LHOTB66:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE8compressEfbi
	.type	_ZN24WaveletCompressorGenericILi32EfE8compressEfbi, @function
_ZN24WaveletCompressorGenericILi32EfE8compressEfbi:
.LFB1381:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	movq	%rdi, %rax
	pushq	-8(%r10)
	pushq	%rbp
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r15
	.cfi_escape 0x10,0xf,0x2,0x76,0x78
	leal	-1(%rdx), %r15d
	pushq	%r14
	.cfi_escape 0x10,0xe,0x2,0x76,0x70
	leaq	131092(%rax), %r14
	pushq	%r13
	.cfi_escape 0x10,0xd,0x2,0x76,0x68
	leaq	84(%rax), %r13
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x58,0x6
	.cfi_escape 0x10,0xc,0x2,0x76,0x60
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x50
	leaq	20(%rax), %rbx
	subq	$8288, %rsp
	cmpl	$2, %edx
	movq	%rdi, -8272(%rbp)
	vmovapd	.LC38(%rip), %ymm14
	sete	-8249(%rbp)
	movzbl	-8249(%rbp), %edi
	movq	%r14, -8264(%rbp)
	movq	%r13, %r14
	vmovapd	.LC39(%rip), %ymm15
	movq	%rbx, %r13
	vmovss	%xmm0, -8316(%rbp)
	movl	%esi, -8320(%rbp)
	movl	%edx, -8256(%rbp)
	movl	%edi, %r12d
	movq	%rbx, -8248(%rbp)
	movq	%rbx, -8312(%rbp)
.L1211:
	movq	%r14, %rdx
	movq	%r13, %rax
	movl	$32, %esi
	jmp	.L1204
	.p2align 4,,10
	.p2align 3
.L1319:
	vmovss	24(%rax), %xmm1
	subq	$-128, %rdx
	subq	$-128, %rax
	vaddss	-100(%rax), %xmm1, %xmm1
	vmovss	.LC37(%rip), %xmm0
	vmovss	-128(%rax), %xmm10
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	-124(%rax), %xmm8
	vmovss	-120(%rax), %xmm9
	vaddss	%xmm8, %xmm10, %xmm6
	vmovss	-112(%rax), %xmm7
	vaddss	-116(%rax), %xmm9, %xmm9
	vmovss	%xmm1, -8228(%rbp)
	vmovss	-96(%rax), %xmm1
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	-92(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm6, %xmm6
	vaddss	-108(%rax), %xmm7, %xmm7
	vmulss	%xmm0, %xmm9, %xmm9
	vmovss	-48(%rax), %xmm4
	vmulss	%xmm0, %xmm1, %xmm1
	vaddss	-44(%rax), %xmm4, %xmm4
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm6, -8240(%rbp)
	vcvtss2sd	%xmm6, %xmm6, %xmm6
	vmovss	%xmm9, -8236(%rbp)
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vmulss	%xmm0, %xmm4, %xmm5
	vmovss	%xmm1, -8224(%rbp)
	vmovss	-88(%rax), %xmm1
	vaddss	-84(%rax), %xmm1, %xmm1
	vmovss	%xmm7, -8232(%rbp)
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmovd	%xmm5, %r11d
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8220(%rbp)
	vmovss	-80(%rax), %xmm1
	vaddss	-76(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8216(%rbp)
	vmovss	-72(%rax), %xmm1
	vaddss	-68(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8212(%rbp)
	vmovss	-128(%rdx), %xmm1
	vaddss	-60(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8208(%rbp)
	vmovss	-56(%rax), %xmm1
	vaddss	-52(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8204(%rbp)
	vmovss	%xmm5, -8200(%rbp)
	vmovss	-40(%rax), %xmm3
	vaddss	-36(%rax), %xmm3, %xmm3
	vmovss	-32(%rax), %xmm2
	vaddss	-28(%rax), %xmm2, %xmm2
	vmulss	%xmm0, %xmm3, %xmm5
	vmovss	-24(%rax), %xmm3
	vaddss	-20(%rax), %xmm3, %xmm3
	vmovss	-16(%rax), %xmm1
	vaddss	-12(%rax), %xmm1, %xmm1
	vmovd	%xmm5, %r10d
	vmovups	-88(%rax), %ymm12
	vmovss	%xmm5, -8196(%rbp)
	vmulss	%xmm0, %xmm2, %xmm5
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vmulsd	.LC35(%rip), %xmm6, %xmm10
	vmulsd	.LC36(%rip), %xmm6, %xmm6
	vmovd	%xmm5, %r8d
	vmovss	%xmm5, -8192(%rbp)
	vmulss	%xmm0, %xmm3, %xmm5
	vaddsd	%xmm9, %xmm10, %xmm10
	vsubsd	%xmm9, %xmm6, %xmm6
	vmovaps	-8240(%rbp), %ymm9
	vmovd	%xmm5, %edi
	vmovss	%xmm5, -8188(%rbp)
	vmulss	%xmm0, %xmm1, %xmm5
	vcvtps2pd	%xmm9, %ymm3
	vextractf128	$0x1, %ymm9, %xmm9
	vmovd	%xmm5, %ecx
	vmovss	%xmm5, -8184(%rbp)
	vmovss	-8(%rax), %xmm5
	vaddss	-4(%rax), %xmm5, %xmm1
	vmulss	%xmm0, %xmm1, %xmm5
	vmovsd	.LC34(%rip), %xmm1
	vmulsd	%xmm1, %xmm7, %xmm7
	vmovss	%xmm5, -8180(%rbp)
	vsubsd	%xmm7, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovups	-120(%rax), %ymm7
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm10, %xmm2
	vmovups	-8232(%rbp), %ymm10
	vmovups	-8236(%rbp), %ymm6
	vsubss	%xmm2, %xmm8, %xmm8
	vcvtps2pd	%xmm6, %ymm11
	vcvtps2pd	%xmm9, %ymm2
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm10, %ymm9
	vcvtps2pd	%xmm6, %ymm6
	vextractf128	$0x1, %ymm10, %xmm10
	vcvtps2pd	%xmm10, %ymm10
	vmulpd	%ymm14, %ymm10, %ymm4
	vshufps	$221, %ymm12, %ymm7, %ymm10
	vshufps	$136, %ymm12, %ymm7, %ymm7
	vperm2f128	$3, %ymm7, %ymm7, %ymm12
	vmulss	%xmm0, %xmm8, %xmm8
	vmulpd	%ymm14, %ymm9, %ymm9
	vmovss	%xmm8, -4144(%rbp)
	vperm2f128	$3, %ymm10, %ymm10, %ymm8
	vshufps	$68, %ymm8, %ymm10, %ymm13
	vshufps	$238, %ymm8, %ymm10, %ymm8
	vshufps	$68, %ymm12, %ymm7, %ymm10
	vinsertf128	$1, %xmm8, %ymm13, %ymm8
	vshufps	$238, %ymm12, %ymm7, %ymm12
	vmulpd	%ymm15, %ymm2, %ymm7
	vinsertf128	$1, %xmm12, %ymm10, %ymm10
	vaddpd	%ymm6, %ymm7, %ymm7
	vsubps	%ymm10, %ymm8, %ymm8
	vmulpd	%ymm15, %ymm3, %ymm10
	vaddpd	%ymm11, %ymm10, %ymm10
	vaddpd	%ymm4, %ymm7, %ymm7
	vaddpd	%ymm9, %ymm10, %ymm10
	vcvtpd2psy	%ymm7, %xmm7
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm7, %ymm10, %ymm7
	vmulpd	%ymm14, %ymm3, %ymm10
	vaddpd	%ymm10, %ymm11, %ymm11
	vmovd	%r11d, %xmm3
	vsubpd	%ymm9, %ymm11, %ymm11
	vmulpd	%ymm14, %ymm2, %ymm9
	vaddpd	%ymm9, %ymm6, %ymm6
	vcvtpd2psy	%ymm11, %xmm11
	vsubpd	%ymm4, %ymm6, %ymm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm4, %xmm9
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-8204(%rbp), %xmm6, %xmm6
	vmulsd	%xmm1, %xmm6, %xmm12
	vmovd	%r10d, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm11, %ymm11
	vsubps	%ymm11, %ymm7, %ymm11
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8208(%rbp), %xmm7, %xmm7
	vsubps	%ymm11, %ymm8, %ymm11
	vmovss	-52(%rax), %xmm8
	vmulps	.LC32(%rip), %ymm11, %ymm11
	vmovups	%ymm11, -4140(%rbp)
	vsubss	-56(%rax), %xmm8, %xmm2
	vmulsd	%xmm1, %xmm7, %xmm11
	vmovsd	.LC40(%rip), %xmm8
	vaddsd	%xmm4, %xmm12, %xmm12
	vmulsd	%xmm8, %xmm7, %xmm10
	vaddsd	%xmm11, %xmm6, %xmm11
	vaddsd	%xmm6, %xmm10, %xmm10
	vsubsd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm9, %xmm10, %xmm10
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm9, %xmm3, %xmm9
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm10, %xmm2, %xmm7
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vmovss	-44(%rax), %xmm10
	vmulsd	%xmm1, %xmm3, %xmm7
	vsubss	-48(%rax), %xmm10, %xmm2
	vmulsd	%xmm8, %xmm6, %xmm10
	vmovd	%r8d, %xmm6
	vsubsd	%xmm7, %xmm12, %xmm12
	vaddsd	%xmm4, %xmm10, %xmm10
	vmulsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vaddsd	%xmm7, %xmm10, %xmm10
	vaddsd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm8, %xmm3, %xmm3
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm12, %xmm10, %xmm11
	vsubss	%xmm11, %xmm2, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vmulsd	%xmm1, %xmm2, %xmm6
	vaddsd	%xmm7, %xmm2, %xmm11
	vaddsd	%xmm2, %xmm3, %xmm3
	vmulsd	%xmm8, %xmm2, %xmm2
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm10, -4104(%rbp)
	vmovss	-36(%rax), %xmm10
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	-40(%rax), %xmm10, %xmm10
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm4, %xmm10, %xmm10
	vmovd	%edi, %xmm4
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm4, %xmm9
	vaddsd	%xmm6, %xmm4, %xmm6
	vaddsd	%xmm4, %xmm2, %xmm2
	vmulsd	%xmm8, %xmm4, %xmm4
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm9, %xmm3, %xmm3
	vsubsd	%xmm9, %xmm11, %xmm11
	vmovss	%xmm10, -4100(%rbp)
	vmovss	-28(%rax), %xmm10
	vsubss	-32(%rax), %xmm10, %xmm10
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm11, %xmm3, %xmm7
	vmovd	%ecx, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm9, %xmm3, %xmm9
	vsubss	%xmm7, %xmm10, %xmm10
	vmovss	-20(%rax), %xmm7
	vsubss	-24(%rax), %xmm7, %xmm7
	vmulss	%xmm0, %xmm10, %xmm10
	vmovss	%xmm10, -4096(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm10
	vmovd	%edi, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm10, %xmm2, %xmm2
	vsubsd	%xmm10, %xmm6, %xmm10
	vmovd	%ecx, %xmm6
	movq	-8240(%rbp), %rcx
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	movq	%rcx, -128(%rax)
	movq	-8232(%rbp), %rcx
	vsubss	%xmm2, %xmm7, %xmm7
	vmovss	-12(%rax), %xmm2
	vsubss	-16(%rax), %xmm2, %xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	movq	%rcx, -120(%rax)
	movq	-8224(%rbp), %rcx
	vmulss	%xmm0, %xmm7, %xmm7
	movq	%rcx, -112(%rax)
	movq	-8216(%rbp), %rcx
	vmovss	%xmm7, -4092(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm1, %xmm7, %xmm7
	movq	%rcx, -104(%rax)
	movq	-8208(%rbp), %rcx
	vaddsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm7, %xmm9, %xmm9
	movq	%rcx, -96(%rax)
	movq	-8200(%rbp), %rcx
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm2, %xmm4, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm6, %xmm4, %xmm4
	vmulsd	.LC33(%rip), %xmm4, %xmm4
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	%xmm5, %xmm6, %xmm6
	vmovss	-4(%rax), %xmm5
	movq	%rcx, -88(%rax)
	vsubss	%xmm2, %xmm10, %xmm2
	vsubss	-8(%rax), %xmm5, %xmm5
	vmulss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, -4088(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm2
	vmulsd	.LC36(%rip), %xmm6, %xmm1
	vsubsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm8, %xmm3, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm4, %xmm4
	vmulsd	.LC35(%rip), %xmm6, %xmm1
	vaddsd	%xmm1, %xmm4, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -4084(%rbp)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-4144(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	je	.L1318
.L1204:
	cmpl	$1, %r15d
	ja	.L1319
	vmovss	(%rax), %xmm7
	vxorpd	%xmm12, %xmm12, %xmm12
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovss	8(%rax), %xmm6
	vcvtss2sd	16(%rax), %xmm10, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	24(%rax), %xmm9, %xmm9
	vcvtss2sd	%xmm7, %xmm12, %xmm12
	vmovups	40(%rax), %ymm8
	vcvtss2sd	%xmm6, %xmm11, %xmm11
	vmovups	8(%rax), %ymm4
	vmulsd	.LC21(%rip), %xmm11, %xmm1
	vmulsd	.LC20(%rip), %xmm12, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm10, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm9, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vshufps	$136, %ymm8, %ymm4, %ymm1
	vshufps	$221, %ymm8, %ymm4, %ymm4
	vmovss	%xmm0, -8240(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm3
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm3
	vmovups	24(%rax), %ymm0
	vshufps	$136, 56(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	16(%rax), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 48(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm5
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vmovups	(%rax), %ymm5
	vshufps	$136, 32(%rax), %ymm5, %ymm5
	vperm2f128	$3, %ymm5, %ymm5, %ymm0
	vshufps	$68, %ymm0, %ymm5, %ymm13
	vshufps	$238, %ymm0, %ymm5, %ymm0
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vinsertf128	$1, %xmm0, %ymm13, %ymm0
	vshufps	$68, %ymm5, %ymm4, %ymm8
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm0, %ymm4
	vinsertf128	$1, %xmm5, %ymm8, %ymm8
	vmulpd	.LC23(%rip), %ymm4, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm3, %ymm4
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm4, %ymm5, %ymm4
	vmulpd	.LC23(%rip), %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vaddpd	%ymm3, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	72(%rax), %xmm3, %xmm3
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm1, %ymm4
	vmulpd	.LC25(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vaddpd	%ymm2, %ymm0, %ymm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	80(%rax), %xmm2, %xmm2
	vsubpd	%ymm4, %ymm5, %ymm4
	vmulsd	.LC26(%rip), %xmm2, %xmm5
	vsubpd	%ymm1, %ymm0, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	88(%rax), %xmm1, %xmm1
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vsubps	%ymm0, %ymm8, %ymm0
	vmovsd	.LC27(%rip), %xmm4
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovups	%ymm0, -8236(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rdx), %xmm0, %xmm0
	vmulsd	.LC26(%rip), %xmm3, %xmm8
	vmulsd	%xmm4, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm1, %xmm13
	vmulsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm0, %xmm0
	vmovss	76(%rax), %xmm8
	vaddsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	84(%rax), %xmm5
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm8, %xmm0
	vmulsd	.LC26(%rip), %xmm1, %xmm8
	vmulsd	%xmm4, %xmm1, %xmm1
	vmovss	%xmm0, -8204(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	96(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm13
	vaddsd	%xmm8, %xmm3, %xmm3
	vaddsd	%xmm8, %xmm2, %xmm2
	vmovss	92(%rax), %xmm8
	vsubsd	%xmm13, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vmulsd	.LC26(%rip), %xmm0, %xmm5
	vmulsd	%xmm4, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	96(%rax), %xmm4, %xmm4
	vmovss	%xmm3, -8200(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm13
	vmulsd	.LC26(%rip), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm0, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm8, %xmm2
	vmulsd	.LC20(%rip), %xmm3, %xmm8
	vmovss	%xmm2, -8196(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm1
	vmulsd	.LC26(%rip), %xmm2, %xmm2
	vsubsd	%xmm1, %xmm5, %xmm1
	vmovss	100(%rax), %xmm5
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmovss	%xmm1, -8192(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	108(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmovss	%xmm0, -8188(%rbp)
	vmulsd	.LC22(%rip), %xmm4, %xmm0
	vsubsd	%xmm8, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmulsd	.LC20(%rip), %xmm1, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	116(%rax), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -8184(%rbp)
	vmovd	%xmm5, %ecx
	movl	32(%rax), %r11d
	vmovss	124(%rax), %xmm5
	vmovss	%xmm6, -4140(%rbp)
	vmovss	96(%rax), %xmm6
	vmovss	%xmm7, -4144(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm13
	vmulsd	.LC28(%rip), %xmm4, %xmm8
	vmovss	%xmm6, -4096(%rbp)
	movl	40(%rax), %r10d
	movl	%r11d, -4128(%rbp)
	vmovss	104(%rax), %xmm6
	movl	48(%rax), %r9d
	movl	56(%rax), %r8d
	vmovss	%xmm6, -4092(%rbp)
	vaddsd	%xmm13, %xmm8, %xmm8
	movl	(%rdx), %edi
	movl	%r10d, -4124(%rbp)
	vmovsd	.LC30(%rip), %xmm13
	vmovss	112(%rax), %xmm6
	movl	%r9d, -4120(%rbp)
	vmulsd	%xmm13, %xmm2, %xmm0
	movl	%r8d, -4116(%rbp)
	vmulsd	%xmm13, %xmm1, %xmm13
	movl	%edi, -4112(%rbp)
	vmovss	%xmm6, -4088(%rbp)
	vsubsd	%xmm0, %xmm8, %xmm8
	vmovss	88(%rax), %xmm0
	vmovss	%xmm0, -4100(%rbp)
	vaddsd	%xmm13, %xmm8, %xmm8
	vmovss	72(%rax), %xmm13
	vmovss	%xmm13, -4108(%rbp)
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm5, %xmm8
	vmovss	16(%rax), %xmm5
	vmovss	%xmm5, -4136(%rbp)
	vmovss	24(%rax), %xmm5
	vmovss	%xmm8, -8180(%rbp)
	vmovss	%xmm5, -4132(%rbp)
	vmovss	80(%rax), %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vmovss	120(%rax), %xmm6
	vmovss	%xmm6, -4084(%rbp)
	testb	%r12b, %r12b
	je	.L1203
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8240(%rbp), %xmm7, %xmm7
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm8, %xmm8, %xmm8
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm12, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4144(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8236(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm11, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8232(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm10, %xmm10
	vxorps	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8228(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4136(%rbp)
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8224(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4132(%rbp)
	vmovd	%r11d, %xmm7
	vcvtss2sd	%xmm7, %xmm9, %xmm9
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8220(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm7, -4128(%rbp)
	vmovd	%r10d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8216(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4124(%rbp)
	vmovd	%r9d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8212(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4120(%rbp)
	vmovd	%r8d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8208(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4116(%rbp)
	vmovd	%edi, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4112(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8204(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm13, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8200(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8196(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8192(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmovss	%xmm5, -4100(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8188(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm5, %xmm5
	vmovss	%xmm5, -4096(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmovss	%xmm5, -4092(%rbp)
	vmovd	%ecx, %xmm5
	vcvtss2sd	%xmm5, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vmulsd	%xmm6, %xmm8, %xmm6
	vaddsd	%xmm0, %xmm2, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm6
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -4088(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -4084(%rbp)
.L1203:
	movq	-4144(%rbp), %rcx
	subq	$-128, %rax
	subq	$-128, %rdx
	movq	%rcx, -128(%rax)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rax)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rax)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rax)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rax)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-8240(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-8232(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-8224(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-8216(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-8208(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	jne	.L1204
.L1318:
	leaq	128(%r13), %r8
	xorl	%edi, %edi
	movq	%r13, %rsi
	addl	$1, %edi
	cmpl	$32, %edi
	je	.L1264
	.p2align 4,,10
	.p2align 3
.L1320:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L1206:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	subq	$-128, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -128(%rdx)
	cmpl	$32, %eax
	jne	.L1206
	addl	$1, %edi
	addq	$132, %r8
	subq	$-128, %rsi
	cmpl	$32, %edi
	jne	.L1320
.L1264:
	movq	%r14, %rdx
	movq	%r13, %rax
	movl	$32, %esi
	vmovapd	.LC38(%rip), %ymm12
	jmp	.L1205
	.p2align 4,,10
	.p2align 3
.L1322:
	vmovss	24(%rax), %xmm1
	subq	$-128, %rdx
	subq	$-128, %rax
	vaddss	-100(%rax), %xmm1, %xmm1
	vmovss	.LC37(%rip), %xmm0
	vmovss	-124(%rax), %xmm8
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	-128(%rax), %xmm9
	vmovss	-120(%rax), %xmm7
	vaddss	%xmm8, %xmm9, %xmm5
	vmovss	-112(%rax), %xmm6
	vaddss	-116(%rax), %xmm7, %xmm7
	vmovss	%xmm1, -8228(%rbp)
	vmovss	-96(%rax), %xmm1
	vsubss	%xmm9, %xmm8, %xmm9
	vaddss	-92(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm5, %xmm5
	vaddss	-108(%rax), %xmm6, %xmm6
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	-48(%rax), %xmm4
	vmulss	%xmm0, %xmm1, %xmm1
	vaddss	-44(%rax), %xmm4, %xmm4
	vmulss	%xmm0, %xmm6, %xmm6
	vmovss	%xmm5, -8240(%rbp)
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmovss	%xmm7, -8236(%rbp)
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmulss	%xmm0, %xmm4, %xmm3
	vmovss	%xmm1, -8224(%rbp)
	vmovss	-88(%rax), %xmm1
	vaddss	-84(%rax), %xmm1, %xmm1
	vmovss	%xmm6, -8232(%rbp)
	vcvtss2sd	%xmm6, %xmm6, %xmm6
	vmovd	%xmm3, %r11d
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8220(%rbp)
	vmovss	-80(%rax), %xmm1
	vaddss	-76(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8216(%rbp)
	vmovss	-72(%rax), %xmm1
	vaddss	-68(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8212(%rbp)
	vmovss	-128(%rdx), %xmm1
	vaddss	-60(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8208(%rbp)
	vmovss	-56(%rax), %xmm1
	vaddss	-52(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8204(%rbp)
	vmovss	%xmm3, -8200(%rbp)
	vmovss	-40(%rax), %xmm3
	vaddss	-36(%rax), %xmm3, %xmm3
	vmovss	-32(%rax), %xmm1
	vaddss	-28(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm3, %xmm3
	vmovups	-88(%rax), %ymm11
	vmulsd	.LC33(%rip), %xmm7, %xmm7
	vmulsd	.LC35(%rip), %xmm5, %xmm8
	vmulsd	.LC36(%rip), %xmm5, %xmm5
	vmovd	%xmm3, %r10d
	vmovss	%xmm3, -8196(%rbp)
	vmovsd	.LC34(%rip), %xmm2
	vmulss	%xmm0, %xmm1, %xmm3
	vmovss	-24(%rax), %xmm1
	vaddss	-20(%rax), %xmm1, %xmm1
	vmulsd	%xmm2, %xmm6, %xmm6
	vaddsd	%xmm7, %xmm8, %xmm8
	vmovups	-8232(%rbp), %ymm10
	vsubsd	%xmm7, %xmm5, %xmm7
	vmovups	-8236(%rbp), %ymm5
	vmovd	%xmm3, %r9d
	vmovss	%xmm3, -8192(%rbp)
	vmulss	%xmm0, %xmm1, %xmm3
	vmovss	-16(%rax), %xmm1
	vcvtps2pd	%xmm5, %ymm4
	vextractf128	$0x1, %ymm5, %xmm5
	vaddss	-12(%rax), %xmm1, %xmm1
	vcvtps2pd	%xmm5, %ymm5
	vsubsd	%xmm6, %xmm8, %xmm8
	vaddsd	%xmm7, %xmm6, %xmm7
	vmovups	-120(%rax), %ymm6
	vmovd	%xmm3, %edi
	vmovss	%xmm3, -8188(%rbp)
	vmulss	%xmm0, %xmm1, %xmm3
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovd	%xmm3, %ecx
	vmovss	%xmm3, -8184(%rbp)
	vmovss	-8(%rax), %xmm3
	vaddss	-4(%rax), %xmm3, %xmm1
	vmulss	%xmm0, %xmm1, %xmm3
	vsubss	%xmm7, %xmm8, %xmm1
	vsubss	%xmm1, %xmm9, %xmm8
	vmovd	%xmm3, %r8d
	vmovss	%xmm3, -8180(%rbp)
	vmulss	%xmm0, %xmm8, %xmm8
	vmovss	%xmm8, -4144(%rbp)
	vmovaps	-8240(%rbp), %ymm8
	vcvtps2pd	%xmm8, %ymm9
	vextractf128	$0x1, %ymm8, %xmm8
	vcvtps2pd	%xmm8, %ymm1
	vcvtps2pd	%xmm10, %ymm8
	vextractf128	$0x1, %ymm10, %xmm10
	vcvtps2pd	%xmm10, %ymm10
	vmulpd	%ymm12, %ymm10, %ymm3
	vshufps	$221, %ymm11, %ymm6, %ymm10
	vshufps	$136, %ymm11, %ymm6, %ymm6
	vperm2f128	$3, %ymm10, %ymm10, %ymm7
	vperm2f128	$3, %ymm6, %ymm6, %ymm11
	vshufps	$68, %ymm7, %ymm10, %ymm13
	vshufps	$238, %ymm7, %ymm10, %ymm7
	vshufps	$68, %ymm11, %ymm6, %ymm10
	vinsertf128	$1, %xmm7, %ymm13, %ymm7
	vshufps	$238, %ymm11, %ymm6, %ymm11
	vinsertf128	$1, %xmm11, %ymm10, %ymm10
	vsubps	%ymm10, %ymm7, %ymm7
	vmulpd	.LC39(%rip), %ymm9, %ymm10
	vmulpd	%ymm12, %ymm9, %ymm9
	vaddpd	%ymm4, %ymm10, %ymm10
	vmulpd	%ymm12, %ymm8, %ymm8
	vmulpd	.LC39(%rip), %ymm1, %ymm6
	vaddpd	%ymm9, %ymm4, %ymm9
	vxorpd	%xmm4, %xmm4, %xmm4
	vaddpd	%ymm5, %ymm6, %ymm6
	vaddpd	%ymm8, %ymm10, %ymm10
	vsubpd	%ymm8, %ymm9, %ymm9
	vmulpd	%ymm12, %ymm1, %ymm8
	vaddpd	%ymm8, %ymm5, %ymm5
	vaddpd	%ymm3, %ymm6, %ymm6
	vcvtpd2psy	%ymm10, %xmm10
	vcvtpd2psy	%ymm9, %xmm9
	vsubpd	%ymm3, %ymm5, %ymm5
	vmovd	%r11d, %xmm3
	vcvtpd2psy	%ymm6, %xmm6
	vinsertf128	$0x1, %xmm6, %ymm10, %ymm6
	vcvtss2sd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm2, %xmm4, %xmm8
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtpd2psy	%ymm5, %xmm5
	vinsertf128	$0x1, %xmm5, %ymm9, %ymm9
	vsubps	%ymm9, %ymm6, %ymm9
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-8204(%rbp), %xmm5, %xmm5
	vcvtss2sd	-8208(%rbp), %xmm6, %xmm6
	vmulsd	%xmm2, %xmm6, %xmm11
	vsubps	%ymm9, %ymm7, %ymm9
	vmulps	.LC32(%rip), %ymm9, %ymm9
	vmovups	%ymm9, -4140(%rbp)
	vmovss	-52(%rax), %xmm9
	vsubss	-56(%rax), %xmm9, %xmm1
	vmovsd	.LC40(%rip), %xmm7
	vaddsd	%xmm11, %xmm5, %xmm11
	vmulsd	%xmm7, %xmm6, %xmm9
	vsubsd	%xmm8, %xmm11, %xmm11
	vaddsd	%xmm5, %xmm9, %xmm9
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm8, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm11, %xmm9, %xmm10
	vmovss	-44(%rax), %xmm9
	vsubss	%xmm10, %xmm1, %xmm6
	vmulsd	%xmm2, %xmm5, %xmm10
	vsubss	-48(%rax), %xmm9, %xmm1
	vmulsd	%xmm7, %xmm5, %xmm9
	vmulss	%xmm0, %xmm6, %xmm6
	vaddsd	%xmm4, %xmm10, %xmm10
	vaddsd	%xmm4, %xmm9, %xmm9
	vmulsd	%xmm7, %xmm4, %xmm4
	vmovss	%xmm6, -4108(%rbp)
	vmovd	%r10d, %xmm6
	vcvtss2sd	%xmm6, %xmm3, %xmm3
	vmulsd	%xmm2, %xmm3, %xmm6
	vaddsd	%xmm8, %xmm3, %xmm8
	vaddsd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm7, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm9, %xmm9
	vsubsd	%xmm6, %xmm10, %xmm10
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm10, %xmm9, %xmm5
	vsubss	%xmm5, %xmm1, %xmm9
	vmovd	%r9d, %xmm5
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm2, %xmm1, %xmm5
	vaddsd	%xmm1, %xmm3, %xmm3
	vaddsd	%xmm6, %xmm1, %xmm6
	vmulsd	%xmm7, %xmm1, %xmm1
	vmulss	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm5, %xmm4, %xmm4
	vsubsd	%xmm5, %xmm8, %xmm8
	vmovss	%xmm9, -4104(%rbp)
	vmovss	-36(%rax), %xmm9
	vsubss	-40(%rax), %xmm9, %xmm9
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm4, %xmm4
	vsubss	%xmm4, %xmm9, %xmm9
	vmovd	%edi, %xmm4
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmulsd	%xmm2, %xmm4, %xmm8
	vaddsd	%xmm5, %xmm4, %xmm5
	vaddsd	%xmm4, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm4, %xmm4
	vmulss	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm8, %xmm3, %xmm3
	vsubsd	%xmm8, %xmm6, %xmm6
	vmovss	%xmm9, -4100(%rbp)
	vmovss	-28(%rax), %xmm9
	vsubss	-32(%rax), %xmm9, %xmm9
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm3, %xmm3
	vmovd	%ecx, %xmm6
	vsubss	%xmm3, %xmm9, %xmm9
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	%xmm6, %xmm3, %xmm3
	vmovss	-20(%rax), %xmm6
	vaddsd	%xmm3, %xmm4, %xmm4
	vsubss	-24(%rax), %xmm6, %xmm6
	vmulss	%xmm0, %xmm9, %xmm9
	vmovss	%xmm9, -4096(%rbp)
	vmulsd	%xmm2, %xmm3, %xmm9
	vaddsd	%xmm8, %xmm3, %xmm3
	vaddsd	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm9, %xmm5, %xmm9
	vmovd	%r8d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmulsd	%xmm2, %xmm5, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm5, %xmm4, %xmm4
	vsubsd	%xmm5, %xmm3, %xmm5
	vxorpd	%xmm3, %xmm3, %xmm3
	vsubss	%xmm1, %xmm6, %xmm6
	vmovss	-12(%rax), %xmm1
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	-16(%rax), %xmm1, %xmm1
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm5, %xmm4, %xmm4
	vmovd	%edi, %xmm5
	vcvtss2sd	%xmm5, %xmm3, %xmm3
	vmulss	%xmm0, %xmm6, %xmm6
	vmovd	%ecx, %xmm5
	movq	-8240(%rbp), %rcx
	vsubss	%xmm4, %xmm1, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm5, %xmm4, %xmm4
	vmovd	%r8d, %xmm5
	vmulsd	.LC33(%rip), %xmm4, %xmm4
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	movq	%rcx, -128(%rax)
	movq	-8232(%rbp), %rcx
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm6, -4092(%rbp)
	vmovss	-4(%rax), %xmm6
	vsubss	-8(%rax), %xmm6, %xmm6
	vsubsd	%xmm4, %xmm8, %xmm2
	movq	%rcx, -120(%rax)
	movq	-8224(%rbp), %rcx
	vmovss	%xmm1, -4088(%rbp)
	vmulsd	.LC36(%rip), %xmm5, %xmm1
	movq	%rcx, -112(%rax)
	movq	-8216(%rbp), %rcx
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm3, %xmm1
	movq	%rcx, -104(%rax)
	movq	-8208(%rbp), %rcx
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm4, %xmm4
	vmulsd	.LC35(%rip), %xmm5, %xmm1
	movq	%rcx, -96(%rax)
	vaddsd	%xmm1, %xmm4, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm1, %xmm6, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -4084(%rbp)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-4144(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	je	.L1321
.L1205:
	cmpl	$1, %r15d
	ja	.L1322
	vmovss	(%rax), %xmm6
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	8(%rax), %xmm10, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	16(%rax), %xmm9, %xmm9
	vxorpd	%xmm8, %xmm8, %xmm8
	vmulsd	.LC21(%rip), %xmm10, %xmm1
	vcvtss2sd	%xmm6, %xmm11, %xmm11
	vcvtss2sd	24(%rax), %xmm8, %xmm8
	vmovups	40(%rax), %ymm7
	vmulsd	.LC20(%rip), %xmm11, %xmm0
	vmovups	8(%rax), %ymm4
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm9, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm8, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vshufps	$136, %ymm7, %ymm4, %ymm1
	vshufps	$221, %ymm7, %ymm4, %ymm4
	vmovss	%xmm0, -8240(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm2
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vmovups	24(%rax), %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vshufps	$136, 56(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm2
	vshufps	$68, %ymm2, %ymm1, %ymm3
	vshufps	$238, %ymm2, %ymm1, %ymm2
	vmovups	16(%rax), %ymm1
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vshufps	$136, 48(%rax), %ymm1, %ymm1
	vperm2f128	$3, %ymm1, %ymm1, %ymm3
	vshufps	$68, %ymm3, %ymm1, %ymm5
	vshufps	$238, %ymm3, %ymm1, %ymm3
	vinsertf128	$1, %xmm3, %ymm5, %ymm3
	vmovups	(%rax), %ymm5
	vshufps	$136, 32(%rax), %ymm5, %ymm5
	vperm2f128	$3, %ymm5, %ymm5, %ymm1
	vshufps	$68, %ymm1, %ymm5, %ymm13
	vshufps	$238, %ymm1, %ymm5, %ymm1
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vinsertf128	$1, %xmm1, %ymm13, %ymm1
	vshufps	$68, %ymm5, %ymm4, %ymm7
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm1, %ymm4
	vinsertf128	$1, %xmm5, %ymm7, %ymm7
	vmulpd	.LC23(%rip), %ymm4, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm0, %ymm4
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vaddpd	%ymm4, %ymm5, %ymm4
	vmulpd	.LC23(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	.LC24(%rip), %ymm0, %ymm0
	vaddpd	%ymm0, %ymm1, %ymm0
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vextractf128	$0x1, %ymm3, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC24(%rip), %ymm1, %ymm1
	vaddpd	%ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm2, %ymm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	72(%rax), %xmm3, %xmm3
	vmulpd	.LC25(%rip), %ymm4, %ymm4
	vaddpd	%ymm1, %ymm0, %ymm1
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC25(%rip), %ymm0, %ymm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	80(%rax), %xmm2, %xmm2
	vsubpd	%ymm4, %ymm5, %ymm4
	vmulsd	.LC26(%rip), %xmm2, %xmm5
	vsubpd	%ymm0, %ymm1, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	88(%rax), %xmm1, %xmm1
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vsubps	%ymm0, %ymm7, %ymm0
	vmovsd	.LC27(%rip), %xmm4
	vmulsd	%xmm4, %xmm2, %xmm2
	vmovups	%ymm0, -8236(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rdx), %xmm0, %xmm0
	vmulsd	.LC26(%rip), %xmm3, %xmm7
	vmulsd	%xmm4, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm1, %xmm13
	vmulsd	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm7, %xmm0, %xmm0
	vmovss	76(%rax), %xmm7
	vaddsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	84(%rax), %xmm5
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm7, %xmm0
	vmulsd	.LC26(%rip), %xmm1, %xmm7
	vmulsd	%xmm4, %xmm1, %xmm1
	vmovss	%xmm0, -8204(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	96(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm13
	vaddsd	%xmm7, %xmm3, %xmm3
	vaddsd	%xmm7, %xmm2, %xmm2
	vmovss	92(%rax), %xmm7
	vsubsd	%xmm13, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm3, %xmm5, %xmm3
	vmulsd	.LC26(%rip), %xmm0, %xmm5
	vmulsd	%xmm4, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	96(%rax), %xmm4, %xmm4
	vmovss	%xmm3, -8200(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm13
	vmulsd	.LC26(%rip), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm3, %xmm5, %xmm1
	vmovss	100(%rax), %xmm5
	vaddsd	%xmm3, %xmm0, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm7, %xmm2
	vmovss	%xmm2, -8196(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm7
	vmulsd	.LC26(%rip), %xmm2, %xmm0
	vsubsd	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	120(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vsubsd	%xmm0, %xmm3, %xmm0
	vmovss	%xmm1, -8192(%rbp)
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovss	108(%rax), %xmm1
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vmulsd	.LC20(%rip), %xmm3, %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmovss	%xmm0, -8188(%rbp)
	vmulsd	.LC22(%rip), %xmm4, %xmm0
	vsubsd	%xmm7, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmulsd	.LC20(%rip), %xmm1, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	116(%rax), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -8184(%rbp)
	vmovd	%xmm5, %ecx
	vmulsd	.LC28(%rip), %xmm4, %xmm7
	vmovss	%xmm6, -4144(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm13
	vmovss	124(%rax), %xmm5
	movl	32(%rax), %ebx
	movl	40(%rax), %r11d
	movl	48(%rax), %r10d
	vaddsd	%xmm13, %xmm7, %xmm7
	movl	56(%rax), %r9d
	vmovsd	.LC30(%rip), %xmm13
	movl	%ebx, -4128(%rbp)
	movl	(%rdx), %r8d
	movl	%r11d, -4124(%rbp)
	vmulsd	%xmm13, %xmm2, %xmm0
	movl	72(%rax), %edi
	movl	%r10d, -4120(%rbp)
	vmulsd	%xmm13, %xmm1, %xmm13
	movl	%r9d, -4116(%rbp)
	movl	%r8d, -4112(%rbp)
	movl	%edi, -4108(%rbp)
	vsubsd	%xmm0, %xmm7, %xmm7
	vmovss	88(%rax), %xmm0
	vmovss	%xmm0, -4100(%rbp)
	vaddsd	%xmm13, %xmm7, %xmm7
	vmovss	80(%rax), %xmm13
	vmovss	%xmm13, -4104(%rbp)
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm7, %xmm5, %xmm7
	vmovss	8(%rax), %xmm5
	vmovss	%xmm5, -4140(%rbp)
	vmovss	16(%rax), %xmm5
	vmovss	%xmm7, -8180(%rbp)
	vmovss	%xmm5, -4136(%rbp)
	vmovss	24(%rax), %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vmovss	96(%rax), %xmm5
	vmovss	%xmm5, -4096(%rbp)
	vmovss	104(%rax), %xmm5
	vmovss	%xmm5, -4092(%rbp)
	vmovss	112(%rax), %xmm5
	vmovss	%xmm5, -4088(%rbp)
	vmovss	120(%rax), %xmm5
	vmovss	%xmm5, -4084(%rbp)
	testb	%r12b, %r12b
	je	.L1210
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8240(%rbp), %xmm5, %xmm5
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm11, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4144(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8236(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm10, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4140(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8232(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm9, %xmm10
	vxorps	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm5, %xmm5
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8228(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm5, -4136(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm10, %xmm8, %xmm10
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtsd2ss	%xmm10, %xmm5, %xmm5
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8224(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm5, -4132(%rbp)
	vmovd	%ebx, %xmm5
	vcvtss2sd	%xmm5, %xmm9, %xmm9
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm5, %xmm5
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8220(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm5, -4128(%rbp)
	vmovd	%r11d, %xmm5
	vcvtss2sd	%xmm5, %xmm8, %xmm8
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8216(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vmovss	%xmm5, -4124(%rbp)
	vmovd	%r10d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8212(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4120(%rbp)
	vmovd	%r9d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8208(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4116(%rbp)
	vmovd	%r8d, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-8204(%rbp), %xmm8, %xmm8
	vmulsd	%xmm6, %xmm8, %xmm8
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4112(%rbp)
	vmovd	%edi, %xmm5
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm8, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4108(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8200(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm13, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8196(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8192(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmovss	%xmm5, -4100(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8188(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm5, %xmm5
	vmovss	%xmm5, -4096(%rbp)
	vmovd	%ecx, %xmm5
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm5, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm7, %xmm6
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -4092(%rbp)
	vaddsd	%xmm0, %xmm2, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm6, %xmm1, %xmm6
	vcvtsd2ss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, -4088(%rbp)
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4084(%rbp)
.L1210:
	movq	-4144(%rbp), %rcx
	subq	$-128, %rax
	subq	$-128, %rdx
	movq	%rcx, -128(%rax)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rax)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rax)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rax)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rax)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-8240(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-8232(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-8224(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-8216(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-8208(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	jne	.L1205
.L1321:
	addq	$4096, %r13
	addq	$4096, %r14
	cmpq	-8264(%rbp), %r13
	jne	.L1211
	movq	-8272(%rbp), %rax
	xorl	%r11d, %r11d
	movq	-8264(%rbp), %r14
	leaq	4116(%rax), %r10
.L1217:
	xorl	%r8d, %r8d
	movslq	%r11d, %rdi
	movq	-8248(%rbp), %rsi
	movq	%r10, %r9
	addl	$1, %r8d
	salq	$5, %rdi
	cmpl	$32, %r8d
	je	.L1213
	.p2align 4,,10
	.p2align 3
.L1323:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L1214:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$4096, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -4096(%rcx)
	cmpl	$32, %edx
	jne	.L1214
	addl	$1, %r8d
	addq	$4100, %r9
	addq	$4096, %rsi
	cmpl	$32, %r8d
	jne	.L1323
.L1213:
	addl	$1, %r11d
	subq	$-128, %r10
	cmpl	$32, %r11d
	jne	.L1217
	movq	-8248(%rbp), %rbx
	movzbl	-8249(%rbp), %r13d
	vmovapd	.LC38(%rip), %ymm15
	vmovapd	.LC39(%rip), %ymm14
	movq	%rbx, %r12
.L1218:
	leaq	64(%r12), %rdx
	movq	%r12, %rax
	movl	$32, %esi
	jmp	.L1222
	.p2align 4,,10
	.p2align 3
.L1325:
	vmovss	24(%rax), %xmm1
	subq	$-128, %rdx
	subq	$-128, %rax
	vaddss	-100(%rax), %xmm1, %xmm1
	vmovss	.LC37(%rip), %xmm0
	vmovss	-128(%rax), %xmm10
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	-124(%rax), %xmm8
	vmovss	-120(%rax), %xmm9
	vaddss	%xmm8, %xmm10, %xmm6
	vmovss	-112(%rax), %xmm7
	vaddss	-116(%rax), %xmm9, %xmm9
	vmovss	%xmm1, -8228(%rbp)
	vmovss	-96(%rax), %xmm1
	vsubss	%xmm10, %xmm8, %xmm8
	vaddss	-92(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm6, %xmm6
	vaddss	-108(%rax), %xmm7, %xmm7
	vmulss	%xmm0, %xmm9, %xmm9
	vmovss	-48(%rax), %xmm4
	vmulss	%xmm0, %xmm1, %xmm1
	vaddss	-44(%rax), %xmm4, %xmm4
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm6, -8240(%rbp)
	vcvtss2sd	%xmm6, %xmm6, %xmm6
	vmovss	%xmm9, -8236(%rbp)
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vmulss	%xmm0, %xmm4, %xmm5
	vmovss	%xmm1, -8224(%rbp)
	vmovss	-88(%rax), %xmm1
	vaddss	-84(%rax), %xmm1, %xmm1
	vmovss	%xmm7, -8232(%rbp)
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmovd	%xmm5, %r11d
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8220(%rbp)
	vmovss	-80(%rax), %xmm1
	vaddss	-76(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8216(%rbp)
	vmovss	-72(%rax), %xmm1
	vaddss	-68(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8212(%rbp)
	vmovss	-128(%rdx), %xmm1
	vaddss	-60(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8208(%rbp)
	vmovss	-56(%rax), %xmm1
	vaddss	-52(%rax), %xmm1, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -8204(%rbp)
	vmovss	%xmm5, -8200(%rbp)
	vmovss	-40(%rax), %xmm3
	vaddss	-36(%rax), %xmm3, %xmm3
	vmovss	-32(%rax), %xmm2
	vaddss	-28(%rax), %xmm2, %xmm2
	vmulss	%xmm0, %xmm3, %xmm5
	vmovss	-24(%rax), %xmm3
	vaddss	-20(%rax), %xmm3, %xmm3
	vmovss	-16(%rax), %xmm1
	vaddss	-12(%rax), %xmm1, %xmm1
	vmovd	%xmm5, %r10d
	vmovups	-88(%rax), %ymm12
	vmovss	%xmm5, -8196(%rbp)
	vmulss	%xmm0, %xmm2, %xmm5
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vmulsd	.LC35(%rip), %xmm6, %xmm10
	vmulsd	.LC36(%rip), %xmm6, %xmm6
	vmovd	%xmm5, %r8d
	vmovss	%xmm5, -8192(%rbp)
	vmulss	%xmm0, %xmm3, %xmm5
	vaddsd	%xmm9, %xmm10, %xmm10
	vsubsd	%xmm9, %xmm6, %xmm6
	vmovaps	-8240(%rbp), %ymm9
	vmovd	%xmm5, %edi
	vmovss	%xmm5, -8188(%rbp)
	vmulss	%xmm0, %xmm1, %xmm5
	vcvtps2pd	%xmm9, %ymm3
	vextractf128	$0x1, %ymm9, %xmm9
	vmovd	%xmm5, %ecx
	vmovss	%xmm5, -8184(%rbp)
	vmovss	-8(%rax), %xmm5
	vaddss	-4(%rax), %xmm5, %xmm1
	vmulss	%xmm0, %xmm1, %xmm5
	vmovsd	.LC34(%rip), %xmm1
	vmulsd	%xmm1, %xmm7, %xmm7
	vmovss	%xmm5, -8180(%rbp)
	vsubsd	%xmm7, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm7, %xmm6
	vmovups	-120(%rax), %ymm7
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm6, %xmm10, %xmm2
	vmovups	-8232(%rbp), %ymm10
	vmovups	-8236(%rbp), %ymm6
	vsubss	%xmm2, %xmm8, %xmm8
	vcvtps2pd	%xmm6, %ymm11
	vcvtps2pd	%xmm9, %ymm2
	vextractf128	$0x1, %ymm6, %xmm6
	vcvtps2pd	%xmm10, %ymm9
	vcvtps2pd	%xmm6, %ymm6
	vextractf128	$0x1, %ymm10, %xmm10
	vcvtps2pd	%xmm10, %ymm10
	vmulpd	%ymm15, %ymm10, %ymm4
	vshufps	$221, %ymm12, %ymm7, %ymm10
	vshufps	$136, %ymm12, %ymm7, %ymm7
	vperm2f128	$3, %ymm7, %ymm7, %ymm12
	vmulss	%xmm0, %xmm8, %xmm8
	vmulpd	%ymm15, %ymm9, %ymm9
	vmovss	%xmm8, -4144(%rbp)
	vperm2f128	$3, %ymm10, %ymm10, %ymm8
	vshufps	$68, %ymm8, %ymm10, %ymm13
	vshufps	$238, %ymm8, %ymm10, %ymm8
	vshufps	$68, %ymm12, %ymm7, %ymm10
	vinsertf128	$1, %xmm8, %ymm13, %ymm8
	vshufps	$238, %ymm12, %ymm7, %ymm12
	vmulpd	%ymm14, %ymm2, %ymm7
	vinsertf128	$1, %xmm12, %ymm10, %ymm10
	vaddpd	%ymm6, %ymm7, %ymm7
	vsubps	%ymm10, %ymm8, %ymm8
	vmulpd	%ymm14, %ymm3, %ymm10
	vaddpd	%ymm11, %ymm10, %ymm10
	vaddpd	%ymm4, %ymm7, %ymm7
	vaddpd	%ymm9, %ymm10, %ymm10
	vcvtpd2psy	%ymm7, %xmm7
	vcvtpd2psy	%ymm10, %xmm10
	vinsertf128	$0x1, %xmm7, %ymm10, %ymm7
	vmulpd	%ymm15, %ymm3, %ymm10
	vaddpd	%ymm10, %ymm11, %ymm11
	vmovd	%r11d, %xmm3
	vsubpd	%ymm9, %ymm11, %ymm11
	vmulpd	%ymm15, %ymm2, %ymm9
	vaddpd	%ymm9, %ymm6, %ymm6
	vcvtpd2psy	%ymm11, %xmm11
	vsubpd	%ymm4, %ymm6, %ymm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm4, %xmm9
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-8204(%rbp), %xmm6, %xmm6
	vmulsd	%xmm1, %xmm6, %xmm12
	vmovd	%r10d, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm11, %ymm11
	vsubps	%ymm11, %ymm7, %ymm11
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8208(%rbp), %xmm7, %xmm7
	vsubps	%ymm11, %ymm8, %ymm11
	vmovss	-52(%rax), %xmm8
	vmulps	.LC32(%rip), %ymm11, %ymm11
	vmovups	%ymm11, -4140(%rbp)
	vsubss	-56(%rax), %xmm8, %xmm2
	vmulsd	%xmm1, %xmm7, %xmm11
	vmovsd	.LC40(%rip), %xmm8
	vaddsd	%xmm4, %xmm12, %xmm12
	vmulsd	%xmm8, %xmm7, %xmm10
	vaddsd	%xmm11, %xmm6, %xmm11
	vaddsd	%xmm6, %xmm10, %xmm10
	vsubsd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm9, %xmm10, %xmm10
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddsd	%xmm9, %xmm3, %xmm9
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm11, %xmm10, %xmm10
	vsubss	%xmm10, %xmm2, %xmm7
	vmulss	%xmm0, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vmovss	-44(%rax), %xmm10
	vmulsd	%xmm1, %xmm3, %xmm7
	vsubss	-48(%rax), %xmm10, %xmm2
	vmulsd	%xmm8, %xmm6, %xmm10
	vmovd	%r8d, %xmm6
	vsubsd	%xmm7, %xmm12, %xmm12
	vaddsd	%xmm4, %xmm10, %xmm10
	vmulsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vaddsd	%xmm7, %xmm10, %xmm10
	vaddsd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm8, %xmm3, %xmm3
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm12, %xmm10, %xmm11
	vsubss	%xmm11, %xmm2, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vmulsd	%xmm1, %xmm2, %xmm6
	vaddsd	%xmm7, %xmm2, %xmm11
	vaddsd	%xmm2, %xmm3, %xmm3
	vmulsd	%xmm8, %xmm2, %xmm2
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm6, %xmm4, %xmm4
	vsubsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm10, -4104(%rbp)
	vmovss	-36(%rax), %xmm10
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	-40(%rax), %xmm10, %xmm10
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm9, %xmm4, %xmm4
	vsubss	%xmm4, %xmm10, %xmm10
	vmovd	%edi, %xmm4
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm4, %xmm9
	vaddsd	%xmm6, %xmm4, %xmm6
	vaddsd	%xmm4, %xmm2, %xmm2
	vmulsd	%xmm8, %xmm4, %xmm4
	vmulss	%xmm0, %xmm10, %xmm10
	vaddsd	%xmm9, %xmm3, %xmm3
	vsubsd	%xmm9, %xmm11, %xmm11
	vmovss	%xmm10, -4100(%rbp)
	vmovss	-28(%rax), %xmm10
	vsubss	-32(%rax), %xmm10, %xmm10
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm11, %xmm3, %xmm7
	vmovd	%ecx, %xmm3
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm9, %xmm3, %xmm9
	vsubss	%xmm7, %xmm10, %xmm10
	vmovss	-20(%rax), %xmm7
	vsubss	-24(%rax), %xmm7, %xmm7
	vmulss	%xmm0, %xmm10, %xmm10
	vmovss	%xmm10, -4096(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm10
	vxorpd	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm10, %xmm2, %xmm2
	vsubsd	%xmm10, %xmm6, %xmm10
	vmovd	%edi, %xmm6
	vcvtss2sd	%xmm6, %xmm3, %xmm3
	vmovd	%ecx, %xmm6
	movq	-8240(%rbp), %rcx
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm10, %xmm2, %xmm2
	movq	%rcx, -128(%rax)
	movq	-8232(%rbp), %rcx
	vsubss	%xmm2, %xmm7, %xmm7
	vmovss	-12(%rax), %xmm2
	vsubss	-16(%rax), %xmm2, %xmm10
	vxorps	%xmm2, %xmm2, %xmm2
	vmulss	%xmm0, %xmm7, %xmm7
	movq	%rcx, -120(%rax)
	movq	-8224(%rbp), %rcx
	vmovss	%xmm7, -4092(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm1, %xmm7, %xmm7
	movq	%rcx, -112(%rax)
	movq	-8216(%rbp), %rcx
	vaddsd	%xmm7, %xmm4, %xmm4
	movq	%rcx, -104(%rax)
	movq	-8208(%rbp), %rcx
	vsubsd	%xmm7, %xmm9, %xmm9
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm9, %xmm2, %xmm2
	vsubss	%xmm2, %xmm4, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm6, %xmm4, %xmm4
	vmulsd	.LC33(%rip), %xmm4, %xmm4
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	%xmm5, %xmm6, %xmm6
	movq	%rcx, -96(%rax)
	vmovss	-4(%rax), %xmm5
	vsubss	%xmm2, %xmm10, %xmm2
	movq	-8200(%rbp), %rcx
	vsubss	-8(%rax), %xmm5, %xmm5
	vmulss	%xmm0, %xmm2, %xmm2
	movq	%rcx, -88(%rax)
	vmovss	%xmm2, -4088(%rbp)
	vmulsd	%xmm1, %xmm3, %xmm2
	vmulsd	.LC36(%rip), %xmm6, %xmm1
	vsubsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm8, %xmm3, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm4, %xmm4
	vmulsd	.LC35(%rip), %xmm6, %xmm1
	vaddsd	%xmm1, %xmm4, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm1
	vsubss	%xmm1, %xmm5, %xmm1
	vmulss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -4084(%rbp)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-4144(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	je	.L1324
.L1222:
	cmpl	$1, %r15d
	ja	.L1325
	vmovss	(%rax), %xmm7
	vxorpd	%xmm12, %xmm12, %xmm12
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovss	8(%rax), %xmm6
	vcvtss2sd	16(%rax), %xmm10, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	24(%rax), %xmm9, %xmm9
	vcvtss2sd	%xmm7, %xmm12, %xmm12
	vmovups	40(%rax), %ymm8
	vcvtss2sd	%xmm6, %xmm11, %xmm11
	vmovups	8(%rax), %ymm4
	vmulsd	.LC21(%rip), %xmm11, %xmm1
	vmulsd	.LC20(%rip), %xmm12, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm10, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm9, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovss	4(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vshufps	$136, %ymm8, %ymm4, %ymm1
	vshufps	$221, %ymm8, %ymm4, %ymm4
	vmovss	%xmm0, -8240(%rbp)
	vperm2f128	$3, %ymm1, %ymm1, %ymm0
	vshufps	$68, %ymm0, %ymm1, %ymm3
	vshufps	$238, %ymm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm0, %ymm3, %ymm3
	vmovups	24(%rax), %ymm0
	vshufps	$136, 56(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm1
	vshufps	$68, %ymm1, %ymm0, %ymm2
	vshufps	$238, %ymm1, %ymm0, %ymm1
	vmovups	16(%rax), %ymm0
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vshufps	$136, 48(%rax), %ymm0, %ymm0
	vperm2f128	$3, %ymm0, %ymm0, %ymm2
	vshufps	$68, %ymm2, %ymm0, %ymm5
	vshufps	$238, %ymm2, %ymm0, %ymm2
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vmovups	(%rax), %ymm5
	vshufps	$136, 32(%rax), %ymm5, %ymm5
	vperm2f128	$3, %ymm5, %ymm5, %ymm0
	vshufps	$68, %ymm0, %ymm5, %ymm13
	vshufps	$238, %ymm0, %ymm5, %ymm0
	vperm2f128	$3, %ymm4, %ymm4, %ymm5
	vinsertf128	$1, %xmm0, %ymm13, %ymm0
	vshufps	$68, %ymm5, %ymm4, %ymm8
	vshufps	$238, %ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm0, %ymm4
	vinsertf128	$1, %xmm5, %ymm8, %ymm8
	vmulpd	.LC23(%rip), %ymm4, %ymm5
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm3, %ymm4
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC24(%rip), %ymm4, %ymm4
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm4, %ymm5, %ymm4
	vmulpd	.LC23(%rip), %ymm0, %ymm0
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm2, %ymm5
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	.LC24(%rip), %ymm3, %ymm3
	vaddpd	%ymm3, %ymm0, %ymm0
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	.LC24(%rip), %ymm5, %ymm5
	vmulpd	.LC24(%rip), %ymm2, %ymm2
	vmovsd	.LC27(%rip), %xmm3
	vaddpd	%ymm5, %ymm4, %ymm5
	vcvtps2pd	%xmm1, %ymm4
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	.LC25(%rip), %ymm4, %ymm4
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC25(%rip), %ymm1, %ymm1
	vaddpd	%ymm2, %ymm0, %ymm0
	vsubpd	%ymm4, %ymm5, %ymm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	80(%rax), %xmm5, %xmm5
	vsubpd	%ymm1, %ymm0, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	72(%rax), %xmm1, %xmm1
	vcvtpd2psy	%ymm4, %xmm4
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vsubps	%ymm0, %ymm8, %ymm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vmulsd	.LC26(%rip), %xmm5, %xmm8
	vcvtss2sd	88(%rax), %xmm4, %xmm4
	vmulsd	%xmm3, %xmm5, %xmm5
	vmovups	%ymm0, -8236(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rdx), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vmulsd	.LC26(%rip), %xmm1, %xmm2
	vmulsd	.LC22(%rip), %xmm4, %xmm13
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm0, %xmm0
	vmovss	76(%rax), %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm0, %xmm0
	vmovss	84(%rax), %xmm8
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm0
	vmulsd	.LC26(%rip), %xmm4, %xmm2
	vmulsd	%xmm3, %xmm4, %xmm4
	vmovss	%xmm0, -8204(%rbp)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	96(%rax), %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm0, %xmm13
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm5, %xmm2, %xmm5
	vsubsd	%xmm13, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm8, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	104(%rax), %xmm8, %xmm8
	vmulsd	.LC22(%rip), %xmm8, %xmm13
	vmulsd	.LC26(%rip), %xmm8, %xmm8
	vmovss	%xmm1, -8200(%rbp)
	vmulsd	.LC26(%rip), %xmm0, %xmm1
	vmulsd	%xmm3, %xmm0, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	104(%rax), %xmm3, %xmm3
	vaddsd	%xmm1, %xmm5, %xmm2
	vmovss	92(%rax), %xmm5
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm8, %xmm0, %xmm0
	vsubsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm8, %xmm1, %xmm1
	vmulsd	.LC20(%rip), %xmm3, %xmm8
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm2, %xmm5, %xmm2
	vmovss	%xmm2, -8196(%rbp)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm4
	vmulsd	.LC26(%rip), %xmm2, %xmm2
	vsubsd	%xmm4, %xmm1, %xmm1
	vmovss	100(%rax), %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	112(%rax), %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm4, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	96(%rax), %xmm4, %xmm4
	vmovss	%xmm1, -8192(%rbp)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	108(%rax), %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	120(%rax), %xmm1, %xmm1
	vmovss	%xmm0, -8188(%rbp)
	vmulsd	.LC22(%rip), %xmm4, %xmm0
	vsubsd	%xmm8, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm0
	vmulsd	.LC20(%rip), %xmm1, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	116(%rax), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm5, %xmm5
	vmovss	%xmm5, -8184(%rbp)
	vmovd	%xmm5, %ecx
	movl	32(%rax), %r11d
	vmovss	124(%rax), %xmm5
	vmovss	%xmm6, -4140(%rbp)
	movl	40(%rax), %r10d
	vmovss	%xmm7, -4144(%rbp)
	vmulsd	.LC29(%rip), %xmm3, %xmm13
	vmulsd	.LC28(%rip), %xmm4, %xmm8
	movl	%r11d, -4128(%rbp)
	movl	48(%rax), %r9d
	movl	56(%rax), %r8d
	movl	%r10d, -4124(%rbp)
	vmovss	96(%rax), %xmm6
	vaddsd	%xmm13, %xmm8, %xmm8
	movl	%r9d, -4120(%rbp)
	vmovsd	.LC30(%rip), %xmm13
	movl	%r8d, -4116(%rbp)
	vmulsd	%xmm13, %xmm2, %xmm0
	vmulsd	%xmm13, %xmm1, %xmm13
	vsubsd	%xmm0, %xmm8, %xmm8
	vmovss	88(%rax), %xmm0
	vaddsd	%xmm13, %xmm8, %xmm8
	vmovss	72(%rax), %xmm13
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm5, %xmm8
	vmovss	16(%rax), %xmm5
	vmovss	%xmm5, -4136(%rbp)
	vmovss	24(%rax), %xmm5
	vmovss	%xmm8, -8180(%rbp)
	vmovss	%xmm5, -4132(%rbp)
	movl	(%rdx), %edi
	vmovss	%xmm6, -4096(%rbp)
	vmovss	104(%rax), %xmm6
	vmovss	80(%rax), %xmm5
	vmovss	%xmm13, -4108(%rbp)
	vmovss	%xmm6, -4092(%rbp)
	vmovss	112(%rax), %xmm6
	movl	%edi, -4112(%rbp)
	vmovss	%xmm5, -4104(%rbp)
	vmovss	%xmm0, -4100(%rbp)
	vmovss	%xmm6, -4088(%rbp)
	vmovss	120(%rax), %xmm6
	vmovss	%xmm6, -4084(%rbp)
	testb	%r13b, %r13b
	je	.L1221
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8240(%rbp), %xmm7, %xmm7
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm5, %xmm5, %xmm5
	vmovsd	.LC33(%rip), %xmm6
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm8, %xmm8, %xmm8
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm12, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4144(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8236(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm11, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8232(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm10, %xmm10
	vxorps	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8228(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4136(%rbp)
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm10
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm10, %xmm7, %xmm7
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8224(%rbp), %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm10
	vmovss	%xmm7, -4132(%rbp)
	vmovd	%r11d, %xmm7
	vcvtss2sd	%xmm7, %xmm9, %xmm9
	vxorps	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8220(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vmovss	%xmm7, -4128(%rbp)
	vmovd	%r10d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8216(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4124(%rbp)
	vmovd	%r9d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8212(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4120(%rbp)
	vmovd	%r8d, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8208(%rbp), %xmm9, %xmm9
	vmulsd	%xmm6, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4116(%rbp)
	vmovd	%edi, %xmm7
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vaddsd	%xmm9, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4112(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8204(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm13, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4108(%rbp)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-8200(%rbp), %xmm7, %xmm7
	vmulsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4104(%rbp)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8196(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vxorps	%xmm5, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8192(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmovss	%xmm5, -4100(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8188(%rbp), %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm5, %xmm5
	vmovss	%xmm5, -4096(%rbp)
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -4092(%rbp)
	vmovd	%ecx, %xmm3
	vcvtss2sd	%xmm3, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm8, %xmm6
	vaddsd	%xmm0, %xmm2, %xmm0
	vxorps	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm6, %xmm1, %xmm6
	vcvtsd2ss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, -4088(%rbp)
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4084(%rbp)
.L1221:
	movq	-4144(%rbp), %rcx
	subq	$-128, %rax
	subq	$-128, %rdx
	movq	%rcx, -128(%rax)
	movq	-4136(%rbp), %rcx
	movq	%rcx, -120(%rax)
	movq	-4128(%rbp), %rcx
	movq	%rcx, -112(%rax)
	movq	-4120(%rbp), %rcx
	movq	%rcx, -104(%rax)
	movq	-4112(%rbp), %rcx
	movq	%rcx, -96(%rax)
	movq	-4104(%rbp), %rcx
	movq	%rcx, -88(%rax)
	movq	-4096(%rbp), %rcx
	movq	%rcx, -80(%rax)
	movq	-4088(%rbp), %rcx
	movq	%rcx, -72(%rax)
	movq	-8240(%rbp), %rcx
	movq	%rcx, -128(%rdx)
	movq	-8232(%rbp), %rcx
	movq	%rcx, -120(%rdx)
	movq	-8224(%rbp), %rcx
	movq	%rcx, -112(%rdx)
	movq	-8216(%rbp), %rcx
	movq	%rcx, -104(%rdx)
	movq	-8208(%rbp), %rcx
	movq	%rcx, -96(%rdx)
	movq	-8200(%rbp), %rcx
	movq	%rcx, -88(%rdx)
	movq	-8192(%rbp), %rcx
	movq	%rcx, -80(%rdx)
	movq	-8184(%rbp), %rcx
	movq	%rcx, -72(%rdx)
	subl	$1, %esi
	jne	.L1222
.L1324:
	addq	$4096, %r12
	cmpq	%r12, %r14
	jne	.L1218
	movq	-8272(%rbp), %rax
	leaq	2100(%rax), %rbx
	leaq	52(%rax), %rdi
	addq	$67636, %rax
	movq	%rbx, -8304(%rbp)
	movq	%rax, -8296(%rbp)
	movq	-8248(%rbp), %rax
	movq	%rdi, -8280(%rbp)
	movq	%rbx, -8264(%rbp)
	movq	%rax, -8288(%rbp)
.L1233:
	movq	-8280(%rbp), %rax
	vmovss	.LC37(%rip), %xmm12
	movq	-8264(%rbp), %rdx
	movzbl	-8249(%rbp), %ecx
	jmp	.L1227
.L1327:
	vmovss	-16(%rax), %xmm6
	subq	$-128, %rax
	vaddss	-140(%rax), %xmm6, %xmm0
	vmovss	-136(%rax), %xmm6
	vaddss	-132(%rax), %xmm6, %xmm3
	vmovss	-156(%rax), %xmm13
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	-160(%rax), %xmm15
	vmulss	%xmm12, %xmm3, %xmm9
	vmovss	-128(%rax), %xmm6
	vmovss	-148(%rax), %xmm11
	vaddss	%xmm13, %xmm15, %xmm2
	vaddss	-124(%rax), %xmm6, %xmm3
	vmovss	%xmm0, -8232(%rbp)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovss	-120(%rax), %xmm6
	vsubss	%xmm15, %xmm13, %xmm15
	vaddss	-152(%rax), %xmm11, %xmm1
	vmulss	%xmm12, %xmm2, %xmm2
	vmovss	%xmm9, -8228(%rbp)
	vaddss	-116(%rax), %xmm6, %xmm4
	vmulss	%xmm12, %xmm3, %xmm8
	vmovss	-112(%rax), %xmm6
	vaddss	-108(%rax), %xmm6, %xmm3
	vmulss	%xmm12, %xmm1, %xmm1
	vmovss	%xmm2, -8240(%rbp)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm2, %xmm13
	vmulss	%xmm12, %xmm4, %xmm7
	vsubss	-152(%rax), %xmm11, %xmm11
	vmovss	%xmm8, -8224(%rbp)
	vmulss	%xmm12, %xmm3, %xmm6
	vmovss	-104(%rax), %xmm3
	vaddss	-100(%rax), %xmm3, %xmm3
	vmovss	%xmm1, -8236(%rbp)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmulsd	.LC36(%rip), %xmm2, %xmm10
	vmulsd	.LC33(%rip), %xmm1, %xmm14
	vmovss	%xmm7, -8220(%rbp)
	vmulss	%xmm12, %xmm3, %xmm5
	vmovss	%xmm6, -8216(%rbp)
	vmovsd	.LC34(%rip), %xmm3
	vmulsd	%xmm3, %xmm0, %xmm4
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm5, -8212(%rbp)
	vsubsd	%xmm14, %xmm10, %xmm14
	vsubsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm13, %xmm14
	vsubss	%xmm14, %xmm15, %xmm13
	vmulsd	%xmm3, %xmm2, %xmm15
	vmulss	%xmm12, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm1, %xmm15
	vmovss	%xmm13, -4144(%rbp)
	vmovsd	.LC40(%rip), %xmm13
	vsubsd	%xmm4, %xmm15, %xmm15
	vmulsd	%xmm13, %xmm2, %xmm14
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm9, %xmm2, %xmm2
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm15, %xmm14, %xmm15
	vmulsd	%xmm13, %xmm1, %xmm14
	vsubss	%xmm15, %xmm11, %xmm11
	vmovss	-140(%rax), %xmm15
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulss	%xmm12, %xmm11, %xmm11
	vmovss	%xmm11, -4140(%rbp)
	vsubss	-144(%rax), %xmm15, %xmm10
	vmulsd	%xmm3, %xmm1, %xmm15
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm11
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	movq	-8240(%rbp), %rsi
	vaddsd	%xmm15, %xmm0, %xmm15
	vmulsd	%xmm13, %xmm0, %xmm0
	movq	%rsi, -160(%rax)
	movq	-8232(%rbp), %rsi
	vaddsd	%xmm11, %xmm14, %xmm14
	vsubsd	%xmm11, %xmm15, %xmm15
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	movq	%rsi, -152(%rax)
	movq	-8224(%rbp), %rsi
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vsubss	%xmm15, %xmm14, %xmm15
	vxorps	%xmm14, %xmm14, %xmm14
	movq	%rsi, -144(%rax)
	movq	-8216(%rbp), %rsi
	vsubss	%xmm15, %xmm10, %xmm10
	vmovss	-132(%rax), %xmm15
	vsubss	-136(%rax), %xmm15, %xmm9
	vaddsd	%xmm2, %xmm4, %xmm15
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm7, %xmm4, %xmm4
	vmovss	-124(%rax), %xmm7
	vmulss	%xmm12, %xmm10, %xmm10
	vsubss	-128(%rax), %xmm7, %xmm8
	movq	%rsi, -136(%rax)
	movq	-4144(%rbp), %rsi
	vmovss	%xmm10, -4136(%rbp)
	vmulsd	%xmm3, %xmm1, %xmm10
	movq	%rsi, -128(%rax)
	vaddsd	%xmm10, %xmm0, %xmm0
	vsubsd	%xmm10, %xmm15, %xmm15
	vaddsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vsubss	%xmm14, %xmm9, %xmm0
	vmulsd	%xmm13, %xmm2, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm11, %xmm14
	vxorps	%xmm11, %xmm11, %xmm11
	vmovss	-116(%rax), %xmm6
	vsubss	-120(%rax), %xmm6, %xmm7
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	-108(%rax), %xmm6
	vsubss	-112(%rax), %xmm6, %xmm6
	vaddsd	%xmm9, %xmm1, %xmm9
	vmovss	%xmm0, -4132(%rbp)
	vmulsd	%xmm3, %xmm4, %xmm0
	movq	-4136(%rbp), %rsi
	movq	%rsi, -120(%rax)
	vaddsd	%xmm0, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm14, %xmm14
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm11, %xmm9, %xmm11
	vmulsd	%xmm13, %xmm1, %xmm9
	vxorps	%xmm1, %xmm1, %xmm1
	vsubss	%xmm11, %xmm8, %xmm8
	vaddsd	%xmm9, %xmm4, %xmm9
	vmulss	%xmm12, %xmm8, %xmm8
	vmovss	%xmm8, -4128(%rbp)
	vmulsd	%xmm3, %xmm2, %xmm8
	vaddsd	%xmm8, %xmm9, %xmm9
	vsubsd	%xmm8, %xmm10, %xmm8
	vcvtsd2ss	%xmm9, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm3, %xmm7, %xmm8
	vxorps	%xmm3, %xmm3, %xmm3
	vmulss	%xmm12, %xmm1, %xmm1
	vmovss	%xmm1, -4124(%rbp)
	vmulsd	%xmm13, %xmm4, %xmm1
	movq	-4128(%rbp), %rsi
	vaddsd	%xmm1, %xmm2, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vsubsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vmovss	-100(%rax), %xmm6
	vsubss	-104(%rax), %xmm6, %xmm5
	vmulss	%xmm12, %xmm3, %xmm3
	vmovss	%xmm3, -4120(%rbp)
	vsubsd	%xmm2, %xmm0, %xmm3
	vmulsd	.LC36(%rip), %xmm7, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm7, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	%xmm0, -4116(%rbp)
	movq	%rsi, -112(%rax)
	movq	-4120(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	je	.L1326
.L1227:
	leaq	-32(%rax), %rsi
	cmpl	$1, %r15d
	ja	.L1327
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-32(%rax), %xmm8, %xmm8
	vcvtss2sd	-24(%rax), %xmm7, %xmm7
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-16(%rax), %xmm6, %xmm6
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8(%rax), %xmm5, %xmm5
	vmulsd	.LC21(%rip), %xmm7, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	(%rax), %xmm4, %xmm4
	vmulsd	.LC20(%rip), %xmm8, %xmm0
	vmulsd	.LC22(%rip), %xmm5, %xmm3
	vmulsd	.LC27(%rip), %xmm8, %xmm2
	vmulsd	.LC22(%rip), %xmm4, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm6, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	-28(%rax), %xmm1
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm9
	vmulsd	.LC26(%rip), %xmm7, %xmm1
	vmulsd	.LC26(%rip), %xmm6, %xmm0
	vmovss	%xmm9, -8240(%rbp)
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm1
	vmovss	-20(%rax), %xmm2
	vsubsd	%xmm3, %xmm1, %xmm1
	vmulsd	.LC27(%rip), %xmm7, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vmovss	-12(%rax), %xmm1
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm2, -8236(%rbp)
	vmovd	%xmm2, %r8d
	vmulsd	.LC26(%rip), %xmm5, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm3
	vsubsd	%xmm14, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm3
	vmulsd	.LC27(%rip), %xmm6, %xmm1
	vmulsd	.LC26(%rip), %xmm4, %xmm0
	vmovss	%xmm3, -8232(%rbp)
	vmovd	%xmm3, %ebx
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	8(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm15
	vaddsd	%xmm1, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vsubsd	%xmm15, %xmm2, %xmm2
	vmulsd	.LC27(%rip), %xmm5, %xmm15
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vmovss	-4(%rax), %xmm2
	vsubss	%xmm1, %xmm2, %xmm13
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	16(%rax), %xmm2, %xmm2
	vmulsd	.LC26(%rip), %xmm3, %xmm1
	vaddsd	%xmm15, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm2, %xmm15
	vmovss	%xmm13, -8228(%rbp)
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm15, %xmm0, %xmm0
	vmovss	4(%rax), %xmm15
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm15, %xmm11
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovss	%xmm11, -8224(%rbp)
	vcvtss2sd	24(%rax), %xmm15, %xmm15
	vmulsd	.LC27(%rip), %xmm4, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC26(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm15, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	12(%rax), %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm1
	vmulsd	.LC20(%rip), %xmm3, %xmm0
	vmovss	%xmm1, -8220(%rbp)
	vsubsd	%xmm0, %xmm14, %xmm0
	vmulsd	.LC21(%rip), %xmm2, %xmm14
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulsd	.LC20(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	20(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm10
	vmulsd	.LC29(%rip), %xmm3, %xmm0
	vmulsd	.LC28(%rip), %xmm4, %xmm14
	vmovss	%xmm10, -8216(%rbp)
	vaddsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm2, %xmm0
	vsubsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	28(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vmovss	-32(%rax), %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovss	-24(%rax), %xmm0
	vmovss	%xmm14, -8212(%rbp)
	vmovss	%xmm0, -4140(%rbp)
	vmovss	-16(%rax), %xmm0
	vmovss	%xmm0, -4136(%rbp)
	vmovss	-8(%rax), %xmm0
	vmovss	%xmm0, -4132(%rbp)
	vmovss	(%rax), %xmm0
	vmovss	%xmm0, -4128(%rbp)
	vmovss	8(%rax), %xmm0
	vmovss	%xmm0, -4124(%rbp)
	vmovss	16(%rax), %xmm0
	vmovss	%xmm0, -4120(%rbp)
	vmovss	24(%rax), %xmm0
	vmovss	%xmm0, -4116(%rbp)
	testb	%cl, %cl
	je	.L1226
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovd	%r8d, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vmovd	%ebx, %xmm7
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4136(%rbp)
	vaddsd	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm11, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm4, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm4, %xmm5, %xmm5
	vmovss	%xmm5, -4128(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm10, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmovss	%xmm5, -4124(%rbp)
	vaddsd	%xmm2, %xmm0, %xmm2
	vmulsd	.LC33(%rip), %xmm14, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vmovss	%xmm2, -4120(%rbp)
	vxorps	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm15, %xmm0, %xmm15
	vcvtsd2ss	%xmm15, %xmm2, %xmm2
	vmovss	%xmm2, -4116(%rbp)
.L1226:
	movq	-4144(%rbp), %rdi
	subq	$-128, %rax
	movq	%rdi, (%rsi)
	movq	-4136(%rbp), %rdi
	movq	%rdi, 8(%rsi)
	movq	-4128(%rbp), %rdi
	movq	%rdi, 16(%rsi)
	movq	-4120(%rbp), %rdi
	movq	%rdi, 24(%rsi)
	movq	-8240(%rbp), %rsi
	movq	%rsi, -128(%rax)
	movq	-8232(%rbp), %rsi
	movq	%rsi, -120(%rax)
	movq	-8224(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-8216(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	jne	.L1227
.L1326:
	movq	-8288(%rbp), %rdx
	movl	$9, %r12d
	xorl	%ecx, %ecx
	movl	$8, %ebx
	movl	$7, %r11d
	movl	$3, %edi
	movl	$2, %esi
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$15, %r14d
	movq	%rdx, %rax
	jmp	.L1263
.L1328:
	vmovss	4(%rax), %xmm0
	vmovss	128(%rax), %xmm1
	vmovss	%xmm0, 128(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %esi
	ja	.L1229
	movslq	%esi, %r13
	vmovss	256(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 256(%rax)
	cmpl	$15, %edi
	ja	.L1229
	movslq	%edi, %r13
	vmovss	384(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 384(%rax)
	cmpl	$15, %r8d
	ja	.L1229
	movslq	%r8d, %r13
	vmovss	512(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 512(%rax)
	cmpl	$15, %r9d
	ja	.L1229
	movslq	%r9d, %r13
	vmovss	640(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 640(%rax)
	cmpl	$15, %r10d
	ja	.L1229
	movslq	%r10d, %r13
	vmovss	768(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 768(%rax)
	cmpl	$15, %r11d
	ja	.L1229
	movslq	%r11d, %r13
	vmovss	896(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 896(%rax)
	cmpl	$15, %ebx
	ja	.L1229
	movslq	%ebx, %r13
	vmovss	1024(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 1024(%rax)
	cmpl	$15, %r12d
	ja	.L1229
	movslq	%r12d, %r13
	vmovss	1152(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	9(%rcx), %r13d
	vmovss	%xmm0, 1152(%rax)
	cmpl	$15, %r13d
	ja	.L1229
	movslq	%r13d, %r13
	vmovss	1280(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	10(%rcx), %r13d
	vmovss	%xmm0, 1280(%rax)
	cmpl	$15, %r13d
	ja	.L1229
	movslq	%r13d, %r13
	vmovss	1408(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	11(%rcx), %r13d
	vmovss	%xmm0, 1408(%rax)
	cmpl	$15, %r13d
	ja	.L1229
	movslq	%r13d, %r13
	vmovss	1536(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	12(%rcx), %r13d
	vmovss	%xmm0, 1536(%rax)
	cmpl	$15, %r13d
	ja	.L1229
	movslq	%r13d, %r13
	vmovss	1664(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	13(%rcx), %r13d
	vmovss	%xmm0, 1664(%rax)
	cmpl	$15, %r13d
	ja	.L1229
	movslq	%r13d, %r13
	vmovss	1792(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 1792(%rax)
	cmpl	$15, %r14d
	jne	.L1229
	vmovss	60(%rdx), %xmm0
	vmovss	1920(%rax), %xmm1
	vmovss	%xmm1, 60(%rdx)
	vmovss	%xmm0, 1920(%rax)
.L1229:
	addl	$1, %r14d
	subq	$-128, %rdx
	addq	$132, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
.L1263:
	addl	$1, %ecx
	cmpl	$16, %ecx
	jne	.L1328
	movq	-8280(%rbp), %rax
	vmovss	.LC37(%rip), %xmm12
	movq	-8264(%rbp), %rdx
	movzbl	-8249(%rbp), %ecx
	jmp	.L1228
.L1330:
	vmovss	-16(%rax), %xmm6
	subq	$-128, %rax
	vaddss	-140(%rax), %xmm6, %xmm0
	vmovss	-136(%rax), %xmm6
	vaddss	-132(%rax), %xmm6, %xmm3
	vmovss	-156(%rax), %xmm13
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	-160(%rax), %xmm15
	vmulss	%xmm12, %xmm3, %xmm9
	vmovss	-128(%rax), %xmm6
	vmovss	-148(%rax), %xmm11
	vaddss	%xmm13, %xmm15, %xmm2
	vaddss	-124(%rax), %xmm6, %xmm3
	vmovss	%xmm0, -8232(%rbp)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovss	-112(%rax), %xmm5
	vsubss	%xmm15, %xmm13, %xmm15
	vaddss	-152(%rax), %xmm11, %xmm1
	vmulss	%xmm12, %xmm2, %xmm2
	vmovss	%xmm9, -8228(%rbp)
	vmulss	%xmm12, %xmm3, %xmm8
	vmovss	-120(%rax), %xmm3
	vaddss	-116(%rax), %xmm3, %xmm4
	vaddss	-108(%rax), %xmm5, %xmm3
	vmulss	%xmm12, %xmm1, %xmm1
	vmovss	%xmm2, -8240(%rbp)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vmulsd	.LC35(%rip), %xmm2, %xmm13
	vmulss	%xmm12, %xmm4, %xmm7
	vsubss	-152(%rax), %xmm11, %xmm11
	vmovss	%xmm8, -8224(%rbp)
	vmulss	%xmm12, %xmm3, %xmm6
	vmovss	-104(%rax), %xmm3
	vaddss	-100(%rax), %xmm3, %xmm3
	vmovss	%xmm1, -8236(%rbp)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmulsd	.LC36(%rip), %xmm2, %xmm10
	vmulsd	.LC33(%rip), %xmm1, %xmm14
	vmovss	%xmm7, -8220(%rbp)
	vmulss	%xmm12, %xmm3, %xmm5
	vmovss	%xmm6, -8216(%rbp)
	vmovsd	.LC34(%rip), %xmm3
	vmulsd	%xmm3, %xmm0, %xmm4
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm5, -8212(%rbp)
	vsubsd	%xmm14, %xmm10, %xmm14
	vsubsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm13, %xmm14
	vsubss	%xmm14, %xmm15, %xmm13
	vmulsd	%xmm3, %xmm2, %xmm15
	vmulss	%xmm12, %xmm13, %xmm13
	vaddsd	%xmm15, %xmm1, %xmm15
	vmovss	%xmm13, -4144(%rbp)
	vmovsd	.LC40(%rip), %xmm13
	vsubsd	%xmm4, %xmm15, %xmm15
	vmulsd	%xmm13, %xmm2, %xmm14
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm9, %xmm2, %xmm2
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm15, %xmm14, %xmm15
	vmulsd	%xmm13, %xmm1, %xmm14
	vsubss	%xmm15, %xmm11, %xmm11
	vmovss	-140(%rax), %xmm15
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulss	%xmm12, %xmm11, %xmm11
	vmovss	%xmm11, -4140(%rbp)
	vsubss	-144(%rax), %xmm15, %xmm10
	vmulsd	%xmm3, %xmm1, %xmm15
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm11
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	movq	-8240(%rbp), %rsi
	vaddsd	%xmm15, %xmm0, %xmm15
	vmulsd	%xmm13, %xmm0, %xmm0
	movq	%rsi, -160(%rax)
	movq	-8232(%rbp), %rsi
	vaddsd	%xmm11, %xmm14, %xmm14
	vsubsd	%xmm11, %xmm15, %xmm15
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	movq	%rsi, -152(%rax)
	movq	-8224(%rbp), %rsi
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vsubss	%xmm15, %xmm14, %xmm15
	vxorps	%xmm14, %xmm14, %xmm14
	movq	%rsi, -144(%rax)
	movq	-8216(%rbp), %rsi
	vsubss	%xmm15, %xmm10, %xmm10
	vmovss	-132(%rax), %xmm15
	vsubss	-136(%rax), %xmm15, %xmm9
	vaddsd	%xmm2, %xmm4, %xmm15
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm7, %xmm4, %xmm4
	vmovss	-124(%rax), %xmm7
	vmulss	%xmm12, %xmm10, %xmm10
	vsubss	-128(%rax), %xmm7, %xmm8
	movq	%rsi, -136(%rax)
	movq	-4144(%rbp), %rsi
	vmovss	%xmm10, -4136(%rbp)
	vmulsd	%xmm3, %xmm1, %xmm10
	movq	%rsi, -128(%rax)
	vaddsd	%xmm10, %xmm0, %xmm0
	vsubsd	%xmm10, %xmm15, %xmm15
	vaddsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm15, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vsubss	%xmm14, %xmm9, %xmm0
	vmulsd	%xmm13, %xmm2, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm11, %xmm14
	vxorps	%xmm11, %xmm11, %xmm11
	vmovss	-116(%rax), %xmm6
	vsubss	-120(%rax), %xmm6, %xmm7
	vmulss	%xmm12, %xmm0, %xmm0
	vaddsd	%xmm9, %xmm1, %xmm9
	vmovss	%xmm0, -4132(%rbp)
	vmulsd	%xmm3, %xmm4, %xmm0
	movq	-4136(%rbp), %rsi
	movq	%rsi, -120(%rax)
	vaddsd	%xmm0, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm14, %xmm14
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm14, %xmm11, %xmm11
	vsubss	%xmm11, %xmm9, %xmm11
	vmulsd	%xmm13, %xmm1, %xmm9
	vxorps	%xmm1, %xmm1, %xmm1
	vsubss	%xmm11, %xmm8, %xmm8
	vaddsd	%xmm9, %xmm4, %xmm9
	vmulss	%xmm12, %xmm8, %xmm8
	vmovss	%xmm8, -4128(%rbp)
	vmulsd	%xmm3, %xmm2, %xmm8
	vaddsd	%xmm8, %xmm9, %xmm9
	vsubsd	%xmm8, %xmm10, %xmm8
	vcvtsd2ss	%xmm9, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm3, %xmm7, %xmm8
	vxorps	%xmm3, %xmm3, %xmm3
	vmovss	-108(%rax), %xmm5
	vsubss	-112(%rax), %xmm5, %xmm6
	vmulss	%xmm12, %xmm1, %xmm1
	vmovss	%xmm1, -4124(%rbp)
	vmulsd	%xmm13, %xmm4, %xmm1
	movq	-4128(%rbp), %rsi
	vaddsd	%xmm1, %xmm2, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vsubsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vmovss	-100(%rax), %xmm6
	vsubss	-104(%rax), %xmm6, %xmm5
	vmulss	%xmm12, %xmm3, %xmm3
	vmovss	%xmm3, -4120(%rbp)
	vsubsd	%xmm2, %xmm0, %xmm3
	vmulsd	.LC36(%rip), %xmm7, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm7, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm12, %xmm0, %xmm0
	vmovss	%xmm0, -4116(%rbp)
	movq	%rsi, -112(%rax)
	movq	-4120(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	je	.L1329
.L1228:
	leaq	-32(%rax), %rsi
	cmpl	$1, %r15d
	ja	.L1330
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-32(%rax), %xmm8, %xmm8
	vcvtss2sd	-24(%rax), %xmm7, %xmm7
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-16(%rax), %xmm6, %xmm6
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8(%rax), %xmm5, %xmm5
	vmulsd	.LC21(%rip), %xmm7, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	(%rax), %xmm4, %xmm4
	vmulsd	.LC20(%rip), %xmm8, %xmm0
	vmulsd	.LC22(%rip), %xmm5, %xmm3
	vmulsd	.LC27(%rip), %xmm8, %xmm2
	vmulsd	.LC22(%rip), %xmm4, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	.LC20(%rip), %xmm6, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vmovss	-28(%rax), %xmm1
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm9
	vmulsd	.LC26(%rip), %xmm7, %xmm1
	vmulsd	.LC26(%rip), %xmm6, %xmm0
	vmovss	%xmm9, -8240(%rbp)
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm1
	vmovss	-20(%rax), %xmm2
	vsubsd	%xmm3, %xmm1, %xmm1
	vmulsd	.LC27(%rip), %xmm7, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vmovss	-12(%rax), %xmm1
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm2, -8236(%rbp)
	vmovd	%xmm2, %r8d
	vmulsd	.LC26(%rip), %xmm5, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm3
	vsubsd	%xmm14, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm3
	vmulsd	.LC27(%rip), %xmm6, %xmm1
	vmulsd	.LC26(%rip), %xmm4, %xmm0
	vmovss	%xmm3, -8232(%rbp)
	vmovd	%xmm3, %ebx
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	8(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm15
	vaddsd	%xmm1, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vsubsd	%xmm15, %xmm2, %xmm2
	vmulsd	.LC27(%rip), %xmm5, %xmm15
	vcvtsd2ss	%xmm2, %xmm1, %xmm1
	vmovss	-4(%rax), %xmm2
	vsubss	%xmm1, %xmm2, %xmm13
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	16(%rax), %xmm2, %xmm2
	vmulsd	.LC26(%rip), %xmm3, %xmm1
	vaddsd	%xmm15, %xmm0, %xmm0
	vmulsd	.LC22(%rip), %xmm2, %xmm15
	vmovss	%xmm13, -8228(%rbp)
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm15, %xmm0, %xmm0
	vmovss	4(%rax), %xmm15
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm15, %xmm11
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovss	%xmm11, -8224(%rbp)
	vcvtss2sd	24(%rax), %xmm15, %xmm15
	vmulsd	.LC27(%rip), %xmm4, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC26(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm15, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	12(%rax), %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm1
	vmulsd	.LC20(%rip), %xmm3, %xmm0
	vmovss	%xmm1, -8220(%rbp)
	vsubsd	%xmm0, %xmm14, %xmm0
	vmulsd	.LC21(%rip), %xmm2, %xmm14
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulsd	.LC20(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	20(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm10
	vmulsd	.LC29(%rip), %xmm3, %xmm0
	vmulsd	.LC28(%rip), %xmm4, %xmm14
	vmovss	%xmm10, -8216(%rbp)
	vaddsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm2, %xmm0
	vsubsd	%xmm0, %xmm14, %xmm14
	vmulsd	.LC30(%rip), %xmm15, %xmm0
	vaddsd	%xmm0, %xmm14, %xmm14
	vmovss	28(%rax), %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm0, %xmm14
	vmovss	-32(%rax), %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovss	-24(%rax), %xmm0
	vmovss	%xmm14, -8212(%rbp)
	vmovss	%xmm0, -4140(%rbp)
	vmovss	-16(%rax), %xmm0
	vmovss	%xmm0, -4136(%rbp)
	vmovss	-8(%rax), %xmm0
	vmovss	%xmm0, -4132(%rbp)
	vmovss	(%rax), %xmm0
	vmovss	%xmm0, -4128(%rbp)
	vmovss	8(%rax), %xmm0
	vmovss	%xmm0, -4124(%rbp)
	vmovss	16(%rax), %xmm0
	vmovss	%xmm0, -4120(%rbp)
	vmovss	24(%rax), %xmm0
	vmovss	%xmm0, -4116(%rbp)
	testb	%cl, %cl
	je	.L1232
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovd	%r8d, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vmovd	%ebx, %xmm7
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm13, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4136(%rbp)
	vxorps	%xmm6, %xmm6, %xmm6
	vaddsd	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm11, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm4, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm4, %xmm6, %xmm6
	vmovss	%xmm6, -4128(%rbp)
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm10, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm5, %xmm5
	vmovss	%xmm5, -4124(%rbp)
	vaddsd	%xmm2, %xmm0, %xmm2
	vmulsd	.LC33(%rip), %xmm14, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vmovss	%xmm2, -4120(%rbp)
	vxorps	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm15, %xmm0, %xmm15
	vcvtsd2ss	%xmm15, %xmm2, %xmm2
	vmovss	%xmm2, -4116(%rbp)
.L1232:
	movq	-4144(%rbp), %rdi
	subq	$-128, %rax
	movq	%rdi, (%rsi)
	movq	-4136(%rbp), %rdi
	movq	%rdi, 8(%rsi)
	movq	-4128(%rbp), %rdi
	movq	%rdi, 16(%rsi)
	movq	-4120(%rbp), %rdi
	movq	%rdi, 24(%rsi)
	movq	-8240(%rbp), %rsi
	movq	%rsi, -128(%rax)
	movq	-8232(%rbp), %rsi
	movq	%rsi, -120(%rax)
	movq	-8224(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-8216(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	jne	.L1228
.L1329:
	addq	$4096, -8264(%rbp)
	addq	$4096, -8288(%rbp)
	movq	-8264(%rbp), %rax
	addq	$4096, -8280(%rbp)
	cmpq	-8296(%rbp), %rax
	jne	.L1233
	movq	-8248(%rbp), %rax
	movl	%r15d, -8288(%rbp)
	movl	$0, -8264(%rbp)
	movq	%rax, -8280(%rbp)
.L1234:
	movslq	-8264(%rbp), %r15
	movl	$8, %r13d
	xorl	%esi, %esi
	movl	$7, %r12d
	movq	-8280(%rbp), %rax
	movl	$6, %ebx
	movl	$5, %r11d
	movl	$4, %r10d
	movq	-8248(%rbp), %rdx
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$15, %r14d
	movq	%r15, %rcx
	salq	$7, %r15
	salq	$5, %rcx
	jmp	.L1237
.L1331:
	vmovss	4(%rax), %xmm0
	vmovss	4096(%rax), %xmm1
	vmovss	%xmm0, 4096(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %r8d
	ja	.L1236
	movslq	%r8d, %rdi
	vmovss	8192(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 8192(%rax)
	cmpl	$15, %r9d
	ja	.L1236
	movslq	%r9d, %rdi
	vmovss	12288(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 12288(%rax)
	cmpl	$15, %r10d
	ja	.L1236
	movslq	%r10d, %rdi
	vmovss	16384(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 16384(%rax)
	cmpl	$15, %r11d
	ja	.L1236
	movslq	%r11d, %rdi
	vmovss	20480(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 20480(%rax)
	cmpl	$15, %ebx
	ja	.L1236
	movslq	%ebx, %rdi
	vmovss	24576(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 24576(%rax)
	cmpl	$15, %r12d
	ja	.L1236
	movslq	%r12d, %rdi
	vmovss	28672(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 28672(%rax)
	cmpl	$15, %r13d
	ja	.L1236
	movslq	%r13d, %rdi
	vmovss	32768(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	8(%rsi), %edi
	vmovss	%xmm0, 32768(%rax)
	cmpl	$15, %edi
	ja	.L1236
	movslq	%edi, %rdi
	vmovss	36864(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	9(%rsi), %edi
	vmovss	%xmm0, 36864(%rax)
	cmpl	$15, %edi
	ja	.L1236
	movslq	%edi, %rdi
	vmovss	40960(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	10(%rsi), %edi
	vmovss	%xmm0, 40960(%rax)
	cmpl	$15, %edi
	ja	.L1236
	movslq	%edi, %rdi
	vmovss	45056(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	11(%rsi), %edi
	vmovss	%xmm0, 45056(%rax)
	cmpl	$15, %edi
	ja	.L1236
	movslq	%edi, %rdi
	vmovss	49152(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	12(%rsi), %edi
	vmovss	%xmm0, 49152(%rax)
	cmpl	$15, %edi
	ja	.L1236
	movslq	%edi, %rdi
	vmovss	53248(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	13(%rsi), %edi
	vmovss	%xmm0, 53248(%rax)
	cmpl	$15, %edi
	ja	.L1236
	movslq	%edi, %rdi
	vmovss	57344(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 57344(%rax)
	cmpl	$15, %r14d
	jne	.L1236
	leaq	(%rdx,%r15), %rdi
	vmovss	61440(%rax), %xmm1
	vmovss	60(%rdi), %xmm0
	vmovss	%xmm1, 60(%rdi)
	vmovss	%xmm0, 61440(%rax)
.L1236:
	addl	$1, %r14d
	addq	$4096, %rdx
	addq	$4100, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
	addl	$1, %r13d
.L1237:
	addl	$1, %esi
	cmpl	$16, %esi
	jne	.L1331
	addl	$1, -8264(%rbp)
	movl	-8264(%rbp), %eax
	subq	$-128, -8280(%rbp)
	cmpl	$16, %eax
	jne	.L1234
	movl	-8288(%rbp), %r15d
	movq	-8304(%rbp), %rdx
	movzbl	-8249(%rbp), %ecx
	vmovsd	.LC26(%rip), %xmm15
.L1238:
	vmovss	.LC37(%rip), %xmm11
	leaq	-2048(%rdx), %rax
	jmp	.L1242
.L1333:
	vmovss	-24(%rax), %xmm3
	subq	$-128, %rax
	vaddss	-148(%rax), %xmm3, %xmm1
	vmovss	-136(%rax), %xmm6
	vaddss	-132(%rax), %xmm6, %xmm3
	vmovss	-156(%rax), %xmm13
	vmulss	%xmm11, %xmm1, %xmm1
	vmovss	-160(%rax), %xmm10
	vmulss	%xmm11, %xmm3, %xmm9
	vmovss	-128(%rax), %xmm3
	vmovss	-144(%rax), %xmm5
	vaddss	%xmm13, %xmm10, %xmm2
	vaddss	-124(%rax), %xmm3, %xmm3
	vmovss	%xmm1, -8236(%rbp)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmovss	-112(%rax), %xmm6
	vsubss	%xmm10, %xmm13, %xmm10
	vaddss	-140(%rax), %xmm5, %xmm0
	vmulss	%xmm11, %xmm2, %xmm2
	vmovss	%xmm9, -8228(%rbp)
	vmulss	%xmm11, %xmm3, %xmm8
	vmovss	-120(%rax), %xmm5
	vaddss	-116(%rax), %xmm5, %xmm3
	vmulss	%xmm11, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm1, %xmm14
	vmovss	%xmm2, -8240(%rbp)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vmulss	%xmm11, %xmm3, %xmm7
	vaddss	-108(%rax), %xmm6, %xmm3
	vmovss	%xmm8, -8224(%rbp)
	vmulsd	.LC36(%rip), %xmm2, %xmm12
	vmulsd	.LC35(%rip), %xmm2, %xmm13
	vmovss	%xmm0, -8232(%rbp)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulss	%xmm11, %xmm3, %xmm6
	vmovss	-104(%rax), %xmm3
	vaddss	-100(%rax), %xmm3, %xmm3
	vmovss	%xmm7, -8220(%rbp)
	vmulss	%xmm11, %xmm3, %xmm5
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm6, -8216(%rbp)
	vmovsd	.LC34(%rip), %xmm3
	vsubsd	%xmm14, %xmm12, %xmm14
	vmulsd	%xmm3, %xmm0, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm12
	vmovss	%xmm5, -8212(%rbp)
	vsubsd	%xmm4, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm14, %xmm13, %xmm14
	vsubss	%xmm14, %xmm10, %xmm13
	vmovss	-148(%rax), %xmm14
	vsubss	-152(%rax), %xmm14, %xmm10
	vmulss	%xmm11, %xmm13, %xmm13
	vmovss	%xmm13, -4144(%rbp)
	vmovsd	.LC40(%rip), %xmm13
	vmulsd	%xmm13, %xmm2, %xmm14
	vaddsd	%xmm12, %xmm1, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm12
	vxorps	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm14
	vcvtsd2ss	%xmm12, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm4, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm2, %xmm14, %xmm14
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm4, %xmm4
	vsubss	%xmm14, %xmm10, %xmm12
	vmulsd	%xmm3, %xmm1, %xmm10
	vmulss	%xmm11, %xmm12, %xmm12
	vmovss	%xmm12, -4140(%rbp)
	vmovss	-140(%rax), %xmm14
	vmulsd	%xmm3, %xmm2, %xmm12
	vsubss	-144(%rax), %xmm14, %xmm9
	vmulsd	%xmm13, %xmm1, %xmm14
	vaddsd	%xmm10, %xmm0, %xmm1
	movq	-8240(%rbp), %rsi
	vsubsd	%xmm12, %xmm1, %xmm10
	vxorps	%xmm1, %xmm1, %xmm1
	vaddsd	%xmm14, %xmm0, %xmm14
	vmulsd	%xmm13, %xmm0, %xmm0
	movq	%rsi, -160(%rax)
	movq	-8232(%rbp), %rsi
	vcvtsd2ss	%xmm10, %xmm1, %xmm1
	vaddsd	%xmm12, %xmm14, %xmm14
	movq	%rsi, -152(%rax)
	movq	-8224(%rbp), %rsi
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm1, %xmm14, %xmm14
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	movq	%rsi, -144(%rax)
	movq	-8216(%rbp), %rsi
	vsubss	%xmm14, %xmm9, %xmm10
	vmovss	-132(%rax), %xmm14
	vsubss	-136(%rax), %xmm14, %xmm9
	vmulss	%xmm11, %xmm10, %xmm10
	movq	%rsi, -136(%rax)
	movq	-4144(%rbp), %rsi
	vmovss	%xmm10, -4136(%rbp)
	vmulsd	%xmm3, %xmm1, %xmm10
	vsubsd	%xmm10, %xmm4, %xmm14
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm7, %xmm4, %xmm4
	vmovss	-124(%rax), %xmm7
	vaddsd	%xmm10, %xmm0, %xmm0
	vsubss	-128(%rax), %xmm7, %xmm8
	movq	%rsi, -128(%rax)
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm14, %xmm0, %xmm14
	vsubss	%xmm14, %xmm9, %xmm0
	vmulsd	%xmm13, %xmm2, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm12, %xmm14
	vxorps	%xmm12, %xmm12, %xmm12
	vmovss	-116(%rax), %xmm6
	vsubss	-120(%rax), %xmm6, %xmm7
	vmulss	%xmm11, %xmm0, %xmm0
	vaddsd	%xmm9, %xmm1, %xmm9
	vmulsd	%xmm13, %xmm1, %xmm1
	vmovss	%xmm0, -4132(%rbp)
	vmulsd	%xmm3, %xmm4, %xmm0
	movq	-4136(%rbp), %rsi
	vaddsd	%xmm1, %xmm4, %xmm1
	movq	%rsi, -120(%rax)
	vaddsd	%xmm0, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm14, %xmm14
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm14, %xmm12, %xmm12
	vsubss	%xmm12, %xmm9, %xmm12
	vsubss	%xmm12, %xmm8, %xmm8
	vmulss	%xmm11, %xmm8, %xmm8
	vmovss	%xmm8, -4128(%rbp)
	vmulsd	%xmm3, %xmm2, %xmm8
	vaddsd	%xmm8, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm10, %xmm8
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm8, %xmm1, %xmm1
	vsubss	%xmm1, %xmm7, %xmm1
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm5, %xmm7, %xmm7
	vmulsd	%xmm3, %xmm7, %xmm8
	vxorps	%xmm3, %xmm3, %xmm3
	vmovss	-108(%rax), %xmm5
	vsubss	-112(%rax), %xmm5, %xmm6
	vmulss	%xmm11, %xmm1, %xmm1
	vmovss	%xmm1, -4124(%rbp)
	vmulsd	%xmm13, %xmm4, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vsubsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm4, %xmm3, %xmm3
	vsubss	%xmm3, %xmm6, %xmm3
	vmulss	%xmm11, %xmm3, %xmm3
	vmovss	%xmm3, -4120(%rbp)
	vmovss	-100(%rax), %xmm3
	vsubss	-104(%rax), %xmm3, %xmm5
	vsubsd	%xmm2, %xmm0, %xmm3
	vmulsd	.LC36(%rip), %xmm7, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	.LC35(%rip), %xmm7, %xmm0
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm0, %xmm0
	vsubss	%xmm0, %xmm3, %xmm0
	vsubss	%xmm0, %xmm5, %xmm0
	vmulss	%xmm11, %xmm0, %xmm0
	vmovss	%xmm0, -4116(%rbp)
	movq	-4128(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-4120(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	je	.L1332
.L1242:
	leaq	-32(%rax), %rsi
	cmpl	$1, %r15d
	ja	.L1333
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-32(%rax), %xmm8, %xmm8
	vcvtss2sd	-24(%rax), %xmm7, %xmm7
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-16(%rax), %xmm6, %xmm6
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-8(%rax), %xmm5, %xmm5
	vmulsd	.LC20(%rip), %xmm8, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	(%rax), %xmm4, %xmm4
	vmulsd	.LC21(%rip), %xmm7, %xmm1
	vmulsd	.LC22(%rip), %xmm5, %xmm3
	vmulsd	%xmm15, %xmm6, %xmm2
	vmulsd	.LC22(%rip), %xmm4, %xmm13
	vaddsd	%xmm1, %xmm0, %xmm1
	vmulsd	.LC20(%rip), %xmm6, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	-28(%rax), %xmm0
	vaddsd	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm9
	vmulsd	%xmm15, %xmm7, %xmm0
	vmulsd	.LC27(%rip), %xmm8, %xmm1
	vmovss	%xmm9, -8240(%rbp)
	vaddsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm1, %xmm0
	vmovss	-20(%rax), %xmm1
	vsubsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm1, %xmm3
	vmulsd	%xmm15, %xmm5, %xmm1
	vmulsd	.LC27(%rip), %xmm7, %xmm0
	vmovss	%xmm3, -8236(%rbp)
	vmovd	%xmm3, %r8d
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	8(%rax), %xmm3, %xmm3
	vmulsd	.LC22(%rip), %xmm3, %xmm14
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovss	-12(%rax), %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm13, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm15, %xmm4, %xmm0
	vmovss	%xmm2, -8232(%rbp)
	vmovd	%xmm2, %r11d
	vmulsd	.LC27(%rip), %xmm6, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	vmovss	-4(%rax), %xmm2
	vaddsd	%xmm0, %xmm1, %xmm1
	vsubsd	%xmm14, %xmm1, %xmm1
	vmulsd	.LC27(%rip), %xmm5, %xmm14
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm15, %xmm3, %xmm1
	vaddsd	%xmm14, %xmm0, %xmm0
	vmovss	%xmm2, -8228(%rbp)
	vmovd	%xmm2, %r10d
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	16(%rax), %xmm2, %xmm2
	vmulsd	.LC22(%rip), %xmm2, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm14, %xmm0, %xmm0
	vmovss	4(%rax), %xmm14
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm0, %xmm14, %xmm12
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	24(%rax), %xmm14, %xmm14
	vmulsd	.LC27(%rip), %xmm4, %xmm0
	vmovss	%xmm12, -8224(%rbp)
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm15, %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	.LC22(%rip), %xmm14, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm1
	vmovss	12(%rax), %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm1, %xmm0, %xmm1
	vmovss	%xmm1, -8220(%rbp)
	vmulsd	.LC20(%rip), %xmm3, %xmm0
	vsubsd	%xmm0, %xmm13, %xmm0
	vmulsd	.LC21(%rip), %xmm2, %xmm13
	vaddsd	%xmm13, %xmm0, %xmm13
	vmulsd	.LC20(%rip), %xmm14, %xmm0
	vaddsd	%xmm0, %xmm13, %xmm13
	vmovss	20(%rax), %xmm0
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vsubss	%xmm13, %xmm0, %xmm10
	vmulsd	.LC29(%rip), %xmm3, %xmm0
	vmulsd	.LC28(%rip), %xmm4, %xmm13
	vmovss	%xmm10, -8216(%rbp)
	vaddsd	%xmm0, %xmm13, %xmm13
	vmulsd	.LC30(%rip), %xmm2, %xmm0
	vsubsd	%xmm0, %xmm13, %xmm13
	vmulsd	.LC30(%rip), %xmm14, %xmm0
	vaddsd	%xmm0, %xmm13, %xmm13
	vmovss	28(%rax), %xmm0
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vsubss	%xmm13, %xmm0, %xmm13
	vmovss	-32(%rax), %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovss	-24(%rax), %xmm0
	vmovss	%xmm13, -8212(%rbp)
	vmovss	%xmm0, -4140(%rbp)
	vmovss	-16(%rax), %xmm0
	vmovss	%xmm0, -4136(%rbp)
	vmovss	-8(%rax), %xmm0
	vmovss	%xmm0, -4132(%rbp)
	vmovss	(%rax), %xmm0
	vmovss	%xmm0, -4128(%rbp)
	vmovss	8(%rax), %xmm0
	vmovss	%xmm0, -4124(%rbp)
	vmovss	16(%rax), %xmm0
	vmovss	%xmm0, -4120(%rbp)
	vmovss	24(%rax), %xmm0
	vmovss	%xmm0, -4116(%rbp)
	testb	%cl, %cl
	je	.L1241
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vmulsd	.LC33(%rip), %xmm9, %xmm9
	vaddsd	%xmm9, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm0, %xmm0
	vmovss	%xmm0, -4144(%rbp)
	vmovd	%r8d, %xmm0
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm7, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -4140(%rbp)
	vmovd	%r11d, %xmm7
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm6, %xmm6
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -4136(%rbp)
	vmovd	%r10d, %xmm6
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vxorps	%xmm6, %xmm6, %xmm6
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm5, %xmm5
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm12, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm5, -4132(%rbp)
	vxorps	%xmm5, %xmm5, %xmm5
	vaddsd	%xmm4, %xmm0, %xmm4
	vmulsd	.LC33(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm4, %xmm5, %xmm5
	vmovss	%xmm5, -4128(%rbp)
	vaddsd	%xmm3, %xmm0, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm10, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtsd2ss	%xmm3, %xmm6, %xmm6
	vmovss	%xmm6, -4124(%rbp)
	vaddsd	%xmm2, %xmm0, %xmm2
	vmulsd	.LC33(%rip), %xmm13, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vmovss	%xmm2, -4120(%rbp)
	vxorps	%xmm2, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm0, %xmm14
	vcvtsd2ss	%xmm14, %xmm2, %xmm2
	vmovss	%xmm2, -4116(%rbp)
.L1241:
	movq	-4144(%rbp), %rdi
	subq	$-128, %rax
	movq	%rdi, (%rsi)
	movq	-4136(%rbp), %rdi
	movq	%rdi, 8(%rsi)
	movq	-4128(%rbp), %rdi
	movq	%rdi, 16(%rsi)
	movq	-4120(%rbp), %rdi
	movq	%rdi, 24(%rsi)
	movq	-8240(%rbp), %rsi
	movq	%rsi, -128(%rax)
	movq	-8232(%rbp), %rsi
	movq	%rsi, -120(%rax)
	movq	-8224(%rbp), %rsi
	movq	%rsi, -112(%rax)
	movq	-8216(%rbp), %rsi
	movq	%rsi, -104(%rax)
	cmpq	%rax, %rdx
	jne	.L1242
.L1332:
	addq	$4096, %rdx
	cmpq	%rdx, -8296(%rbp)
	jne	.L1238
	movq	-8272(%rbp), %rax
	movq	-8248(%rbp), %r12
	leaq	32788(%rax), %r15
.L1249:
	leaq	1024(%r12), %rbx
	movq	%r12, %r13
.L1244:
	movl	-8256(%rbp), %esi
	movq	%r13, %rdi
	subq	$-128, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%rbx, %r13
	jne	.L1244
	leaq	896(%r12), %rax
	movq	%r12, %rdx
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %edi
	movl	$2, %esi
	movl	$7, %ecx
.L1246:
	cmpl	$14, %ecx
	je	.L1248
	vmovss	-892(%rax), %xmm0
	vmovss	-768(%rax), %xmm1
	vmovss	%xmm0, -768(%rax)
	vmovss	%xmm1, -892(%rax)
	cmpl	$7, %esi
	ja	.L1248
	movslq	%esi, %r11
	vmovss	-640(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -640(%rax)
	cmpl	$7, %edi
	ja	.L1248
	movslq	%edi, %r11
	vmovss	-512(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -512(%rax)
	cmpl	$7, %r8d
	ja	.L1248
	movslq	%r8d, %r11
	vmovss	-384(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -384(%rax)
	cmpl	$7, %r9d
	ja	.L1248
	movslq	%r9d, %r11
	vmovss	-256(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -256(%rax)
	cmpl	$7, %r10d
	ja	.L1248
	movslq	%r10d, %r11
	vmovss	-128(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -128(%rax)
	cmpl	$7, %ecx
	jne	.L1248
	vmovss	28(%rdx), %xmm0
	vmovss	(%rax), %xmm1
	vmovss	%xmm1, 28(%rdx)
	vmovss	%xmm0, (%rax)
.L1248:
	addl	$1, %ecx
	subq	$-128, %rdx
	addq	$132, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	cmpl	$15, %ecx
	jne	.L1246
	movq	%r12, %r13
.L1247:
	movl	-8256(%rbp), %esi
	movq	%r13, %rdi
	subq	$-128, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r13, %rbx
	jne	.L1247
	addq	$4096, %r12
	cmpq	%r15, %r12
	jne	.L1249
	movq	-8272(%rbp), %rax
	xorl	%r14d, %r14d
	leaq	24(%rax), %r13
.L1253:
	movq	-8248(%rbp), %rdx
	movslq	%r14d, %rdi
	movq	%r13, %rax
	movl	$6, %ebx
	movq	%rdi, %rcx
	movl	$5, %r11d
	salq	$7, %rdi
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$7, %esi
	salq	$5, %rcx
.L1252:
	cmpl	$14, %esi
	je	.L1251
	vmovss	(%rax), %xmm0
	vmovss	4092(%rax), %xmm1
	vmovss	%xmm0, 4092(%rax)
	vmovss	%xmm1, (%rax)
	cmpl	$7, %r8d
	ja	.L1251
	movslq	%r8d, %r12
	vmovss	8188(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 8188(%rax)
	cmpl	$7, %r9d
	ja	.L1251
	movslq	%r9d, %r12
	vmovss	12284(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 12284(%rax)
	cmpl	$7, %r10d
	ja	.L1251
	movslq	%r10d, %r12
	vmovss	16380(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 16380(%rax)
	cmpl	$7, %r11d
	ja	.L1251
	movslq	%r11d, %r12
	vmovss	20476(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 20476(%rax)
	cmpl	$7, %ebx
	ja	.L1251
	movslq	%ebx, %r12
	vmovss	24572(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 24572(%rax)
	cmpl	$7, %esi
	jne	.L1251
	leaq	(%rdx,%rdi), %r12
	vmovss	28668(%rax), %xmm1
	vmovss	28(%r12), %xmm0
	vmovss	%xmm1, 28(%r12)
	vmovss	%xmm0, 28668(%rax)
.L1251:
	addl	$1, %esi
	addq	$4096, %rdx
	addq	$4100, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	cmpl	$15, %esi
	jne	.L1252
	addl	$1, %r14d
	subq	$-128, %r13
	cmpl	$8, %r14d
	jne	.L1253
	movq	-8312(%rbp), %rbx
.L1254:
	leaq	1024(%rbx), %r13
	movq	%rbx, %r12
.L1255:
	movl	-8256(%rbp), %esi
	movq	%r12, %rdi
	subq	$-128, %r12
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb1EEEvPfi
	cmpq	%r12, %r13
	jne	.L1255
	addq	$4096, %rbx
	cmpq	%r15, %rbx
	jne	.L1254
	movl	$4096, %edx
	xorl	%esi, %esi
	movl	$6, %r13d
	leaq	-8240(%rbp), %rdi
	movl	$5, %r12d
	call	memset
	movq	-8272(%rbp), %r14
	leaq	-8240(%rbp), %rsi
	movq	-8248(%rbp), %rcx
	vmovss	-8316(%rbp), %xmm0
	leaq	135200(%r14), %rbx
	leaq	16(%r14), %rdi
	movq	%rbx, %rdx
	call	_ZN18WaveletsOnInterval19FullTransformEngineILi32ELi32ELi32ELi32EE9thresholdIfLi32EEEifRSt6bitsetIXmlmlT0_T0_T0_EEPT_PA32_A32_Kf
	movl	$4096, %edx
	leaq	-4144(%rbp), %rdi
	movl	%eax, -8248(%rbp)
	leaq	-8240(%rbp), %rsi
	call	memcpy
	leaq	131104(%r14), %r15
	movl	$3, %edx
	movl	$7, %r14d
	movl	$4, %r11d
	movl	$2, %r10d
	movl	$1, %r9d
	xorl	%r8d, %r8d
	movl	$1, %edi
.L1257:
	movl	%r9d, %ecx
	movq	%rdi, %rax
	salq	%cl, %rax
	movq	%rax, %rcx
	movq	%r9, %rax
	shrq	$6, %rax
	testq	%rcx, -4144(%rbp,%rax,8)
	movl	%r10d, %ecx
	setne	%al
	movzbl	%al, %eax
	leal	(%rax,%rax), %esi
	movq	%rdi, %rax
	salq	%cl, %rax
	movq	%rax, %rcx
	movq	%r10, %rax
	shrq	$6, %rax
	testq	%rcx, -4144(%rbp,%rax,8)
	movl	%r8d, %ecx
	setne	%al
	movzbl	%al, %eax
	sall	$2, %eax
	orl	%esi, %eax
	movq	%rdi, %rsi
	salq	%cl, %rsi
	movq	%r8, %rcx
	shrq	$6, %rcx
	testq	%rsi, -4144(%rbp,%rcx,8)
	movl	%edx, %ecx
	setne	%sil
	movzbl	%sil, %esi
	orl	%esi, %eax
	movq	%rdi, %rsi
	salq	%cl, %rsi
	movq	%rdx, %rcx
	shrq	$6, %rcx
	testq	%rsi, -4144(%rbp,%rcx,8)
	movq	%rdi, %rsi
	setne	%cl
	movzbl	%cl, %ecx
	sall	$3, %ecx
	orl	%ecx, %eax
	movl	%r11d, %ecx
	salq	%cl, %rsi
	movq	%r11, %rcx
	shrq	$6, %rcx
	testq	%rsi, -4144(%rbp,%rcx,8)
	movl	%r12d, %ecx
	setne	%sil
	movzbl	%sil, %esi
	sall	$4, %esi
	orl	%eax, %esi
	movq	%rdi, %rax
	salq	%cl, %rax
	movq	%rax, %rcx
	movq	%r12, %rax
	shrq	$6, %rax
	testq	%rcx, -4144(%rbp,%rax,8)
	movl	%r13d, %ecx
	setne	%al
	movzbl	%al, %eax
	sall	$5, %eax
	orl	%esi, %eax
	movq	%rdi, %rsi
	salq	%cl, %rsi
	movq	%r13, %rcx
	shrq	$6, %rcx
	testq	%rsi, -4144(%rbp,%rcx,8)
	movl	%r14d, %ecx
	setne	%sil
	movzbl	%sil, %esi
	sall	$6, %esi
	orl	%eax, %esi
	movq	%rdi, %rax
	salq	%cl, %rax
	movq	%rax, %rcx
	movq	%r14, %rax
	shrq	$6, %rax
	testq	%rcx, -4144(%rbp,%rax,8)
	setne	%al
	addq	$8, %r8
	addq	$8, %r9
	movzbl	%al, %eax
	addq	$8, %r10
	addq	$8, %rdx
	sall	$7, %eax
	addq	$8, %r11
	addq	$8, %r12
	orl	%esi, %eax
	addq	$8, %r13
	addq	$8, %r14
	movb	%al, (%r15)
	addq	$1, %r15
	cmpq	$32768, %r8
	jne	.L1257
	cmpb	$0, -8320(%rbp)
	je	.L1258
	movl	-8248(%rbp), %eax
	xorl	%esi, %esi
	movw	$31, %r8w
	xorl	%edi, %edi
	testl	%eax, %eax
	jle	.L1261
.L1281:
	movl	(%rbx,%rsi,4), %eax
	movl	%eax, %edx
	andl	$2147483647, %edx
	shrl	$23, %edx
	subl	$112, %edx
	cmpl	$31, %edx
	cmovg	%r8d, %edx
	movl	%edx, %ecx
	sall	$10, %ecx
	testl	%edx, %edx
	movl	%eax, %edx
	cmovle	%edi, %ecx
	andl	$-2147483648, %edx
	andl	$8388607, %eax
	shrl	$16, %edx
	orl	%ecx, %edx
	shrl	$13, %eax
	orl	%edx, %eax
	movw	%ax, (%rbx,%rsi,2)
	addq	$1, %rsi
	cmpl	%esi, -8248(%rbp)
	jg	.L1281
.L1261:
	movslq	-8248(%rbp), %rax
	leaq	4096(%rax,%rax), %rax
	jmp	.L1314
.L1258:
	movslq	-8248(%rbp), %rax
	leaq	4096(,%rax,4), %rax
.L1314:
	addq	$8288, %rsp
	popq	%rbx
	popq	%r10
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1381:
	.size	_ZN24WaveletCompressorGenericILi32EfE8compressEfbi, .-_ZN24WaveletCompressorGenericILi32EfE8compressEfbi
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbi,comdat
.LCOLDE66:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE8compressEfbi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE8compressEfbi,comdat
.LHOTE66:
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,comdat
	.align 2
.LCOLDB67:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,comdat
.LHOTB67:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi
	.type	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi, @function
_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi:
.LFB1388:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movzbl	%sil, %esi
	movq	%rdi, %rbx
	subq	$112, %rsp
	.cfi_def_cfa_offset 128
	call	_ZN24WaveletCompressorGenericILi32EfE8compressEfbi
	movq	%rsp, %rdi
	movl	$14, %ecx
	movl	$-1, %esi
	movq	%rax, %rdx
	xorl	%eax, %eax
	rep; stosq
	leaq	131104(%rbx), %rax
	movl	%edx, 8(%rsp)
	addq	$266280, %rbx
	movl	$.LC59, %edx
	movq	%rsp, %rdi
	movq	%rax, (%rsp)
	movl	$135168, 32(%rsp)
	movq	%rbx, 24(%rsp)
	movb	$112, %cl
	call	deflateInit_
	testl	%eax, %eax
	je	.L1335
.L1336:
	movl	$.LC60, %edi
	call	puts
	call	abort
	.p2align 4,,10
	.p2align 3
.L1335:
	movl	$4, %esi
	movq	%rsp, %rdi
	call	deflate
	cmpl	$1, %eax
	jne	.L1336
	movq	40(%rsp), %rbx
	movq	%rsp, %rdi
	call	deflateEnd
	addq	$112, %rsp
	.cfi_def_cfa_offset 16
	movslq	%ebx, %rax
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1388:
	.size	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi, .-_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,comdat
.LCOLDE67:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi,comdat
.LHOTE67:
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,comdat
.LCOLDB68:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,comdat
.LHOTB68:
	.p2align 4,,15
	.weak	_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi
	.type	_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi, @function
_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi:
.LFB1674:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	leal	-1(%rsi), %eax
	movl	$16, %edx
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r12
	.cfi_offset 12, -24
	movl	%esi, %r12d
	pushq	%r10
	leaq	16(%rdi), %rsi
	pushq	%rbx
	.cfi_offset 10, -32
	.cfi_offset 3, -40
	movq	%rdi, %rbx
	subq	$40, %rsp
	cmpl	$1, %eax
	movq	(%rdi), %rax
	movq	%rax, -64(%rbp)
	movq	8(%rdi), %rax
	leaq	-48(%rbp), %rdi
	movq	%rax, -56(%rbp)
	jbe	.L1352
	call	memcpy
	vxorpd	%xmm8, %xmm8, %xmm8
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	-60(%rbp), %xmm0, %xmm0
	vcvtss2sd	-64(%rbp), %xmm8, %xmm8
	vmovsd	.LC36(%rip), %xmm3
	vxorpd	%xmm4, %xmm4, %xmm4
	vmulsd	%xmm5, %xmm0, %xmm9
	vcvtss2sd	-56(%rbp), %xmm4, %xmm4
	vmovss	-48(%rbp), %xmm10
	vmovsd	.LC34(%rip), %xmm1
	vmulsd	%xmm3, %xmm8, %xmm2
	vmovsd	.LC35(%rip), %xmm6
	vmulsd	%xmm1, %xmm4, %xmm7
	vsubsd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm7, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm10, %xmm2, %xmm2
	vmovss	%xmm2, (%rbx)
	vmulsd	%xmm6, %xmm8, %xmm2
	vaddsd	%xmm2, %xmm9, %xmm9
	vmulsd	%xmm1, %xmm8, %xmm2
	vsubsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm2, %xmm0, %xmm2
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vaddss	%xmm9, %xmm10, %xmm9
	vmovsd	.LC40(%rip), %xmm10
	vmulsd	%xmm10, %xmm8, %xmm8
	vsubsd	%xmm7, %xmm2, %xmm2
	vmovss	%xmm9, 4(%rbx)
	vmovss	-44(%rbp), %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm8, %xmm0, %xmm8
	vmovss	%xmm2, 8(%rbx)
	vmulsd	%xmm1, %xmm0, %xmm2
	vmulsd	%xmm10, %xmm0, %xmm0
	vaddsd	%xmm8, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vaddss	%xmm7, %xmm9, %xmm7
	vmovss	-40(%rbp), %xmm9
	vaddsd	%xmm2, %xmm4, %xmm8
	vmovss	%xmm7, 12(%rbx)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-52(%rbp), %xmm7, %xmm7
	vmulsd	%xmm1, %xmm7, %xmm1
	vmulsd	%xmm6, %xmm7, %xmm6
	vsubsd	%xmm1, %xmm8, %xmm8
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm9, %xmm8, %xmm8
	vmovss	%xmm8, 16(%rbx)
	vaddsd	%xmm0, %xmm4, %xmm8
	vmulsd	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm4, %xmm0
	vsubsd	%xmm4, %xmm2, %xmm4
	vmulsd	%xmm3, %xmm7, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm9, %xmm1
	vaddsd	%xmm6, %xmm0, %xmm0
	vmovss	%xmm1, 20(%rbx)
	vmovss	-36(%rbp), %xmm1
	vaddsd	%xmm2, %xmm4, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm2
	vmovss	%xmm0, 24(%rbx)
	vmovss	%xmm2, 28(%rbx)
	addq	$40, %rsp
	popq	%rbx
	popq	%r10
	popq	%r12
	popq	%rbp
	.cfi_remember_state
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L1352:
	.cfi_restore_state
	call	memcpy
	cmpl	$2, %r12d
	jne	.L1343
	vmovaps	-48(%rbp), %xmm4
	vxorps	%xmm0, %xmm0, %xmm0
	vmovapd	.LC52(%rip), %xmm3
	vcvtps2pd	%xmm4, %xmm1
	vmovaps	-64(%rbp), %xmm5
	vmulpd	%xmm3, %xmm1, %xmm1
	vcvtps2pd	%xmm5, %xmm2
	vmovhlps	%xmm5, %xmm0, %xmm0
	vcvtps2pd	%xmm0, %xmm0
	vsubpd	%xmm1, %xmm2, %xmm1
	vxorps	%xmm2, %xmm2, %xmm2
	vmovhlps	%xmm4, %xmm2, %xmm2
	vcvtps2pd	%xmm2, %xmm2
	vmulpd	%xmm3, %xmm2, %xmm2
	vsubpd	%xmm2, %xmm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vmovaps	%xmm0, -64(%rbp)
.L1343:
	vmovss	-64(%rbp), %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-60(%rbp), %xmm1, %xmm1
	vxorpd	%xmm8, %xmm8, %xmm8
	vmovsd	.LC20(%rip), %xmm9
	vcvtss2sd	-56(%rbp), %xmm8, %xmm8
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-52(%rbp), %xmm7, %xmm7
	vmovsd	.LC21(%rip), %xmm10
	vmovss	%xmm0, (%rbx)
	vmovss	-60(%rbp), %xmm0
	vmovsd	.LC22(%rip), %xmm2
	vmulsd	%xmm10, %xmm1, %xmm5
	vmovsd	.LC26(%rip), %xmm4
	vmovss	%xmm0, 8(%rbx)
	vmovss	-56(%rbp), %xmm0
	vmulsd	%xmm2, %xmm7, %xmm11
	vmovss	%xmm0, 16(%rbx)
	vmovss	-52(%rbp), %xmm0
	vmovss	%xmm0, 24(%rbx)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-64(%rbp), %xmm0, %xmm0
	vmulsd	%xmm9, %xmm0, %xmm3
	vmulsd	.LC27(%rip), %xmm0, %xmm6
	vmulsd	%xmm2, %xmm0, %xmm2
	vmulsd	.LC28(%rip), %xmm0, %xmm0
	vaddsd	%xmm5, %xmm3, %xmm5
	vmulsd	%xmm9, %xmm8, %xmm3
	vsubsd	%xmm3, %xmm5, %xmm5
	vmulsd	%xmm9, %xmm1, %xmm3
	vaddsd	%xmm11, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-48(%rbp), %xmm5, %xmm5
	vmovss	%xmm5, 4(%rbx)
	vmulsd	%xmm4, %xmm1, %xmm5
	vmulsd	%xmm4, %xmm8, %xmm4
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vaddsd	%xmm5, %xmm6, %xmm5
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm4, %xmm5, %xmm4
	vsubsd	%xmm11, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-44(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 12(%rbx)
	vsubsd	%xmm3, %xmm2, %xmm4
	vmulsd	%xmm10, %xmm8, %xmm2
	vaddsd	%xmm2, %xmm4, %xmm3
	vmulsd	%xmm9, %xmm7, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-40(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 20(%rbx)
	vmovsd	.LC30(%rip), %xmm2
	vmulsd	%xmm2, %xmm8, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm1
	vmulsd	%xmm2, %xmm7, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-36(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 28(%rbx)
	addq	$40, %rsp
	popq	%rbx
	popq	%r10
	popq	%r12
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1674:
	.size	_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi, .-_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi
	.section	.text.unlikely._ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,comdat
.LCOLDE68:
	.section	.text._ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,"axG",@progbits,_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi,comdat
.LHOTE68:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,comdat
	.align 2
.LCOLDB73:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,comdat
.LHOTB73:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi
	.type	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi, @function
_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi:
.LFB1395:
	.cfi_startproc
	leaq	8(%rsp), %r10
	.cfi_def_cfa 10, 0
	andq	$-32, %rsp
	pushq	-8(%r10)
	pushq	%rbp
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r15
	.cfi_escape 0x10,0xf,0x2,0x76,0x78
	movl	%esi, %r15d
	xorl	%esi, %esi
	pushq	%r14
	.cfi_escape 0x10,0xe,0x2,0x76,0x70
	movl	$7, %r14d
	pushq	%r13
	pushq	%r12
	pushq	%r10
	.cfi_escape 0xf,0x3,0x76,0x58,0x6
	.cfi_escape 0x10,0xd,0x2,0x76,0x68
	.cfi_escape 0x10,0xc,0x2,0x76,0x60
	pushq	%rbx
	.cfi_escape 0x10,0x3,0x2,0x76,0x50
	movq	%rdi, %rbx
	leaq	67108896(%rbx), %r12
	subq	$4195616, %rsp
	movq	%rdi, -4195424(%rbp)
	movl	$1, %ebx
	movq	%rdx, -4195416(%rbp)
	leaq	-4194352(%rbp), %rdi
	movl	$2097152, %edx
	movl	%ecx, -4195504(%rbp)
	call	memset
	movl	$2, %r11d
	movl	$6, %edi
	movl	$3, %r10d
	movl	$4, %r9d
	movl	$5, %r8d
	xorl	%esi, %esi
	movl	$1, %eax
	jmp	.L1370
	.p2align 4,,10
	.p2align 3
.L1354:
	andn	-4194352(%rbp,%r13,8), %rcx, %rcx
.L1355:
	movq	%rcx, -4194352(%rbp,%r13,8)
	movl	%ebx, %ecx
	movq	%rbx, %r13
	andl	$63, %ecx
	shrq	$6, %r13
	shlx	%rcx, %rax, %rcx
	testb	$2, %dl
	jne	.L1696
	andn	-4194352(%rbp,%r13,8), %rcx, %rcx
.L1357:
	movq	%rcx, -4194352(%rbp,%r13,8)
	movl	%r11d, %ecx
	movq	%r11, %r13
	andl	$63, %ecx
	shrq	$6, %r13
	shlx	%rcx, %rax, %rcx
	testb	$4, %dl
	jne	.L1697
	andn	-4194352(%rbp,%r13,8), %rcx, %rcx
.L1359:
	movq	%rcx, -4194352(%rbp,%r13,8)
	movl	%r10d, %ecx
	movq	%r10, %r13
	andl	$63, %ecx
	shrq	$6, %r13
	shlx	%rcx, %rax, %rcx
	testb	$8, %dl
	jne	.L1698
	andn	-4194352(%rbp,%r13,8), %rcx, %rcx
.L1361:
	movq	%rcx, -4194352(%rbp,%r13,8)
	movl	%r9d, %ecx
	movq	%r9, %r13
	andl	$63, %ecx
	shrq	$6, %r13
	shlx	%rcx, %rax, %rcx
	testb	$16, %dl
	jne	.L1699
	andn	-4194352(%rbp,%r13,8), %rcx, %rcx
.L1363:
	movq	%rcx, -4194352(%rbp,%r13,8)
	movl	%r8d, %ecx
	movq	%r8, %r13
	andl	$63, %ecx
	shrq	$6, %r13
	shlx	%rcx, %rax, %rcx
	testb	$32, %dl
	jne	.L1700
	andn	-4194352(%rbp,%r13,8), %rcx, %rcx
.L1365:
	movq	%rcx, -4194352(%rbp,%r13,8)
	movl	%edi, %ecx
	movq	%rdi, %r13
	andl	$63, %ecx
	shrq	$6, %r13
	shlx	%rcx, %rax, %rcx
	testb	$64, %dl
	jne	.L1701
	andn	-4194352(%rbp,%r13,8), %rcx, %rcx
.L1367:
	movq	%rcx, -4194352(%rbp,%r13,8)
	movq	%r14, %rcx
	movl	%r14d, %r13d
	andl	$63, %r13d
	shrq	$6, %rcx
	testb	%dl, %dl
	shlx	%r13, %rax, %rdx
	js	.L1702
	andn	-4194352(%rbp,%rcx,8), %rdx, %rdx
.L1369:
	addq	$8, %rsi
	movq	%rdx, -4194352(%rbp,%rcx,8)
	addq	$1, %r12
	addq	$8, %r14
	addq	$8, %rdi
	addq	$8, %r8
	addq	$8, %r9
	addq	$8, %r10
	addq	$8, %r11
	addq	$8, %rbx
	cmpq	$16777216, %rsi
	je	.L1703
.L1370:
	movzbl	(%r12), %edx
	movl	%esi, %ecx
	movq	%rsi, %r13
	andl	$63, %ecx
	shrq	$6, %r13
	shlx	%rcx, %rax, %rcx
	testb	$1, %dl
	je	.L1354
	orq	-4194352(%rbp,%r13,8), %rcx
	jmp	.L1355
.L1703:
	movq	-4195416(%rbp), %rax
	movq	$0, -4195408(%rbp)
	movq	$0, -4195400(%rbp)
	movq	$0, -4195392(%rbp)
	subq	$2097152, %rax
	cmpb	$1, %r15b
	sbbq	%rcx, %rcx
	xorl	%edx, %edx
	andl	$2, %ecx
	addq	$2, %rcx
	divq	%rcx
	movslq	%eax, %rbx
	movq	%rax, %r13
	movl	%eax, -4195416(%rbp)
	testq	%rbx, %rbx
	jne	.L1704
	movq	$0, -4195392(%rbp)
	movq	%rsp, %r12
	movq	$0, -4195400(%rbp)
	testb	%r15b, %r15b
	je	.L1705
.L1386:
	movq	%r12, %rsp
.L1375:
	leaq	-4194352(%rbp), %rsi
	movl	$2097152, %edx
	leaq	-2097200(%rbp), %rdi
	call	memcpy
	movq	-4195424(%rbp), %r15
	subq	$2097152, %rsp
	movl	$2097152, %edx
	leaq	-2097200(%rbp), %rsi
	movq	%rsp, %rdi
	leaq	16(%r15), %rbx
	movq	%r15, %rax
	leaq	2097176(%r15), %r15
	addq	$24, %rax
	movq	%rax, %r12
	movq	%rax, -4195472(%rbp)
	call	memcpy
	movq	%r12, %rdx
	movq	%rbx, %rdi
	leaq	-4195408(%rbp), %rsi
	call	_ZN18WaveletsOnInterval19FullTransformEngineILi256ELi256ELi256ELi256EE4loadIfEEvRSt6vectorIT_SaIS4_EESt6bitsetILm16777216EEPA256_A256_f
	addq	$2097152, %rsp
.L1387:
	leaq	8192(%r12), %r13
	movq	%r12, %rbx
.L1388:
	movl	-4195504(%rbp), %esi
	movq	%rbx, %rdi
	addq	$1024, %rbx
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi
	cmpq	%r13, %rbx
	jne	.L1388
	addq	$262144, %r12
	cmpq	%r15, %r12
	jne	.L1387
	movq	-4195424(%rbp), %rax
	xorl	%r14d, %r14d
	leaq	28(%rax), %r13
.L1392:
	movq	-4195472(%rbp), %rdx
	movslq	%r14d, %rdi
	movq	%r13, %rax
	movl	$6, %ebx
	movq	%rdi, %rcx
	movl	$5, %r11d
	salq	$10, %rdi
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$7, %esi
	salq	$8, %rcx
.L1391:
	cmpl	$14, %esi
	je	.L1390
	vmovss	(%rax), %xmm0
	vmovss	262140(%rax), %xmm1
	vmovss	%xmm0, 262140(%rax)
	vmovss	%xmm1, (%rax)
	cmpl	$7, %r8d
	ja	.L1390
	movslq	%r8d, %r12
	vmovss	524284(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 524284(%rax)
	cmpl	$7, %r9d
	ja	.L1390
	movslq	%r9d, %r12
	vmovss	786428(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 786428(%rax)
	cmpl	$7, %r10d
	ja	.L1390
	movslq	%r10d, %r12
	vmovss	1048572(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1048572(%rax)
	cmpl	$7, %r11d
	ja	.L1390
	movslq	%r11d, %r12
	vmovss	1310716(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1310716(%rax)
	cmpl	$7, %ebx
	ja	.L1390
	movslq	%ebx, %r12
	vmovss	1572860(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 1572860(%rax)
	cmpl	$7, %esi
	jne	.L1390
	leaq	(%rdx,%rdi), %r12
	vmovss	1835004(%rax), %xmm1
	vmovss	28(%r12), %xmm0
	vmovss	%xmm1, 28(%r12)
	vmovss	%xmm0, 1835004(%rax)
.L1390:
	addl	$1, %esi
	addq	$262144, %rdx
	addq	$262148, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	cmpl	$15, %esi
	jne	.L1391
	addl	$1, %r14d
	addq	$1024, %r13
	cmpl	$8, %r14d
	jne	.L1392
	movq	-4195472(%rbp), %r12
.L1398:
	leaq	8192(%r12), %rbx
	movq	%r12, %r13
.L1393:
	movl	-4195504(%rbp), %esi
	movq	%r13, %rdi
	addq	$1024, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi
	cmpq	%rbx, %r13
	jne	.L1393
	leaq	7168(%r12), %rax
	movq	%r12, %rdx
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %edi
	movl	$2, %esi
	movl	$7, %ecx
.L1395:
	cmpl	$14, %ecx
	je	.L1397
	vmovss	-7164(%rax), %xmm0
	vmovss	-6144(%rax), %xmm1
	vmovss	%xmm0, -6144(%rax)
	vmovss	%xmm1, -7164(%rax)
	cmpl	$7, %esi
	ja	.L1397
	movslq	%esi, %r11
	vmovss	-5120(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -5120(%rax)
	cmpl	$7, %edi
	ja	.L1397
	movslq	%edi, %r11
	vmovss	-4096(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -4096(%rax)
	cmpl	$7, %r8d
	ja	.L1397
	movslq	%r8d, %r11
	vmovss	-3072(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -3072(%rax)
	cmpl	$7, %r9d
	ja	.L1397
	movslq	%r9d, %r11
	vmovss	-2048(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -2048(%rax)
	cmpl	$7, %r10d
	ja	.L1397
	movslq	%r10d, %r11
	vmovss	-1024(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -1024(%rax)
	cmpl	$7, %ecx
	jne	.L1397
	vmovss	28(%rdx), %xmm0
	vmovss	(%rax), %xmm1
	vmovss	%xmm1, 28(%rdx)
	vmovss	%xmm0, (%rax)
.L1397:
	addl	$1, %ecx
	addq	$1024, %rdx
	addq	$1028, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	cmpl	$15, %ecx
	jne	.L1395
	movq	%r12, %r13
.L1396:
	movl	-4195504(%rbp), %esi
	movq	%r13, %rdi
	addq	$1024, %r13
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi
	cmpq	%rbx, %r13
	jne	.L1396
	addq	$262144, %r12
	cmpq	%r15, %r12
	jne	.L1398
	movl	-4195504(%rbp), %eax
	leal	-1(%rax), %ebx
	cmpl	$2, %eax
	movq	-4195424(%rbp), %rax
	sete	-4195416(%rbp)
	movl	%ebx, -4195428(%rbp)
	leaq	16440(%rax), %r12
	leaq	4210744(%rax), %r13
.L1399:
	leaq	-16384(%r12), %rbx
	jmp	.L1406
	.p2align 4,,10
	.p2align 3
.L1400:
	call	memcpy
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC36(%rip), %xmm4
	vcvtss2sd	-4195376(%rbp), %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm3
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm6
	vcvtss2sd	-4195368(%rbp), %xmm5, %xmm5
	vmovsd	.LC34(%rip), %xmm7
	addq	$1024, %rbx
	vmovss	-4194864(%rbp), %xmm2
	vmulsd	%xmm7, %xmm5, %xmm10
	vsubsd	%xmm3, %xmm6, %xmm6
	vaddsd	%xmm10, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm2, %xmm6, %xmm6
	vmovss	%xmm6, -1056(%rbx)
	vmovsd	.LC35(%rip), %xmm6
	vmulsd	%xmm6, %xmm1, %xmm8
	vaddsd	%xmm8, %xmm3, %xmm3
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-4195364(%rbp), %xmm8, %xmm8
	vmulsd	%xmm7, %xmm8, %xmm9
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm3
	vmovss	%xmm2, -1052(%rbx)
	vmovss	-4194860(%rbp), %xmm2
	vaddsd	%xmm3, %xmm0, %xmm3
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm2, %xmm3, %xmm3
	vmovss	%xmm3, -1048(%rbx)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm10, %xmm1
	vaddsd	%xmm8, %xmm10, %xmm10
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm0, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vmovss	%xmm1, -1044(%rbx)
	vmovss	-4194856(%rbp), %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vaddsd	%xmm0, %xmm5, %xmm0
	vsubsd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm1, %xmm2, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	-4194852(%rbp), %xmm1
	vmovss	%xmm2, -1040(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195360(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm11
	vmovss	%xmm0, -1036(%rbx)
	vmulsd	%xmm3, %xmm5, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4195356(%rbp), %xmm5, %xmm5
	vaddsd	%xmm2, %xmm9, %xmm9
	vsubsd	%xmm11, %xmm10, %xmm10
	vaddsd	%xmm0, %xmm8, %xmm0
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm1, %xmm10, %xmm10
	vaddsd	%xmm0, %xmm11, %xmm0
	vaddsd	%xmm5, %xmm11, %xmm11
	vmovss	%xmm10, -1032(%rbx)
	vmovss	-4194848(%rbp), %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm3, %xmm8, %xmm1
	vmovss	-4194844(%rbp), %xmm8
	vmovss	%xmm0, -1028(%rbx)
	vmulsd	%xmm7, %xmm5, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vmulsd	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vmovss	%xmm9, -1024(%rbx)
	vmovss	%xmm1, -1020(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195352(%rbp), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm5, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm9, %xmm2
	vsubsd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm1, %xmm0, %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm8, %xmm11, %xmm11
	vmovss	-4194840(%rbp), %xmm8
	vmovss	%xmm2, -1012(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195348(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm7
	vmovss	%xmm11, -1016(%rbx)
	vmulsd	%xmm6, %xmm2, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm3, %xmm7, %xmm7
	vxorps	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm8, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	%xmm9, -1008(%rbx)
	vmovss	%xmm3, -1004(%rbx)
	vmovss	-4194836(%rbp), %xmm3
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm5, -1000(%rbx)
	vmovss	%xmm0, -996(%rbx)
	cmpq	%r12, %rbx
	je	.L1706
.L1406:
	movq	-32(%rbx), %rax
	movl	$32, %edx
	movq	%rbx, %rsi
	cmpl	$1, -4195428(%rbp)
	leaq	-4194864(%rbp), %rdi
	movq	%rax, -4195376(%rbp)
	movq	-24(%rbx), %rax
	movq	%rax, -4195368(%rbp)
	movq	-16(%rbx), %rax
	movq	%rax, -4195360(%rbp)
	movq	-8(%rbx), %rax
	movq	%rax, -4195352(%rbp)
	ja	.L1400
	call	memcpy
	cmpb	$0, -4195416(%rbp)
	je	.L1707
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	-4195376(%rbp), %xmm2, %xmm2
	vmovss	-4194864(%rbp), %xmm7
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-4195372(%rbp), %xmm14, %xmm14
	vxorpd	%xmm9, %xmm9, %xmm9
	movl	-4194860(%rbp), %r8d
	vcvtss2sd	-4195368(%rbp), %xmm9, %xmm9
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vcvtss2sd	-4195364(%rbp), %xmm13, %xmm13
	movl	-4194856(%rbp), %edi
	movl	-4194852(%rbp), %esi
	vxorps	%xmm11, %xmm11, %xmm11
	vxorps	%xmm8, %xmm8, %xmm8
	vxorps	%xmm4, %xmm4, %xmm4
	movl	-4194848(%rbp), %ecx
	vxorps	%xmm3, %xmm3, %xmm3
	vmovd	%r8d, %xmm6
	movl	-4194844(%rbp), %edx
	movl	-4194840(%rbp), %eax
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%edi, %xmm6
	vmovss	-4194836(%rbp), %xmm15
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm14, %xmm14
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%esi, %xmm6
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubsd	%xmm0, %xmm9, %xmm9
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%ecx, %xmm6
	vmulsd	%xmm5, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmovd	%edx, %xmm6
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm13, %xmm13
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195360(%rbp), %xmm0, %xmm0
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195356(%rbp), %xmm1, %xmm1
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm0, %xmm11, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%eax, %xmm6
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195352(%rbp), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm8, %xmm8
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195348(%rbp), %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm15, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm3, %xmm3
.L1403:
	vmovss	%xmm2, -32(%rbx)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm10
	vmovss	%xmm14, -24(%rbx)
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	addq	$1024, %rbx
	vmovsd	.LC22(%rip), %xmm6
	vmulsd	%xmm10, %xmm2, %xmm0
	vmovss	%xmm9, -1040(%rbx)
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vmulsd	.LC21(%rip), %xmm14, %xmm5
	vmovss	%xmm13, -1032(%rbx)
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vmulsd	%xmm6, %xmm13, %xmm12
	vmovss	%xmm3, -1000(%rbx)
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vmovss	%xmm11, -1024(%rbx)
	vmovss	%xmm4, -1008(%rbx)
	vmovss	%xmm8, -1016(%rbx)
	vaddsd	%xmm5, %xmm0, %xmm5
	vmulsd	%xmm10, %xmm9, %xmm0
	vsubsd	%xmm0, %xmm5, %xmm5
	vaddsd	%xmm12, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm5
	vmovsd	.LC27(%rip), %xmm7
	vmulsd	%xmm5, %xmm14, %xmm0
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovss	%xmm1, -1052(%rbx)
	vmulsd	%xmm5, %xmm9, %xmm1
	vmulsd	%xmm7, %xmm14, %xmm14
	vmulsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovd	%r8d, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovd	%edi, %xmm1
	vsubsd	%xmm12, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm11, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm2, %xmm12
	vmovss	%xmm0, -1044(%rbx)
	vmulsd	%xmm5, %xmm13, %xmm0
	vmulsd	%xmm7, %xmm13, %xmm13
	vmulsd	%xmm7, %xmm2, %xmm7
	vaddsd	%xmm0, %xmm14, %xmm14
	vaddsd	%xmm9, %xmm0, %xmm9
	vsubsd	%xmm12, %xmm14, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	%xmm1, %xmm14, %xmm14
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm1, %xmm0
	vmovss	%xmm14, -1036(%rbx)
	vmulsd	%xmm5, %xmm2, %xmm14
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm14, %xmm9, %xmm9
	vaddsd	%xmm13, %xmm14, %xmm13
	vsubsd	%xmm0, %xmm9, %xmm9
	vmovd	%esi, %xmm0
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vaddss	%xmm0, %xmm9, %xmm9
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm4, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm14
	vmovd	%ecx, %xmm4
	vmulsd	%xmm6, %xmm3, %xmm6
	vmovss	%xmm9, -1028(%rbx)
	vmulsd	%xmm5, %xmm1, %xmm9
	vmulsd	%xmm5, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm0, %xmm11
	vaddsd	%xmm9, %xmm13, %xmm13
	vaddsd	%xmm7, %xmm9, %xmm9
	vmovd	%edx, %xmm7
	vsubsd	%xmm14, %xmm13, %xmm13
	vaddsd	%xmm5, %xmm9, %xmm5
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vaddss	%xmm4, %xmm13, %xmm13
	vsubsd	%xmm6, %xmm5, %xmm5
	vmovss	%xmm13, -1020(%rbx)
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vmovd	%eax, %xmm7
	vmovss	%xmm5, -1012(%rbx)
	vmulsd	%xmm10, %xmm1, %xmm5
	vmulsd	%xmm10, %xmm3, %xmm10
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vsubsd	%xmm5, %xmm12, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm11, %xmm5, %xmm5
	vaddsd	%xmm10, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vmovss	%xmm5, -1004(%rbx)
	vmovsd	.LC30(%rip), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -996(%rbx)
	cmpq	%r12, %rbx
	jne	.L1406
.L1706:
	leaq	262144(%rbx), %r12
	cmpq	%r13, %r12
	jne	.L1399
	movq	-4195472(%rbp), %rax
	xorl	%r15d, %r15d
	movq	%rax, -4195536(%rbp)
.L1407:
	movslq	%r15d, %rdi
	movq	-4195536(%rbp), %rax
	xorl	%esi, %esi
	movl	$15, %r14d
	movq	%rdi, %rcx
	salq	$10, %rdi
	movq	-4195472(%rbp), %rdx
	movl	$8, %r13d
	movl	$7, %r12d
	movl	$6, %ebx
	salq	$8, %rcx
	movq	%rdi, -4195568(%rbp)
	movl	$5, %r11d
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	jmp	.L1410
	.p2align 4,,10
	.p2align 3
.L1708:
	vmovss	4(%rax), %xmm0
	vmovss	262144(%rax), %xmm1
	vmovss	%xmm0, 262144(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %r8d
	ja	.L1409
	movslq	%r8d, %rdi
	vmovss	524288(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 524288(%rax)
	cmpl	$15, %r9d
	ja	.L1409
	movslq	%r9d, %rdi
	vmovss	786432(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 786432(%rax)
	cmpl	$15, %r10d
	ja	.L1409
	movslq	%r10d, %rdi
	vmovss	1048576(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1048576(%rax)
	cmpl	$15, %r11d
	ja	.L1409
	movslq	%r11d, %rdi
	vmovss	1310720(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1310720(%rax)
	cmpl	$15, %ebx
	ja	.L1409
	movslq	%ebx, %rdi
	vmovss	1572864(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1572864(%rax)
	cmpl	$15, %r12d
	ja	.L1409
	movslq	%r12d, %rdi
	vmovss	1835008(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 1835008(%rax)
	cmpl	$15, %r13d
	ja	.L1409
	movslq	%r13d, %rdi
	vmovss	2097152(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	8(%rsi), %edi
	vmovss	%xmm0, 2097152(%rax)
	cmpl	$15, %edi
	ja	.L1409
	movslq	%edi, %rdi
	vmovss	2359296(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	9(%rsi), %edi
	vmovss	%xmm0, 2359296(%rax)
	cmpl	$15, %edi
	ja	.L1409
	movslq	%edi, %rdi
	vmovss	2621440(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	10(%rsi), %edi
	vmovss	%xmm0, 2621440(%rax)
	cmpl	$15, %edi
	ja	.L1409
	movslq	%edi, %rdi
	vmovss	2883584(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	11(%rsi), %edi
	vmovss	%xmm0, 2883584(%rax)
	cmpl	$15, %edi
	ja	.L1409
	movslq	%edi, %rdi
	vmovss	3145728(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	12(%rsi), %edi
	vmovss	%xmm0, 3145728(%rax)
	cmpl	$15, %edi
	ja	.L1409
	movslq	%edi, %rdi
	vmovss	3407872(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	13(%rsi), %edi
	vmovss	%xmm0, 3407872(%rax)
	cmpl	$15, %edi
	ja	.L1409
	movslq	%edi, %rdi
	vmovss	3670016(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 3670016(%rax)
	cmpl	$15, %r14d
	jne	.L1409
	movq	-4195568(%rbp), %rdi
	vmovss	3932160(%rax), %xmm1
	addq	%rdx, %rdi
	vmovss	60(%rdi), %xmm0
	vmovss	%xmm1, 60(%rdi)
	vmovss	%xmm0, 3932160(%rax)
.L1409:
	addl	$1, %r14d
	addq	$262144, %rdx
	addq	$262148, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
	addl	$1, %r13d
.L1410:
	addl	$1, %esi
	cmpl	$16, %esi
	jne	.L1708
	addq	$1024, -4195536(%rbp)
	addl	$1, %r15d
	cmpl	$16, %r15d
	jne	.L1407
	movq	-4195424(%rbp), %rax
	movq	-4195472(%rbp), %r15
	addq	$4194328, %rax
	movq	%rax, -4195536(%rbp)
.L1421:
	leaq	16384(%r15), %r12
	movq	%r15, %rbx
.L1411:
	movl	-4195504(%rbp), %esi
	movq	%rbx, %rdi
	addq	$1024, %rbx
	call	_ZN18WaveletsOnInterval3WI49transformILi16ELb0EEEvPfi
	cmpq	%rbx, %r12
	jne	.L1411
	movq	%r15, %rax
	movq	%r15, %rdx
	movl	$8, %ebx
	movl	$15, %r14d
	movl	$9, %r12d
	xorl	%ecx, %ecx
	movl	$7, %r11d
	movl	$3, %edi
	movl	$2, %esi
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	jmp	.L1560
	.p2align 4,,10
	.p2align 3
.L1709:
	vmovss	4(%rax), %xmm0
	vmovss	1024(%rax), %xmm1
	vmovss	%xmm0, 1024(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %esi
	ja	.L1413
	movslq	%esi, %r13
	vmovss	2048(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 2048(%rax)
	cmpl	$15, %edi
	ja	.L1413
	movslq	%edi, %r13
	vmovss	3072(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 3072(%rax)
	cmpl	$15, %r8d
	ja	.L1413
	movslq	%r8d, %r13
	vmovss	4096(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 4096(%rax)
	cmpl	$15, %r9d
	ja	.L1413
	movslq	%r9d, %r13
	vmovss	5120(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 5120(%rax)
	cmpl	$15, %r10d
	ja	.L1413
	movslq	%r10d, %r13
	vmovss	6144(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 6144(%rax)
	cmpl	$15, %r11d
	ja	.L1413
	movslq	%r11d, %r13
	vmovss	7168(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 7168(%rax)
	cmpl	$15, %ebx
	ja	.L1413
	movslq	%ebx, %r13
	vmovss	8192(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 8192(%rax)
	cmpl	$15, %r12d
	ja	.L1413
	movslq	%r12d, %r13
	vmovss	9216(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	9(%rcx), %r13d
	vmovss	%xmm0, 9216(%rax)
	cmpl	$15, %r13d
	ja	.L1413
	movslq	%r13d, %r13
	vmovss	10240(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	10(%rcx), %r13d
	vmovss	%xmm0, 10240(%rax)
	cmpl	$15, %r13d
	ja	.L1413
	movslq	%r13d, %r13
	vmovss	11264(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	11(%rcx), %r13d
	vmovss	%xmm0, 11264(%rax)
	cmpl	$15, %r13d
	ja	.L1413
	movslq	%r13d, %r13
	vmovss	12288(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	12(%rcx), %r13d
	vmovss	%xmm0, 12288(%rax)
	cmpl	$15, %r13d
	ja	.L1413
	movslq	%r13d, %r13
	vmovss	13312(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	leal	13(%rcx), %r13d
	vmovss	%xmm0, 13312(%rax)
	cmpl	$15, %r13d
	ja	.L1413
	movslq	%r13d, %r13
	vmovss	14336(%rax), %xmm1
	leaq	(%rdx,%r13,4), %r13
	vmovss	0(%r13), %xmm0
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, 14336(%rax)
	cmpl	$15, %r14d
	jne	.L1413
	vmovss	60(%rdx), %xmm0
	vmovss	15360(%rax), %xmm1
	vmovss	%xmm1, 60(%rdx)
	vmovss	%xmm0, 15360(%rax)
.L1413:
	addl	$1, %r14d
	addq	$1024, %rdx
	addq	$1028, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
.L1560:
	addl	$1, %ecx
	cmpl	$16, %ecx
	jne	.L1709
	leaq	32(%r15), %rbx
	leaq	16416(%r15), %r12
	jmp	.L1420
	.p2align 4,,10
	.p2align 3
.L1414:
	call	memcpy
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC36(%rip), %xmm4
	vcvtss2sd	-4195376(%rbp), %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm0, %xmm3
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	%xmm4, %xmm1, %xmm6
	vcvtss2sd	-4195368(%rbp), %xmm5, %xmm5
	vmovsd	.LC34(%rip), %xmm7
	addq	$1024, %rbx
	vmovss	-4194864(%rbp), %xmm2
	vmulsd	%xmm7, %xmm5, %xmm10
	vsubsd	%xmm3, %xmm6, %xmm6
	vaddsd	%xmm10, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm2, %xmm6, %xmm6
	vmovss	%xmm6, -1056(%rbx)
	vmovsd	.LC35(%rip), %xmm6
	vmulsd	%xmm6, %xmm1, %xmm8
	vaddsd	%xmm8, %xmm3, %xmm3
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-4195364(%rbp), %xmm8, %xmm8
	vmulsd	%xmm7, %xmm8, %xmm9
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm3
	vmovss	%xmm2, -1052(%rbx)
	vmovss	-4194860(%rbp), %xmm2
	vaddsd	%xmm3, %xmm0, %xmm3
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm2, %xmm3, %xmm3
	vmovss	%xmm3, -1048(%rbx)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm10, %xmm1
	vaddsd	%xmm8, %xmm10, %xmm10
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm0, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vmovss	%xmm1, -1044(%rbx)
	vmovss	-4194856(%rbp), %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vaddsd	%xmm0, %xmm5, %xmm0
	vsubsd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm1, %xmm2, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	-4194852(%rbp), %xmm1
	vmovss	%xmm2, -1040(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195360(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm11
	vmovss	%xmm0, -1036(%rbx)
	vmulsd	%xmm3, %xmm5, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4195356(%rbp), %xmm5, %xmm5
	vaddsd	%xmm2, %xmm9, %xmm9
	vsubsd	%xmm11, %xmm10, %xmm10
	vaddsd	%xmm0, %xmm8, %xmm0
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm1, %xmm10, %xmm10
	vaddsd	%xmm0, %xmm11, %xmm0
	vaddsd	%xmm5, %xmm11, %xmm11
	vmovss	%xmm10, -1032(%rbx)
	vmovss	-4194848(%rbp), %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmulsd	%xmm3, %xmm8, %xmm1
	vmovss	-4194844(%rbp), %xmm8
	vmovss	%xmm0, -1028(%rbx)
	vmulsd	%xmm7, %xmm5, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vmulsd	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vmovss	%xmm9, -1024(%rbx)
	vmovss	%xmm1, -1020(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195352(%rbp), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm5, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm9, %xmm2
	vsubsd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm1, %xmm0, %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm8, %xmm11, %xmm11
	vmovss	-4194840(%rbp), %xmm8
	vmovss	%xmm2, -1012(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195348(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm7
	vmovss	%xmm11, -1016(%rbx)
	vmulsd	%xmm6, %xmm2, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm3, %xmm7, %xmm7
	vxorps	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm8, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	%xmm9, -1008(%rbx)
	vmovss	%xmm3, -1004(%rbx)
	vmovss	-4194836(%rbp), %xmm3
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm5, -1000(%rbx)
	vmovss	%xmm0, -996(%rbx)
	cmpq	%rbx, %r12
	je	.L1710
.L1420:
	movq	-32(%rbx), %rax
	movl	$32, %edx
	movq	%rbx, %rsi
	cmpl	$1, -4195428(%rbp)
	leaq	-4194864(%rbp), %rdi
	movq	%rax, -4195376(%rbp)
	movq	-24(%rbx), %rax
	movq	%rax, -4195368(%rbp)
	movq	-16(%rbx), %rax
	movq	%rax, -4195360(%rbp)
	movq	-8(%rbx), %rax
	movq	%rax, -4195352(%rbp)
	ja	.L1414
	call	memcpy
	cmpb	$0, -4195416(%rbp)
	je	.L1711
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	-4195376(%rbp), %xmm2, %xmm2
	vmovss	-4194864(%rbp), %xmm7
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-4195372(%rbp), %xmm14, %xmm14
	vxorpd	%xmm9, %xmm9, %xmm9
	movl	-4194860(%rbp), %r8d
	vcvtss2sd	-4195368(%rbp), %xmm9, %xmm9
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	%xmm7, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vcvtss2sd	-4195364(%rbp), %xmm13, %xmm13
	movl	-4194856(%rbp), %edi
	movl	-4194852(%rbp), %esi
	vxorps	%xmm11, %xmm11, %xmm11
	vxorps	%xmm8, %xmm8, %xmm8
	vxorps	%xmm4, %xmm4, %xmm4
	movl	-4194848(%rbp), %ecx
	vxorps	%xmm3, %xmm3, %xmm3
	vmovd	%r8d, %xmm6
	movl	-4194844(%rbp), %edx
	movl	-4194840(%rbp), %eax
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%edi, %xmm6
	vmovss	-4194836(%rbp), %xmm15
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm14, %xmm14
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%esi, %xmm6
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubsd	%xmm0, %xmm9, %xmm9
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%ecx, %xmm6
	vmulsd	%xmm5, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmovd	%edx, %xmm6
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubsd	%xmm0, %xmm13, %xmm13
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195360(%rbp), %xmm0, %xmm0
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195356(%rbp), %xmm1, %xmm1
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm0, %xmm11, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%eax, %xmm6
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195352(%rbp), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm8, %xmm8
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195348(%rbp), %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm15, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm3, %xmm3
.L1417:
	vmovss	%xmm2, -32(%rbx)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm10
	vmovss	%xmm14, -24(%rbx)
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	addq	$1024, %rbx
	vmovsd	.LC22(%rip), %xmm6
	vmulsd	%xmm10, %xmm2, %xmm0
	vmovss	%xmm9, -1040(%rbx)
	vcvtss2sd	%xmm9, %xmm9, %xmm9
	vmulsd	.LC21(%rip), %xmm14, %xmm5
	vmovss	%xmm13, -1032(%rbx)
	vcvtss2sd	%xmm13, %xmm13, %xmm13
	vmulsd	%xmm6, %xmm13, %xmm12
	vmovss	%xmm3, -1000(%rbx)
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vmovss	%xmm11, -1024(%rbx)
	vmovss	%xmm4, -1008(%rbx)
	vmovss	%xmm8, -1016(%rbx)
	vaddsd	%xmm5, %xmm0, %xmm5
	vmulsd	%xmm10, %xmm9, %xmm0
	vsubsd	%xmm0, %xmm5, %xmm5
	vaddsd	%xmm12, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm5
	vmovsd	.LC27(%rip), %xmm7
	vmulsd	%xmm5, %xmm14, %xmm0
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovss	%xmm1, -1052(%rbx)
	vmulsd	%xmm5, %xmm9, %xmm1
	vmulsd	%xmm7, %xmm14, %xmm14
	vmulsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovd	%r8d, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovd	%edi, %xmm1
	vsubsd	%xmm12, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm11, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm2, %xmm12
	vmovss	%xmm0, -1044(%rbx)
	vmulsd	%xmm5, %xmm13, %xmm0
	vmulsd	%xmm7, %xmm13, %xmm13
	vmulsd	%xmm7, %xmm2, %xmm7
	vaddsd	%xmm0, %xmm14, %xmm14
	vaddsd	%xmm9, %xmm0, %xmm9
	vsubsd	%xmm12, %xmm14, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	%xmm1, %xmm14, %xmm14
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm8, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm1, %xmm0
	vmovss	%xmm14, -1036(%rbx)
	vmulsd	%xmm5, %xmm2, %xmm14
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm14, %xmm9, %xmm9
	vaddsd	%xmm13, %xmm14, %xmm13
	vsubsd	%xmm0, %xmm9, %xmm9
	vmovd	%esi, %xmm0
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vaddss	%xmm0, %xmm9, %xmm9
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm4, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm14
	vmovd	%ecx, %xmm4
	vmulsd	%xmm6, %xmm3, %xmm6
	vmovss	%xmm9, -1028(%rbx)
	vmulsd	%xmm5, %xmm1, %xmm9
	vmulsd	%xmm5, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm0, %xmm11
	vaddsd	%xmm9, %xmm13, %xmm13
	vaddsd	%xmm7, %xmm9, %xmm9
	vmovd	%edx, %xmm7
	vsubsd	%xmm14, %xmm13, %xmm13
	vaddsd	%xmm5, %xmm9, %xmm5
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vaddss	%xmm4, %xmm13, %xmm13
	vsubsd	%xmm6, %xmm5, %xmm5
	vmovss	%xmm13, -1020(%rbx)
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vmovd	%eax, %xmm7
	vmovss	%xmm5, -1012(%rbx)
	vmulsd	%xmm10, %xmm1, %xmm5
	vmulsd	%xmm10, %xmm3, %xmm10
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vsubsd	%xmm5, %xmm12, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm11, %xmm5, %xmm5
	vaddsd	%xmm10, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm7, %xmm5, %xmm5
	vmovss	%xmm5, -1004(%rbx)
	vmovsd	.LC30(%rip), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -996(%rbx)
	cmpq	%rbx, %r12
	jne	.L1420
.L1710:
	addq	$262144, %r15
	cmpq	-4195536(%rbp), %r15
	jne	.L1421
	movq	-4195424(%rbp), %rax
	movq	-4195472(%rbp), %r13
	leaq	8388632(%rax), %r12
.L1422:
	leaq	32768(%r13), %r14
	movq	%r13, %rbx
.L1423:
	movl	-4195504(%rbp), %esi
	movq	%rbx, %rdi
	addq	$1024, %rbx
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi
	cmpq	%rbx, %r14
	jne	.L1423
	addq	$262144, %r13
	cmpq	%r12, %r13
	jne	.L1422
	movq	-4195424(%rbp), %rax
	xorl	%r8d, %r8d
	addq	$262168, %rax
	movq	%rax, -4195440(%rbp)
	movq	%rax, %r10
.L1429:
	xorl	%r9d, %r9d
	movslq	%r8d, %rdi
	movq	-4195472(%rbp), %rsi
	movq	%r10, %r11
	addl	$1, %r9d
	salq	$8, %rdi
	cmpl	$32, %r9d
	je	.L1425
.L1712:
	movq	%r11, %rcx
	movl	%r9d, %edx
	.p2align 4,,10
	.p2align 3
.L1426:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$32, %edx
	jne	.L1426
	addl	$1, %r9d
	addq	$262148, %r11
	addq	$262144, %rsi
	cmpl	$32, %r9d
	jne	.L1712
.L1425:
	addl	$1, %r8d
	addq	$1024, %r10
	cmpl	$32, %r8d
	jne	.L1429
	movq	-4195472(%rbp), %r13
.L1434:
	leaq	32768(%r13), %rbx
	movq	%r13, %r14
.L1430:
	movl	-4195504(%rbp), %esi
	movq	%r14, %rdi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi
	cmpq	%r14, %rbx
	jne	.L1430
	leaq	1024(%r13), %r8
	xorl	%edi, %edi
	movq	%r13, %rsi
	addl	$1, %edi
	cmpl	$32, %edi
	je	.L1568
.L1713:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L1432:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$32, %eax
	jne	.L1432
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$32, %edi
	jne	.L1713
.L1568:
	movq	%r13, %r14
.L1431:
	movl	-4195504(%rbp), %esi
	movq	%r14, %rdi
	addq	$1024, %r14
	call	_ZN18WaveletsOnInterval3WI49transformILi32ELb0EEEvPfi
	cmpq	%rbx, %r14
	jne	.L1431
	addq	$262144, %r13
	cmpq	%r12, %r13
	jne	.L1434
	movq	-4195424(%rbp), %rax
	movq	-4195472(%rbp), %r14
	movl	-4195428(%rbp), %r12d
	vmovsd	.LC33(%rip), %xmm11
	vmovapd	.LC38(%rip), %ymm5
	addq	$16777240, %rax
	vmovapd	.LC39(%rip), %ymm9
	movq	%rax, -4195504(%rbp)
	vmovapd	.LC31(%rip), %ymm10
.L1435:
	leaq	128(%r14), %r13
	movq	%r14, %rbx
	movl	$64, %r15d
	jmp	.L1445
.L1436:
	shrq	$2, %rax
	jne	.L1714
.L1444:
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195376(%rbp), %xmm3, %xmm3
	vmovsd	.LC36(%rip), %xmm8
	vcvtss2sd	-4195372(%rbp), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC35(%rip), %xmm12
	vmulsd	%xmm8, %xmm3, %xmm4
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmovsd	.LC34(%rip), %xmm13
	vmulsd	%xmm12, %xmm3, %xmm3
	vmovss	-4194864(%rbp), %xmm0
	vmulsd	%xmm13, %xmm1, %xmm1
	vsubsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm2, %xmm1
	vmovaps	-4195376(%rbp), %ymm2
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm0, %xmm4, %xmm4
	vcvtps2pd	%xmm2, %ymm15
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovups	-4195372(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm6
	vmulpd	%ymm5, %ymm15, %ymm2
	vmulpd	%ymm9, %ymm15, %ymm15
	vcvtps2pd	%xmm1, %ymm14
	vmovss	%xmm4, (%rbx)
	vextractf128	$0x1, %ymm1, %xmm1
	vaddpd	%ymm14, %ymm2, %ymm2
	vmovss	%xmm0, 4(%rbx)
	vcvtps2pd	%xmm1, %ymm4
	vaddpd	%ymm15, %ymm14, %ymm14
	vmovups	-4195368(%rbp), %ymm0
	vmovups	-4194860(%rbp), %ymm1
	vcvtps2pd	%xmm0, %ymm7
	vextractf128	$0x1, %ymm0, %xmm3
	vmulpd	%ymm5, %ymm6, %ymm0
	vaddpd	%ymm4, %ymm0, %ymm0
	vmulpd	%ymm5, %ymm7, %ymm7
	vcvtps2pd	%xmm3, %ymm3
	vsubpd	%ymm7, %ymm2, %ymm2
	vmulpd	%ymm5, %ymm3, %ymm3
	vaddpd	%ymm14, %ymm7, %ymm7
	vsubpd	%ymm3, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm1, %ymm0, %ymm2
	vmulpd	%ymm9, %ymm6, %ymm0
	vaddpd	%ymm0, %ymm4, %ymm0
	vcvtpd2psy	%ymm7, %xmm4
	vaddpd	%ymm0, %ymm3, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vaddps	%ymm0, %ymm1, %ymm0
	vunpcklps	%ymm0, %ymm2, %ymm1
	vunpckhps	%ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm2, 8(%rbx)
	vmovups	-4195340(%rbp), %ymm1
	vmovaps	-4195344(%rbp), %ymm2
	vmovups	%ymm0, 40(%rbx)
	vmovups	-4195336(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm14
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vcvtps2pd	%xmm2, %ymm15
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm6
	vmulpd	%ymm5, %ymm15, %ymm2
	vcvtps2pd	%xmm0, %ymm7
	vaddpd	%ymm14, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm3
	vmulpd	%ymm5, %ymm6, %ymm0
	vaddpd	%ymm4, %ymm0, %ymm0
	vmulpd	%ymm5, %ymm7, %ymm7
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm5, %ymm3, %ymm3
	vmovups	-4194828(%rbp), %ymm1
	vmulpd	%ymm9, %ymm15, %ymm15
	vaddpd	%ymm15, %ymm14, %ymm14
	vsubpd	%ymm7, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm14, %ymm7, %ymm7
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm1, %ymm0, %ymm2
	vmulpd	%ymm9, %ymm6, %ymm0
	vaddpd	%ymm0, %ymm4, %ymm0
	vcvtpd2psy	%ymm7, %xmm4
	vaddpd	%ymm0, %ymm3, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vaddps	%ymm0, %ymm1, %ymm0
	vunpcklps	%ymm0, %ymm2, %ymm1
	vunpckhps	%ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm2, 72(%rbx)
	vmovups	-4195308(%rbp), %ymm1
	vmovaps	-4195312(%rbp), %ymm2
	vmovups	%ymm0, 104(%rbx)
	vmovups	-4195304(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm14
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vcvtps2pd	%xmm2, %ymm15
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm6
	vmulpd	%ymm5, %ymm15, %ymm2
	vcvtps2pd	%xmm0, %ymm7
	vaddpd	%ymm14, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm3
	vmulpd	%ymm5, %ymm6, %ymm0
	vaddpd	%ymm4, %ymm0, %ymm0
	vmulpd	%ymm5, %ymm7, %ymm7
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm5, %ymm3, %ymm3
	vmovups	-4194796(%rbp), %ymm1
	vmulpd	%ymm9, %ymm15, %ymm15
	vaddpd	%ymm15, %ymm14, %ymm14
	vsubpd	%ymm7, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm14, %ymm7, %ymm7
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm1, %ymm0, %ymm2
	vmulpd	%ymm9, %ymm6, %ymm0
	vaddpd	%ymm0, %ymm4, %ymm0
	vcvtpd2psy	%ymm7, %xmm4
	vaddpd	%ymm0, %ymm3, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm4, %ymm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vaddps	%ymm0, %ymm1, %ymm0
	vcvtss2sd	-4195280(%rbp), %xmm4, %xmm4
	vmulsd	%xmm13, %xmm4, %xmm3
	vunpcklps	%ymm0, %ymm2, %ymm1
	vunpckhps	%ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm0, 168(%rbx)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195272(%rbp), %xmm0, %xmm0
	vmulsd	%xmm13, %xmm0, %xmm6
	vmovups	%ymm2, 136(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195276(%rbp), %xmm2, %xmm2
	vmovss	-4194764(%rbp), %xmm1
	vaddsd	%xmm2, %xmm3, %xmm3
	vmulsd	%xmm13, %xmm2, %xmm14
	vsubsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm14, %xmm0, %xmm14
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm1, %xmm3, %xmm3
	vmovss	%xmm3, 200(%rbx)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm2, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm6, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	%xmm1, 204(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195268(%rbp), %xmm1, %xmm1
	vmulsd	%xmm13, %xmm1, %xmm7
	vmovss	-4194760(%rbp), %xmm4
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm6, %xmm6
	vmulsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm7, %xmm14, %xmm14
	vaddsd	%xmm2, %xmm7, %xmm2
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm4, %xmm14, %xmm14
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	-4194756(%rbp), %xmm4
	vmovss	%xmm14, 208(%rbx)
	vmovss	%xmm2, 212(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195264(%rbp), %xmm2, %xmm2
	vmulsd	%xmm13, %xmm2, %xmm14
	vaddsd	%xmm2, %xmm7, %xmm7
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm14, %xmm0
	vsubsd	%xmm14, %xmm6, %xmm6
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm4, %xmm6, %xmm6
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195260(%rbp), %xmm4, %xmm4
	vaddsd	%xmm2, %xmm4, %xmm2
	vmovss	%xmm0, 220(%rbx)
	vmulsd	%xmm13, %xmm4, %xmm0
	vaddsd	%xmm4, %xmm14, %xmm14
	vmovss	%xmm6, 216(%rbx)
	vmovss	-4194752(%rbp), %xmm6
	vmulsd	%xmm3, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm7, %xmm7
	vaddsd	%xmm1, %xmm0, %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm6, %xmm7, %xmm7
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm6, %xmm1
	vmovss	-4194748(%rbp), %xmm6
	vmovss	%xmm7, 224(%rbx)
	vmovss	%xmm1, 228(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195256(%rbp), %xmm1, %xmm1
	vmulsd	%xmm13, %xmm1, %xmm7
	vaddsd	%xmm4, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm7, %xmm2
	vsubsd	%xmm7, %xmm14, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm7
	vmulsd	%xmm11, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubss	%xmm6, %xmm14, %xmm14
	vmovss	-4194744(%rbp), %xmm6
	vmovss	%xmm2, 236(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195252(%rbp), %xmm2, %xmm2
	vmulsd	%xmm13, %xmm2, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm12
	vaddsd	%xmm1, %xmm4, %xmm4
	vmovss	%xmm14, 232(%rbx)
	vmulsd	%xmm8, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm13, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm13, %xmm13
	vxorps	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm12, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm13, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vsubss	%xmm6, %xmm7, %xmm7
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm3, 244(%rbx)
	vmovss	-4194740(%rbp), %xmm3
	vmovss	%xmm7, 240(%rbx)
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm4, 248(%rbx)
	vmovss	%xmm0, 252(%rbx)
.L1443:
	addq	$1024, %rbx
	addq	$1024, %r13
	subl	$1, %r15d
	je	.L1715
.L1445:
	movq	(%rbx), %rax
	leaq	256(%rbx), %rdx
	subq	%r13, %rdx
	movq	%rax, -4195376(%rbp)
	movq	8(%rbx), %rax
	movq	%rax, -4195368(%rbp)
	movq	16(%rbx), %rax
	movq	%rax, -4195360(%rbp)
	movq	24(%rbx), %rax
	movq	%rax, -4195352(%rbp)
	movq	32(%rbx), %rax
	movq	%rax, -4195344(%rbp)
	movq	40(%rbx), %rax
	movq	%rax, -4195336(%rbp)
	movq	48(%rbx), %rax
	movq	%rax, -4195328(%rbp)
	movq	56(%rbx), %rax
	movq	%rax, -4195320(%rbp)
	movq	64(%rbx), %rax
	movq	%rax, -4195312(%rbp)
	movq	72(%rbx), %rax
	movq	%rax, -4195304(%rbp)
	movq	80(%rbx), %rax
	movq	%rax, -4195296(%rbp)
	movq	88(%rbx), %rax
	movq	%rax, -4195288(%rbp)
	movq	96(%rbx), %rax
	movq	%rax, -4195280(%rbp)
	movq	104(%rbx), %rax
	movq	%rax, -4195272(%rbp)
	movq	112(%rbx), %rax
	movq	%rax, -4195264(%rbp)
	movq	120(%rbx), %rax
	movq	%rax, -4195256(%rbp)
	movq	%rdx, %rax
	cmpl	$1, %r12d
	ja	.L1436
	shrq	$2, %rax
	jne	.L1716
.L1437:
	cmpb	$0, -4195416(%rbp)
	je	.L1440
	vmovaps	-4195376(%rbp), %ymm1
	vmovaps	-4194864(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	%ymm10, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vmulpd	%ymm10, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195344(%rbp), %ymm1
	vmovaps	%ymm0, -4195376(%rbp)
	vmovaps	-4194832(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	%ymm10, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vmulpd	%ymm10, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195312(%rbp), %ymm1
	vmovaps	%ymm0, -4195344(%rbp)
	vmovaps	-4194800(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	%ymm10, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vmulpd	%ymm10, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195280(%rbp), %ymm1
	vmovaps	%ymm0, -4195312(%rbp)
	vmovaps	-4194768(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	%ymm10, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vmulpd	%ymm10, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195280(%rbp)
.L1440:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1439:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, (%rbx,%rax,2)
	addq	$4, %rax
	cmpq	$128, %rax
	jne	.L1439
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm7
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC21(%rip), %xmm8
	vcvtss2sd	-4195368(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm1
	movl	$1, %edi
	vxorpd	%xmm12, %xmm12, %xmm12
	vmovsd	.LC22(%rip), %xmm4
	vmulsd	%xmm8, %xmm0, %xmm0
	movl	$30, %r8d
	vmovsd	.LC26(%rip), %xmm3
	vxorpd	%xmm14, %xmm14, %xmm14
	vmulsd	%xmm7, %xmm2, %xmm2
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-4195372(%rbp), %xmm12, %xmm12
	vcvtss2sd	-4195368(%rbp), %xmm14, %xmm14
	leaq	-4195376(%rbp), %rax
	vcvtss2sd	-4195376(%rbp), %xmm13, %xmm13
	vmovsd	.LC27(%rip), %xmm6
	leaq	12(%rax), %rcx
	leaq	-4194864(%rbp), %rax
	vaddsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195364(%rbp), %xmm0, %xmm0
	vmulsd	%xmm4, %xmm0, %xmm0
	leaq	4(%rax), %rdx
	leaq	12(%rbx), %rax
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 4(%rbx)
.L1441:
	vmulsd	%xmm6, %xmm13, %xmm13
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rcx), %xmm0, %xmm0
	addl	$4, %edi
	vmulsd	%xmm3, %xmm12, %xmm2
	movl	%r8d, %esi
	addq	$16, %rcx
	vmulsd	%xmm3, %xmm14, %xmm1
	subl	%edi, %esi
	addq	$16, %rdx
	vmulsd	%xmm6, %xmm12, %xmm12
	addq	$32, %rax
	vmulsd	%xmm6, %xmm14, %xmm14
	vaddsd	%xmm2, %xmm13, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm13
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm12, %xmm1
	vsubsd	%xmm13, %xmm2, %xmm2
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-12(%rcx), %xmm13, %xmm13
	vmulsd	%xmm4, %xmm13, %xmm12
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-16(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -32(%rax)
	vmulsd	%xmm3, %xmm0, %xmm2
	vmulsd	%xmm6, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm14, %xmm2, %xmm2
	vsubsd	%xmm12, %xmm1, %xmm1
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-8(%rcx), %xmm12, %xmm12
	vmulsd	%xmm4, %xmm12, %xmm14
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-12(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -24(%rax)
	vmulsd	%xmm3, %xmm13, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm1
	vmulsd	%xmm3, %xmm12, %xmm0
	vsubsd	%xmm14, %xmm2, %xmm2
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-4(%rcx), %xmm14, %xmm14
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-8(%rdx), %xmm2, %xmm2
	vaddsd	%xmm0, %xmm1, %xmm0
	vmovss	%xmm2, -16(%rax)
	vmulsd	%xmm4, %xmm14, %xmm2
	vsubsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	$29, %edi
	jne	.L1441
	leaq	236(%rbx), %rdx
	addl	$29, %esi
	movl	$29, %eax
	vmovapd	%xmm4, %xmm12
	.p2align 4,,10
	.p2align 3
.L1442:
	leal	-1(%rax), %edi
	vxorpd	%xmm1, %xmm1, %xmm1
	addq	$8, %rdx
	leal	1(%rax), %ecx
	movslq	%edi, %rdi
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm6, %xmm1, %xmm2
	movslq	%eax, %rdi
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm0
	movslq	%ecx, %r8
	addl	$2, %eax
	cltq
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp,%r8,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rax,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%ecx, %eax
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rdi,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rdx)
	cmpl	%esi, %ecx
	jne	.L1442
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195264(%rbp), %xmm3, %xmm3
	vcvtss2sd	-4195260(%rbp), %xmm0, %xmm0
	vmulsd	%xmm12, %xmm3, %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195256(%rbp), %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm0, %xmm6
	vcvtss2sd	-4195252(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm2, %xmm8
	vmulsd	%xmm7, %xmm1, %xmm7
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vmulsd	.LC28(%rip), %xmm3, %xmm3
	vsubsd	%xmm6, %xmm4, %xmm4
	vaddsd	%xmm8, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm3
	vmovsd	.LC30(%rip), %xmm0
	vaddsd	%xmm7, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-4194744(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 244(%rbx)
	vmulsd	%xmm0, %xmm2, %xmm4
	vmulsd	%xmm0, %xmm1, %xmm0
	vsubsd	%xmm4, %xmm3, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194740(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 252(%rbx)
	jmp	.L1443
.L1702:
	orq	-4194352(%rbp,%rcx,8), %rdx
	jmp	.L1369
.L1705:
	xorl	%r14d, %r14d
	xorl	%r12d, %r12d
.L1559:
	movq	-4195424(%rbp), %rax
	movq	%r12, %rdx
	movq	%r14, %rdi
	leaq	69206048(%rax), %rsi
	call	memcpy
	jmp	.L1375
.L1699:
	orq	-4194352(%rbp,%r13,8), %rcx
	jmp	.L1363
.L1698:
	orq	-4194352(%rbp,%r13,8), %rcx
	jmp	.L1361
.L1697:
	orq	-4194352(%rbp,%r13,8), %rcx
	jmp	.L1359
.L1696:
	orq	-4194352(%rbp,%r13,8), %rcx
	jmp	.L1357
.L1701:
	orq	-4194352(%rbp,%r13,8), %rcx
	jmp	.L1367
.L1700:
	orq	-4194352(%rbp,%r13,8), %rcx
	jmp	.L1365
.L1704:
	movabsq	$4611686018427387903, %rax
	cmpq	%rax, %rbx
	ja	.L1717
	leaq	0(,%rbx,4), %r12
	movq	%r12, %rdi
	call	_Znwm
	movq	%r12, %rdx
	xorl	%esi, %esi
	leaq	(%rax,%r12), %r8
	movq	%rax, %rdi
	movq	%rax, %r14
	movq	%rax, -4195408(%rbp)
	movq	%r8, -4195392(%rbp)
	movq	%r8, -4195472(%rbp)
	call	memset
	movq	-4195472(%rbp), %r8
	movq	%r8, -4195400(%rbp)
	testb	%r15b, %r15b
	je	.L1559
	leaq	(%rbx,%rbx), %rdx
	movq	%rsp, %r12
	leaq	16(%rdx), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	movq	-4195424(%rbp), %rax
	movq	%rsp, %rdi
	leaq	69206048(%rax), %rsi
	call	memcpy
	movq	%rsp, %rcx
	testl	%r13d, %r13d
	jle	.L1386
	movq	%r14, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpl	%r13d, %edx
	cmova	%r13d, %edx
	cmpl	$16, %r13d
	cmovbe	%r13d, %edx
	testl	%edx, %edx
	je	.L1566
	xorl	%edi, %edi
.L1379:
	movzwl	(%rcx,%rdi,2), %esi
	movl	%esi, %eax
	andl	$1023, %eax
	sall	$13, %eax
	movl	%eax, %r8d
	movl	%esi, %eax
	andl	$31744, %esi
	andw	$-32768, %ax
	sall	$13, %esi
	movzwl	%ax, %eax
	addl	$939524096, %esi
	sall	$16, %eax
	orl	%r8d, %eax
	orl	%esi, %eax
	movl	%eax, (%r14,%rdi,4)
	leal	1(%rdi), %esi
	addq	$1, %rdi
	cmpl	%edi, %edx
	ja	.L1379
	cmpl	%r13d, %edx
	je	.L1386
.L1378:
	leal	-1(%r13), %eax
	movl	%r13d, %r8d
	movl	%edx, %r10d
	subl	%edx, %r8d
	subl	%edx, %eax
	leal	-16(%r8), %edi
	shrl	$4, %edi
	addl	$1, %edi
	movl	%edi, %r9d
	sall	$4, %r9d
	cmpl	$14, %eax
	jbe	.L1381
	vmovdqa	.LC69(%rip), %ymm6
	leaq	(%rcx,%r10,2), %r11
	xorl	%eax, %eax
	xorl	%edx, %edx
	vmovdqa	.LC70(%rip), %ymm5
	leaq	(%r14,%r10,4), %r10
	vmovdqa	.LC71(%rip), %ymm4
	vmovdqa	.LC72(%rip), %ymm3
.L1382:
	vmovdqu	(%r11,%rax), %ymm0
	addl	$1, %edx
	vpand	%ymm0, %ymm6, %ymm8
	vpand	%ymm0, %ymm5, %ymm1
	vpmovzxwd	%xmm8, %ymm7
	vpand	%ymm0, %ymm4, %ymm0
	vpmovzxwd	%xmm0, %ymm2
	vpslld	$16, %ymm7, %ymm7
	vextracti128	$0x1, %ymm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0
	vpslld	$13, %ymm2, %ymm2
	vpor	%ymm7, %ymm2, %ymm2
	vpmovzxwd	%xmm1, %ymm7
	vextracti128	$0x1, %ymm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1
	vpsrad	$10, %ymm7, %ymm7
	vpaddd	%ymm3, %ymm7, %ymm7
	vpsrad	$10, %ymm1, %ymm1
	vpslld	$23, %ymm7, %ymm7
	vpaddd	%ymm3, %ymm1, %ymm1
	vpor	%ymm7, %ymm2, %ymm2
	vmovaps	%ymm2, (%r10,%rax,2)
	vextracti128	$0x1, %ymm8, %xmm2
	vpmovzxwd	%xmm2, %ymm2
	vpslld	$13, %ymm0, %ymm0
	vpslld	$16, %ymm2, %ymm2
	vpslld	$23, %ymm1, %ymm1
	vpor	%ymm2, %ymm0, %ymm0
	vpor	%ymm1, %ymm0, %ymm0
	vmovaps	%ymm0, 32(%r10,%rax,2)
	addq	$32, %rax
	cmpl	%edi, %edx
	jb	.L1382
	addl	%r9d, %esi
	cmpl	%r8d, %r9d
	je	.L1386
.L1381:
	movslq	%esi, %rax
	leaq	(%r14,%rax,4), %rdi
.L1384:
	movzwl	(%rcx,%rax,2), %edx
	addl	$1, %esi
	addq	$4, %rdi
	movl	%edx, %eax
	andl	$1023, %eax
	sall	$13, %eax
	movl	%eax, %r8d
	movl	%edx, %eax
	andl	$31744, %edx
	andw	$-32768, %ax
	sall	$13, %edx
	movzwl	%ax, %eax
	addl	$939524096, %edx
	sall	$16, %eax
	orl	%r8d, %eax
	orl	%edx, %eax
	movl	%eax, -4(%rdi)
	cmpl	%esi, -4195416(%rbp)
	jle	.L1386
	movslq	%esi, %rax
	jmp	.L1384
.L1566:
	xorl	%esi, %esi
	jmp	.L1378
.L1717:
	call	_ZSt17__throw_bad_allocv
	.p2align 4,,10
	.p2align 3
.L1707:
	vmovss	-4195376(%rbp), %xmm2
	vmovss	-4195372(%rbp), %xmm14
	vmovss	-4195368(%rbp), %xmm9
	vmovss	-4195364(%rbp), %xmm13
	vmovss	-4195360(%rbp), %xmm11
	vmovss	-4195356(%rbp), %xmm8
	vmovss	-4195352(%rbp), %xmm4
	vmovss	-4195348(%rbp), %xmm3
	vmovss	-4194864(%rbp), %xmm7
	movl	-4194860(%rbp), %r8d
	movl	-4194856(%rbp), %edi
	movl	-4194852(%rbp), %esi
	movl	-4194848(%rbp), %ecx
	movl	-4194844(%rbp), %edx
	movl	-4194840(%rbp), %eax
	vmovss	-4194836(%rbp), %xmm15
	jmp	.L1403
.L1711:
	vmovss	-4195376(%rbp), %xmm2
	vmovss	-4195372(%rbp), %xmm14
	vmovss	-4195368(%rbp), %xmm9
	vmovss	-4195364(%rbp), %xmm13
	vmovss	-4195360(%rbp), %xmm11
	vmovss	-4195356(%rbp), %xmm8
	vmovss	-4195352(%rbp), %xmm4
	vmovss	-4195348(%rbp), %xmm3
	vmovss	-4194864(%rbp), %xmm7
	movl	-4194860(%rbp), %r8d
	movl	-4194856(%rbp), %edi
	movl	-4194852(%rbp), %esi
	movl	-4194848(%rbp), %ecx
	movl	-4194844(%rbp), %edx
	movl	-4194840(%rbp), %eax
	vmovss	-4194836(%rbp), %xmm15
	jmp	.L1417
.L1714:
	movq	%r13, %rsi
	vmovapd	%ymm10, -4195632(%rbp)
	leaq	-4194864(%rbp), %rdi
	vmovapd	%ymm9, -4195600(%rbp)
	vmovapd	%ymm5, -4195568(%rbp)
	vmovsd	%xmm11, -4195536(%rbp)
	call	memcpy
	vmovapd	-4195632(%rbp), %ymm10
	vmovapd	-4195600(%rbp), %ymm9
	vmovapd	-4195568(%rbp), %ymm5
	vmovsd	-4195536(%rbp), %xmm11
	jmp	.L1444
.L1716:
	movq	%r13, %rsi
	vmovapd	%ymm10, -4195632(%rbp)
	leaq	-4194864(%rbp), %rdi
	vmovapd	%ymm9, -4195600(%rbp)
	vmovapd	%ymm5, -4195568(%rbp)
	vmovsd	%xmm11, -4195536(%rbp)
	call	memcpy
	vmovapd	-4195632(%rbp), %ymm10
	vmovapd	-4195600(%rbp), %ymm9
	vmovapd	-4195568(%rbp), %ymm5
	vmovsd	-4195536(%rbp), %xmm11
	jmp	.L1437
.L1715:
	addq	$262144, %r14
	cmpq	-4195504(%rbp), %r14
	jne	.L1435
	movq	-4195440(%rbp), %r11
	xorl	%r10d, %r10d
.L1446:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4195472(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$64, %r8d
	je	.L1447
.L1718:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L1448:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$64, %edx
	jne	.L1448
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$64, %r8d
	jne	.L1718
.L1447:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$64, %r10d
	jne	.L1446
	movq	-4195472(%rbp), %r15
	movl	-4195428(%rbp), %ebx
	vmovapd	.LC38(%rip), %ymm12
	vmovapd	.LC39(%rip), %ymm15
	vmovapd	%ymm12, %ymm13
.L1473:
	leaq	128(%r15), %r14
	movq	%r15, %r13
	movl	$64, %r12d
	vmovsd	.LC33(%rip), %xmm8
	movq	%r14, %rcx
	movq	%r14, -4195536(%rbp)
	movl	%ebx, %r14d
	movq	%rcx, %rbx
	jmp	.L1460
.L1451:
	shrq	$2, %rax
	jne	.L1719
.L1459:
	vxorpd	%xmm7, %xmm7, %xmm7
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp), %xmm7, %xmm7
	vmovsd	.LC36(%rip), %xmm5
	vcvtss2sd	-4195372(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vmovsd	.LC35(%rip), %xmm4
	vmulsd	%xmm5, %xmm7, %xmm2
	vcvtss2sd	-4195368(%rbp), %xmm6, %xmm6
	vmovsd	.LC34(%rip), %xmm0
	vmulsd	%xmm4, %xmm7, %xmm7
	vmovss	-4194864(%rbp), %xmm3
	vmulsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm6, %xmm1, %xmm1
	vmovups	-4194860(%rbp), %ymm6
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovups	-4195368(%rbp), %ymm3
	vmovss	%xmm2, 0(%r13)
	vmovaps	-4195376(%rbp), %ymm2
	vmovss	%xmm1, 4(%r13)
	vmovups	-4195372(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm11
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm9
	vmulpd	%ymm12, %ymm9, %ymm14
	vcvtps2pd	%xmm1, %ymm10
	vmulpd	%ymm15, %ymm9, %ymm9
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm7
	vmulpd	%ymm12, %ymm11, %ymm1
	vaddpd	%ymm7, %ymm14, %ymm14
	vmulpd	%ymm15, %ymm11, %ymm11
	vcvtps2pd	%xmm3, %ymm2
	vaddpd	%ymm9, %ymm7, %ymm7
	vmulpd	%ymm12, %ymm2, %ymm2
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm10, %ymm1, %ymm1
	vmulpd	%ymm12, %ymm3, %ymm3
	vaddpd	%ymm11, %ymm10, %ymm10
	vsubpd	%ymm3, %ymm14, %ymm14
	vsubpd	%ymm2, %ymm1, %ymm1
	vaddpd	%ymm7, %ymm3, %ymm3
	vaddpd	%ymm10, %ymm2, %ymm2
	vcvtpd2psy	%ymm14, %xmm14
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm14, %ymm1, %ymm1
	vsubps	%ymm6, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vaddps	%ymm2, %ymm6, %ymm2
	vmovups	-4194828(%rbp), %ymm6
	vunpcklps	%ymm2, %ymm1, %ymm3
	vunpckhps	%ymm2, %ymm1, %ymm1
	vinsertf128	$1, %xmm1, %ymm3, %ymm2
	vperm2f128	$49, %ymm1, %ymm3, %ymm1
	vmovups	%ymm2, 8(%r13)
	vmovaps	-4195344(%rbp), %ymm2
	vmovups	%ymm1, 40(%r13)
	vmovups	-4195340(%rbp), %ymm1
	vcvtps2pd	%xmm2, %ymm11
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm9
	vmulpd	%ymm12, %ymm9, %ymm14
	vcvtps2pd	%xmm1, %ymm10
	vmulpd	%ymm15, %ymm9, %ymm9
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm7
	vmulpd	%ymm12, %ymm11, %ymm1
	vaddpd	%ymm7, %ymm14, %ymm14
	vmulpd	%ymm15, %ymm11, %ymm11
	vmovups	-4195336(%rbp), %ymm3
	vaddpd	%ymm9, %ymm7, %ymm7
	vaddpd	%ymm10, %ymm1, %ymm1
	vcvtps2pd	%xmm3, %ymm2
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm12, %ymm2, %ymm2
	vaddpd	%ymm11, %ymm10, %ymm10
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm12, %ymm3, %ymm3
	vsubpd	%ymm3, %ymm14, %ymm14
	vsubpd	%ymm2, %ymm1, %ymm1
	vaddpd	%ymm7, %ymm3, %ymm3
	vaddpd	%ymm10, %ymm2, %ymm2
	vcvtpd2psy	%ymm14, %xmm14
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm14, %ymm1, %ymm1
	vsubps	%ymm6, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm3, %ymm2, %ymm2
	vaddps	%ymm2, %ymm6, %ymm2
	vmovups	-4194796(%rbp), %ymm6
	vunpcklps	%ymm2, %ymm1, %ymm3
	vunpckhps	%ymm2, %ymm1, %ymm1
	vinsertf128	$1, %xmm1, %ymm3, %ymm2
	vperm2f128	$49, %ymm1, %ymm3, %ymm1
	vmovaps	-4195312(%rbp), %ymm3
	vmovups	%ymm1, 104(%r13)
	vmovups	-4195308(%rbp), %ymm1
	vmovups	%ymm2, 72(%r13)
	vcvtps2pd	%xmm3, %ymm11
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm9
	vmulpd	%ymm12, %ymm9, %ymm14
	vcvtps2pd	%xmm1, %ymm10
	vmulpd	%ymm15, %ymm9, %ymm9
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm7
	vmulpd	%ymm12, %ymm11, %ymm1
	vaddpd	%ymm7, %ymm14, %ymm14
	vmulpd	%ymm15, %ymm11, %ymm11
	vmovups	-4195304(%rbp), %ymm2
	vaddpd	%ymm9, %ymm7, %ymm7
	vmovsd	.LC40(%rip), %xmm9
	vaddpd	%ymm10, %ymm1, %ymm1
	vcvtps2pd	%xmm2, %ymm3
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm12, %ymm3, %ymm3
	vaddpd	%ymm11, %ymm10, %ymm10
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm12, %ymm2, %ymm2
	vsubpd	%ymm2, %ymm14, %ymm14
	vsubpd	%ymm3, %ymm1, %ymm1
	vaddpd	%ymm7, %ymm2, %ymm2
	vaddpd	%ymm10, %ymm3, %ymm3
	vcvtpd2psy	%ymm14, %xmm14
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm14, %ymm1, %ymm1
	vsubps	%ymm6, %ymm1, %ymm1
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm3, %xmm3
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm6, %ymm2
	vmovss	-4194764(%rbp), %xmm6
	vunpcklps	%ymm2, %ymm1, %ymm3
	vunpckhps	%ymm2, %ymm1, %ymm1
	vinsertf128	$1, %xmm1, %ymm3, %ymm2
	vmovups	%ymm2, 136(%r13)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195280(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm7
	vperm2f128	$49, %ymm1, %ymm3, %ymm1
	vmulsd	%xmm9, %xmm2, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195276(%rbp), %xmm3, %xmm3
	vmovups	%ymm1, 168(%r13)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195272(%rbp), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm10
	vaddsd	%xmm3, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm3, %xmm2
	vsubsd	%xmm10, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm10, %xmm2
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm6, %xmm7, %xmm7
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm6, %xmm2
	vmovss	%xmm7, 200(%r13)
	vmulsd	%xmm0, %xmm3, %xmm7
	vmovss	%xmm2, 204(%r13)
	vmulsd	%xmm9, %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195268(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm11
	vmovss	-4194760(%rbp), %xmm6
	vaddsd	%xmm10, %xmm2, %xmm10
	vmovss	-4194748(%rbp), %xmm14
	vaddsd	%xmm1, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm1, %xmm3
	vmulsd	%xmm9, %xmm1, %xmm1
	vsubsd	%xmm11, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm11, %xmm3
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm6, %xmm7, %xmm7
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm6, %xmm3
	vmovss	-4194756(%rbp), %xmm6
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm9, %xmm2, %xmm2
	vmovss	%xmm7, 208(%r13)
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-4195264(%rbp), %xmm7, %xmm7
	vaddsd	%xmm7, %xmm11, %xmm11
	vmovss	%xmm3, 212(%r13)
	vmulsd	%xmm0, %xmm7, %xmm3
	vaddsd	%xmm2, %xmm7, %xmm2
	vaddsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm10, %xmm10
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm6, %xmm1
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm6, %xmm10, %xmm10
	vmovss	%xmm1, 220(%r13)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195260(%rbp), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm6
	vmovss	%xmm10, 216(%r13)
	vmovss	-4194752(%rbp), %xmm10
	vsubsd	%xmm6, %xmm11, %xmm11
	vaddsd	%xmm2, %xmm6, %xmm2
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm10, %xmm11, %xmm11
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm10, %xmm2
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-4195256(%rbp), %xmm10, %xmm10
	vmovss	%xmm11, 224(%r13)
	vmulsd	%xmm0, %xmm10, %xmm11
	vmovss	%xmm2, 228(%r13)
	vaddsd	%xmm1, %xmm3, %xmm2
	vaddsd	%xmm10, %xmm6, %xmm3
	vsubsd	%xmm11, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm14, %xmm2, %xmm2
	vmovss	%xmm2, 232(%r13)
	vmulsd	%xmm9, %xmm7, %xmm2
	vmovss	-4194744(%rbp), %xmm7
	vaddsd	%xmm2, %xmm1, %xmm2
	vmulsd	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm11, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm14, %xmm2
	vmovss	%xmm2, 236(%r13)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195252(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm0
	vmulsd	%xmm4, %xmm2, %xmm4
	vsubsd	%xmm0, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm7, %xmm3, %xmm3
	vmovss	%xmm3, 240(%r13)
	vaddsd	%xmm1, %xmm10, %xmm3
	vaddsd	%xmm3, %xmm0, %xmm0
	vmovss	-4194740(%rbp), %xmm3
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vmovss	%xmm0, 244(%r13)
	vmulsd	%xmm8, %xmm10, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vsubsd	%xmm0, %xmm6, %xmm6
	vmulsd	%xmm5, %xmm2, %xmm0
	vaddsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm6, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm1, 248(%r13)
	vmovss	%xmm0, 252(%r13)
.L1458:
	addq	$1024, %r13
	addq	$1024, %rbx
	subl	$1, %r12d
	je	.L1720
.L1460:
	movq	0(%r13), %rax
	leaq	256(%r13), %rdx
	subq	%rbx, %rdx
	movq	%rax, -4195376(%rbp)
	movq	8(%r13), %rax
	movq	%rax, -4195368(%rbp)
	movq	16(%r13), %rax
	movq	%rax, -4195360(%rbp)
	movq	24(%r13), %rax
	movq	%rax, -4195352(%rbp)
	movq	32(%r13), %rax
	movq	%rax, -4195344(%rbp)
	movq	40(%r13), %rax
	movq	%rax, -4195336(%rbp)
	movq	48(%r13), %rax
	movq	%rax, -4195328(%rbp)
	movq	56(%r13), %rax
	movq	%rax, -4195320(%rbp)
	movq	64(%r13), %rax
	movq	%rax, -4195312(%rbp)
	movq	72(%r13), %rax
	movq	%rax, -4195304(%rbp)
	movq	80(%r13), %rax
	movq	%rax, -4195296(%rbp)
	movq	88(%r13), %rax
	movq	%rax, -4195288(%rbp)
	movq	96(%r13), %rax
	movq	%rax, -4195280(%rbp)
	movq	104(%r13), %rax
	movq	%rax, -4195272(%rbp)
	movq	112(%r13), %rax
	movq	%rax, -4195264(%rbp)
	movq	120(%r13), %rax
	movq	%rax, -4195256(%rbp)
	movq	%rdx, %rax
	cmpl	$1, %r14d
	ja	.L1451
	shrq	$2, %rax
	jne	.L1721
.L1452:
	cmpb	$0, -4195416(%rbp)
	je	.L1455
	vmovaps	-4195376(%rbp), %ymm0
	vmovaps	-4194864(%rbp), %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vsubpd	%ymm3, %ymm2, %ymm2
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194832(%rbp), %ymm1
	vmovaps	%ymm0, -4195376(%rbp)
	vmovaps	-4195344(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194800(%rbp), %ymm1
	vmovaps	%ymm0, -4195344(%rbp)
	vmovaps	-4195312(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194768(%rbp), %ymm1
	vmovaps	%ymm0, -4195312(%rbp)
	vmovaps	-4195280(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195280(%rbp)
.L1455:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1454:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, 0(%r13,%rax,2)
	addq	$4, %rax
	cmpq	$128, %rax
	jne	.L1454
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp), %xmm0, %xmm0
	vmovsd	.LC20(%rip), %xmm7
	vcvtss2sd	-4195372(%rbp), %xmm1, %xmm1
	movl	$1, %edi
	vxorpd	%xmm6, %xmm6, %xmm6
	vmovsd	.LC21(%rip), %xmm9
	vmulsd	%xmm7, %xmm0, %xmm0
	movl	$30, %r8d
	vmovsd	.LC22(%rip), %xmm4
	vxorpd	%xmm11, %xmm11, %xmm11
	vmulsd	%xmm9, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovsd	.LC26(%rip), %xmm3
	vcvtss2sd	-4195372(%rbp), %xmm6, %xmm6
	leaq	-4195376(%rbp), %rax
	vcvtss2sd	-4195368(%rbp), %xmm11, %xmm11
	vcvtss2sd	-4195376(%rbp), %xmm10, %xmm10
	vmovsd	.LC27(%rip), %xmm5
	leaq	12(%rax), %rsi
	leaq	-4194864(%rbp), %rax
	leaq	4(%rax), %rdx
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm1
	leaq	12(%r13), %rax
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195364(%rbp), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 4(%r13)
.L1456:
	vmulsd	%xmm5, %xmm10, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	(%rsi), %xmm2, %xmm2
	addl	$4, %edi
	vmulsd	%xmm3, %xmm6, %xmm1
	movl	%r8d, %r9d
	addq	$16, %rsi
	vmulsd	%xmm3, %xmm11, %xmm0
	subl	%edi, %r9d
	addq	$16, %rdx
	vmulsd	%xmm5, %xmm6, %xmm6
	addq	$32, %rax
	vmulsd	%xmm5, %xmm11, %xmm11
	vaddsd	%xmm1, %xmm10, %xmm1
	vmulsd	%xmm4, %xmm2, %xmm10
	vaddsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm0, %xmm0
	vsubsd	%xmm10, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-12(%rsi), %xmm10, %xmm10
	vmulsd	%xmm4, %xmm10, %xmm6
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-16(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -32(%rax)
	vmulsd	%xmm3, %xmm2, %xmm1
	vmulsd	%xmm5, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm11, %xmm1, %xmm1
	vsubsd	%xmm6, %xmm0, %xmm0
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-8(%rsi), %xmm6, %xmm6
	vmulsd	%xmm4, %xmm6, %xmm11
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-12(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -24(%rax)
	vmulsd	%xmm3, %xmm10, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm11, %xmm1, %xmm1
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-4(%rsi), %xmm11, %xmm11
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-8(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -16(%rax)
	vmulsd	%xmm3, %xmm6, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	%xmm4, %xmm11, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	$29, %edi
	jne	.L1456
	leaq	236(%r13), %rdx
	addl	$29, %r9d
	movl	$29, %eax
	vmovapd	%xmm4, %xmm6
	.p2align 4,,10
	.p2align 3
.L1457:
	leal	-1(%rax), %esi
	vxorpd	%xmm0, %xmm0, %xmm0
	addq	$8, %rdx
	leal	1(%rax), %ecx
	movslq	%esi, %rsi
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm1
	movslq	%eax, %rsi
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	movslq	%ecx, %rdi
	addl	$2, %eax
	cltq
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rax,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%ecx, %eax
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rsi,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rdx)
	cmpl	%r9d, %ecx
	jne	.L1457
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195264(%rbp), %xmm0, %xmm0
	vcvtss2sd	-4195260(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm0, %xmm6
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195256(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm4, %xmm1
	vcvtss2sd	-4195252(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm7
	vmulsd	.LC28(%rip), %xmm0, %xmm0
	vmulsd	.LC29(%rip), %xmm4, %xmm4
	vsubsd	%xmm1, %xmm6, %xmm6
	vmulsd	%xmm9, %xmm3, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm6, %xmm1
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-4194744(%rbp), %xmm1, %xmm1
	vmovss	%xmm1, 244(%r13)
	vmovsd	.LC30(%rip), %xmm1
	vmulsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194740(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 252(%r13)
	jmp	.L1458
.L1719:
	movq	%rbx, %rsi
	vmovapd	%ymm13, -4195664(%rbp)
	leaq	-4194864(%rbp), %rdi
	vmovapd	%ymm15, -4195632(%rbp)
	vmovapd	%ymm12, -4195600(%rbp)
	vmovsd	%xmm8, -4195568(%rbp)
	call	memcpy
	vmovapd	-4195664(%rbp), %ymm13
	vmovapd	-4195632(%rbp), %ymm15
	vmovapd	-4195600(%rbp), %ymm12
	vmovsd	-4195568(%rbp), %xmm8
	jmp	.L1459
.L1721:
	movq	%rbx, %rsi
	vmovapd	%ymm13, -4195664(%rbp)
	leaq	-4194864(%rbp), %rdi
	vmovapd	%ymm15, -4195632(%rbp)
	vmovapd	%ymm12, -4195600(%rbp)
	vmovsd	%xmm8, -4195568(%rbp)
	call	memcpy
	vmovapd	-4195664(%rbp), %ymm13
	vmovapd	-4195632(%rbp), %ymm15
	vmovapd	-4195600(%rbp), %ymm12
	vmovsd	-4195568(%rbp), %xmm8
	jmp	.L1452
.L1720:
	xorl	%edi, %edi
	movl	%r14d, %ebx
	movq	%r15, %rsi
	movq	-4195536(%rbp), %r14
	leaq	1024(%r15), %r8
	addl	$1, %edi
	cmpl	$64, %edi
	je	.L1570
.L1722:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L1462:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$64, %eax
	jne	.L1462
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$64, %edi
	jne	.L1722
.L1570:
	movq	%r15, %r13
	movl	$64, %r12d
	vmovapd	.LC39(%rip), %ymm14
	vmovapd	.LC31(%rip), %ymm9
	jmp	.L1461
	.p2align 4,,10
	.p2align 3
.L1464:
	shrq	$2, %rax
	jne	.L1723
.L1472:
	vxorpd	%xmm5, %xmm5, %xmm5
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC35(%rip), %xmm8
	vcvtss2sd	-4195376(%rbp), %xmm5, %xmm5
	vcvtss2sd	-4195372(%rbp), %xmm1, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195368(%rbp), %xmm4, %xmm4
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vmulsd	.LC36(%rip), %xmm5, %xmm2
	vmulsd	%xmm8, %xmm5, %xmm5
	vmovss	-4194864(%rbp), %xmm3
	vmovsd	.LC34(%rip), %xmm0
	vmovups	-4194860(%rbp), %ymm7
	vmulsd	%xmm0, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm4, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovaps	-4195376(%rbp), %ymm3
	vmovss	%xmm2, 0(%r13)
	vmovups	-4195372(%rbp), %ymm2
	vcvtps2pd	%xmm3, %ymm11
	vextractf128	$0x1, %ymm3, %xmm3
	vmovss	%xmm1, 4(%r13)
	vmovups	-4195368(%rbp), %ymm1
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	%ymm13, %ymm5, %ymm6
	vcvtps2pd	%xmm2, %ymm3
	vmulpd	%ymm14, %ymm5, %ymm5
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm4
	vcvtps2pd	%xmm1, %ymm10
	vextractf128	$0x1, %ymm1, %xmm2
	vaddpd	%ymm4, %ymm6, %ymm6
	vmulpd	%ymm13, %ymm11, %ymm1
	vaddpd	%ymm5, %ymm4, %ymm4
	vmulpd	%ymm14, %ymm11, %ymm11
	vmulpd	%ymm13, %ymm10, %ymm10
	vcvtps2pd	%xmm2, %ymm2
	vaddpd	%ymm3, %ymm1, %ymm1
	vmulpd	%ymm13, %ymm2, %ymm2
	vaddpd	%ymm11, %ymm3, %ymm3
	vsubpd	%ymm2, %ymm6, %ymm6
	vsubpd	%ymm10, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm10, %ymm3
	vaddpd	%ymm4, %ymm2, %ymm2
	vcvtpd2psy	%ymm6, %xmm6
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm6, %ymm1, %ymm1
	vsubps	%ymm7, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm7, %ymm7
	vunpcklps	%ymm7, %ymm1, %ymm2
	vunpckhps	%ymm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm3
	vperm2f128	$49, %ymm1, %ymm2, %ymm1
	vmovups	%ymm3, 8(%r13)
	vmovaps	-4195344(%rbp), %ymm3
	vmovups	%ymm1, 40(%r13)
	vmovups	-4195340(%rbp), %ymm1
	vcvtps2pd	%xmm3, %ymm11
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	%ymm13, %ymm5, %ymm6
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm14, %ymm5, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vmulpd	%ymm13, %ymm11, %ymm1
	vaddpd	%ymm4, %ymm6, %ymm6
	vmulpd	%ymm14, %ymm11, %ymm11
	vmovups	-4195336(%rbp), %ymm2
	vaddpd	%ymm5, %ymm4, %ymm4
	vmovups	-4194828(%rbp), %ymm7
	vaddpd	%ymm3, %ymm1, %ymm1
	vcvtps2pd	%xmm2, %ymm10
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm13, %ymm10, %ymm10
	vaddpd	%ymm11, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm13, %ymm2, %ymm2
	vsubpd	%ymm2, %ymm6, %ymm6
	vsubpd	%ymm10, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm10, %ymm3
	vaddpd	%ymm4, %ymm2, %ymm2
	vcvtpd2psy	%ymm6, %xmm6
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm6, %ymm1, %ymm1
	vsubps	%ymm7, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm7, %ymm7
	vunpcklps	%ymm7, %ymm1, %ymm2
	vunpckhps	%ymm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm3
	vperm2f128	$49, %ymm1, %ymm2, %ymm1
	vmovups	%ymm3, 72(%r13)
	vmovaps	-4195312(%rbp), %ymm3
	vmovups	%ymm1, 104(%r13)
	vmovups	-4195308(%rbp), %ymm1
	vcvtps2pd	%xmm3, %ymm11
	vextractf128	$0x1, %ymm3, %xmm3
	vcvtps2pd	%xmm3, %ymm5
	vmulpd	%ymm13, %ymm5, %ymm6
	vcvtps2pd	%xmm1, %ymm3
	vmulpd	%ymm14, %ymm5, %ymm5
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm4
	vmulpd	%ymm13, %ymm11, %ymm1
	vaddpd	%ymm4, %ymm6, %ymm6
	vmulpd	%ymm14, %ymm11, %ymm11
	vmovups	-4195304(%rbp), %ymm2
	vaddpd	%ymm5, %ymm4, %ymm4
	vmovups	-4194796(%rbp), %ymm7
	vaddpd	%ymm3, %ymm1, %ymm1
	vcvtps2pd	%xmm2, %ymm10
	vextractf128	$0x1, %ymm2, %xmm2
	vmulpd	%ymm13, %ymm10, %ymm10
	vaddpd	%ymm11, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm13, %ymm2, %ymm2
	vsubpd	%ymm2, %ymm6, %ymm6
	vsubpd	%ymm10, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm10, %ymm3
	vaddpd	%ymm4, %ymm2, %ymm2
	vcvtpd2psy	%ymm6, %xmm6
	vmovss	-4194764(%rbp), %xmm4
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm6, %ymm1, %ymm1
	vsubps	%ymm7, %ymm1, %ymm1
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm3, %ymm2
	vaddps	%ymm2, %ymm7, %ymm7
	vunpcklps	%ymm7, %ymm1, %ymm2
	vunpckhps	%ymm7, %ymm1, %ymm1
	vinsertf128	$1, %xmm1, %ymm2, %ymm3
	vperm2f128	$49, %ymm1, %ymm2, %ymm1
	vmovups	%ymm1, 168(%r13)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195280(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm5
	vmovups	%ymm3, 136(%r13)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195272(%rbp), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm7
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195276(%rbp), %xmm3, %xmm3
	vaddsd	%xmm3, %xmm5, %xmm5
	vsubsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm4, %xmm5, %xmm5
	vmovss	%xmm5, 200(%r13)
	vmulsd	%xmm0, %xmm3, %xmm5
	vmovsd	.LC40(%rip), %xmm6
	vmulsd	%xmm6, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm5, %xmm5
	vaddsd	%xmm2, %xmm3, %xmm2
	vmulsd	%xmm6, %xmm3, %xmm3
	vaddsd	%xmm2, %xmm7, %xmm2
	vaddsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	-4194760(%rbp), %xmm4
	vmovss	%xmm2, 204(%r13)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195268(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm10
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm7, %xmm2, %xmm7
	vmulsd	%xmm6, %xmm2, %xmm2
	vsubsd	%xmm10, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm10, %xmm3
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm4, %xmm5, %xmm5
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm4, %xmm3
	vmovss	-4194756(%rbp), %xmm4
	vmovss	%xmm5, 208(%r13)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4195264(%rbp), %xmm5, %xmm5
	vaddsd	%xmm2, %xmm5, %xmm2
	vmovss	%xmm3, 212(%r13)
	vmulsd	%xmm0, %xmm5, %xmm3
	vaddsd	%xmm5, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm3, %xmm1
	vsubsd	%xmm3, %xmm7, %xmm7
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm4, %xmm7, %xmm7
	vmovss	%xmm1, 220(%r13)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195260(%rbp), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm4
	vmovss	%xmm7, 216(%r13)
	vaddsd	%xmm1, %xmm3, %xmm3
	vmovss	-4194752(%rbp), %xmm7
	vaddsd	%xmm2, %xmm4, %xmm2
	vsubsd	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm7, %xmm10, %xmm10
	vmovss	%xmm2, 228(%r13)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195256(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm7
	vmovss	%xmm10, 224(%r13)
	vmovss	-4194748(%rbp), %xmm10
	vsubsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm10, %xmm3, %xmm3
	vmovss	%xmm3, 232(%r13)
	vmulsd	%xmm6, %xmm5, %xmm3
	vaddsd	%xmm2, %xmm4, %xmm5
	vaddsd	%xmm3, %xmm1, %xmm3
	vmulsd	%xmm6, %xmm1, %xmm1
	vaddsd	%xmm3, %xmm7, %xmm3
	vmovss	-4194744(%rbp), %xmm7
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm10, %xmm3
	vmovss	%xmm3, 236(%r13)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195252(%rbp), %xmm3, %xmm3
	vmulsd	%xmm0, %xmm3, %xmm0
	vmulsd	%xmm8, %xmm3, %xmm8
	vsubsd	%xmm0, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm7, %xmm5, %xmm5
	vmovss	%xmm5, 240(%r13)
	vaddsd	%xmm1, %xmm2, %xmm5
	vaddsd	%xmm5, %xmm0, %xmm0
	vmovss	-4194740(%rbp), %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm7, %xmm0
	vmovss	%xmm0, 244(%r13)
	vmulsd	.LC33(%rip), %xmm2, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vsubsd	%xmm0, %xmm4, %xmm4
	vmulsd	.LC36(%rip), %xmm3, %xmm0
	vaddsd	%xmm8, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm4, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vmovss	%xmm1, 248(%r13)
	vmovss	%xmm0, 252(%r13)
.L1471:
	addq	$1024, %r13
	addq	$1024, %r14
	subl	$1, %r12d
	je	.L1724
.L1461:
	movq	0(%r13), %rax
	leaq	256(%r13), %rdx
	subq	%r14, %rdx
	movq	%rax, -4195376(%rbp)
	movq	8(%r13), %rax
	movq	%rax, -4195368(%rbp)
	movq	16(%r13), %rax
	movq	%rax, -4195360(%rbp)
	movq	24(%r13), %rax
	movq	%rax, -4195352(%rbp)
	movq	32(%r13), %rax
	movq	%rax, -4195344(%rbp)
	movq	40(%r13), %rax
	movq	%rax, -4195336(%rbp)
	movq	48(%r13), %rax
	movq	%rax, -4195328(%rbp)
	movq	56(%r13), %rax
	movq	%rax, -4195320(%rbp)
	movq	64(%r13), %rax
	movq	%rax, -4195312(%rbp)
	movq	72(%r13), %rax
	movq	%rax, -4195304(%rbp)
	movq	80(%r13), %rax
	movq	%rax, -4195296(%rbp)
	movq	88(%r13), %rax
	movq	%rax, -4195288(%rbp)
	movq	96(%r13), %rax
	movq	%rax, -4195280(%rbp)
	movq	104(%r13), %rax
	movq	%rax, -4195272(%rbp)
	movq	112(%r13), %rax
	movq	%rax, -4195264(%rbp)
	movq	120(%r13), %rax
	movq	%rax, -4195256(%rbp)
	movq	%rdx, %rax
	cmpl	$1, %ebx
	ja	.L1464
	shrq	$2, %rax
	jne	.L1725
.L1465:
	cmpb	$0, -4195416(%rbp)
	je	.L1468
	vmovaps	-4195376(%rbp), %ymm0
	vmovaps	-4194864(%rbp), %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm9, %ymm3, %ymm3
	vcvtps2pd	%xmm1, %ymm1
	vsubpd	%ymm3, %ymm2, %ymm2
	vmulpd	%ymm9, %ymm1, %ymm1
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194832(%rbp), %ymm1
	vmovaps	%ymm0, -4195376(%rbp)
	vmovaps	-4195344(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm9, %ymm3, %ymm3
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vmulpd	%ymm9, %ymm1, %ymm1
	vsubpd	%ymm3, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194800(%rbp), %ymm1
	vmovaps	%ymm0, -4195344(%rbp)
	vmovaps	-4195312(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm9, %ymm3, %ymm3
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vmulpd	%ymm9, %ymm1, %ymm1
	vsubpd	%ymm3, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194768(%rbp), %ymm1
	vmovaps	%ymm0, -4195312(%rbp)
	vmovaps	-4195280(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vmulpd	%ymm9, %ymm3, %ymm3
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vmulpd	%ymm9, %ymm1, %ymm1
	vsubpd	%ymm3, %ymm2, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195280(%rbp)
.L1468:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1467:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, 0(%r13,%rax,2)
	addq	$4, %rax
	cmpq	$128, %rax
	jne	.L1467
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp), %xmm0, %xmm0
	vmovsd	.LC20(%rip), %xmm7
	vcvtss2sd	-4195372(%rbp), %xmm1, %xmm1
	movl	$1, %edi
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC21(%rip), %xmm8
	vmulsd	%xmm7, %xmm0, %xmm0
	movl	$30, %r8d
	vmovsd	.LC22(%rip), %xmm4
	vxorpd	%xmm6, %xmm6, %xmm6
	vmulsd	%xmm8, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovsd	.LC26(%rip), %xmm3
	vcvtss2sd	-4195372(%rbp), %xmm2, %xmm2
	leaq	-4195376(%rbp), %rax
	vcvtss2sd	-4195368(%rbp), %xmm6, %xmm6
	vcvtss2sd	-4195376(%rbp), %xmm10, %xmm10
	vmovsd	.LC27(%rip), %xmm5
	leaq	12(%rax), %rsi
	leaq	-4194864(%rbp), %rax
	leaq	4(%rax), %rdx
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm1
	leaq	12(%r13), %rax
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195364(%rbp), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 4(%r13)
.L1469:
	vmulsd	%xmm5, %xmm10, %xmm10
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	(%rsi), %xmm11, %xmm11
	addl	$4, %edi
	vmulsd	%xmm3, %xmm2, %xmm1
	movl	%r8d, %ecx
	addq	$16, %rsi
	vmulsd	%xmm3, %xmm6, %xmm0
	subl	%edi, %ecx
	addq	$16, %rdx
	vmulsd	%xmm5, %xmm2, %xmm2
	addq	$32, %rax
	vmulsd	%xmm5, %xmm6, %xmm6
	vaddsd	%xmm1, %xmm10, %xmm1
	vmulsd	%xmm4, %xmm11, %xmm10
	vaddsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm0, %xmm0
	vsubsd	%xmm10, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-12(%rsi), %xmm10, %xmm10
	vmulsd	%xmm4, %xmm10, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-16(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -32(%rax)
	vmulsd	%xmm3, %xmm11, %xmm1
	vmulsd	%xmm5, %xmm11, %xmm11
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm6, %xmm1, %xmm1
	vsubsd	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-8(%rsi), %xmm2, %xmm2
	vmulsd	%xmm4, %xmm2, %xmm6
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-12(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -24(%rax)
	vmulsd	%xmm3, %xmm10, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm11, %xmm0, %xmm0
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-4(%rsi), %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-8(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -16(%rax)
	vmulsd	%xmm3, %xmm2, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	%xmm4, %xmm6, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	$29, %edi
	jne	.L1469
	leaq	236(%r13), %rax
	addl	$29, %ecx
	vmovapd	%xmm4, %xmm6
	.p2align 4,,10
	.p2align 3
.L1470:
	leal	-1(%rdi), %esi
	vxorpd	%xmm0, %xmm0, %xmm0
	addq	$8, %rax
	leal	1(%rdi), %edx
	movslq	%esi, %rsi
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm1
	movslq	%edi, %rsi
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	movslq	%edx, %r8
	addl	$2, %edi
	movslq	%edi, %rdi
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%r8,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%edx, %edi
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rsi,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	%ecx, %edx
	jne	.L1470
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195264(%rbp), %xmm0, %xmm0
	vcvtss2sd	-4195260(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm0, %xmm6
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195256(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm4, %xmm1
	vcvtss2sd	-4195252(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm7
	vmulsd	.LC28(%rip), %xmm0, %xmm0
	vmulsd	.LC29(%rip), %xmm4, %xmm4
	vsubsd	%xmm1, %xmm6, %xmm6
	vmulsd	%xmm8, %xmm3, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm6, %xmm1
	vaddsd	%xmm7, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-4194744(%rbp), %xmm1, %xmm1
	vmovss	%xmm1, 244(%r13)
	vmovsd	.LC30(%rip), %xmm1
	vmulsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194740(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 252(%r13)
	jmp	.L1471
.L1723:
	movq	%r14, %rsi
	vmovapd	%ymm13, -4195664(%rbp)
	leaq	-4194864(%rbp), %rdi
	vmovapd	%ymm15, -4195632(%rbp)
	vmovapd	%ymm12, -4195600(%rbp)
	vmovapd	%ymm9, -4195568(%rbp)
	vmovapd	%ymm14, -4195536(%rbp)
	call	memcpy
	vmovapd	-4195664(%rbp), %ymm13
	vmovapd	-4195632(%rbp), %ymm15
	vmovapd	-4195600(%rbp), %ymm12
	vmovapd	-4195568(%rbp), %ymm9
	vmovapd	-4195536(%rbp), %ymm14
	jmp	.L1472
.L1725:
	movq	%r14, %rsi
	vmovapd	%ymm13, -4195664(%rbp)
	leaq	-4194864(%rbp), %rdi
	vmovapd	%ymm15, -4195632(%rbp)
	vmovapd	%ymm12, -4195600(%rbp)
	vmovapd	%ymm9, -4195568(%rbp)
	vmovapd	%ymm14, -4195536(%rbp)
	call	memcpy
	vmovapd	-4195664(%rbp), %ymm13
	vmovapd	-4195632(%rbp), %ymm15
	vmovapd	-4195600(%rbp), %ymm12
	vmovapd	-4195568(%rbp), %ymm9
	vmovapd	-4195536(%rbp), %ymm14
	jmp	.L1465
.L1724:
	addq	$262144, %r15
	cmpq	-4195504(%rbp), %r15
	jne	.L1473
	movq	-4195424(%rbp), %rax
	movl	-4195428(%rbp), %r14d
	vmovapd	.LC38(%rip), %ymm6
	vmovapd	.LC39(%rip), %ymm7
	leaq	131352(%rax), %rbx
	leaq	33685784(%rax), %r13
	movq	%rbx, %r12
	movq	%rbx, -4195568(%rbp)
.L1474:
	leaq	-131072(%r12), %r15
	.p2align 4,,10
	.p2align 3
.L1485:
	leaq	-256(%r15), %rbx
	vmovapd	%ymm7, -4195536(%rbp)
	movl	$256, %edx
	vmovapd	%ymm6, -4195504(%rbp)
	movq	%rbx, %rsi
	leaq	-4195376(%rbp), %rdi
	cmpl	$1, %r14d
	jbe	.L1726
	call	memcpy
	movl	$256, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	vxorpd	%xmm3, %xmm3, %xmm3
	xorl	%eax, %eax
	vmovsd	.LC36(%rip), %xmm12
	vcvtss2sd	-4195376(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC35(%rip), %xmm13
	vcvtss2sd	-4195372(%rbp), %xmm2, %xmm2
	vmulsd	%xmm12, %xmm3, %xmm4
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmovsd	.LC34(%rip), %xmm11
	vmulsd	%xmm13, %xmm3, %xmm3
	vmovss	-4194864(%rbp), %xmm0
	vmulsd	%xmm11, %xmm1, %xmm1
	vmovapd	-4195536(%rbp), %ymm7
	leaq	-4195376(%rbp), %rbx
	vmovapd	-4195504(%rbp), %ymm6
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	leaq	8(%rbx), %rdi
	leaq	4(%rbx), %rsi
	leaq	-4194864(%rbp), %rbx
	leaq	-248(%r15), %rdx
	vsubsd	%xmm2, %xmm4, %xmm4
	leaq	4(%rbx), %rcx
	vaddsd	%xmm3, %xmm2, %xmm3
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm3, %xmm1
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm0, %xmm4, %xmm4
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmovss	%xmm4, -256(%r15)
	vmovss	%xmm1, -252(%r15)
	.p2align 4,,10
	.p2align 3
.L1484:
	vmovups	(%rdi,%rax), %ymm0
	vmovups	(%rsi,%rax), %ymm4
	vcvtps2pd	%xmm0, %ymm8
	vextractf128	$0x1, %ymm0, %xmm3
	vmulpd	%ymm6, %ymm8, %ymm8
	vmovaps	-4195376(%rbp,%rax), %ymm5
	vcvtps2pd	%xmm4, %ymm9
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm3, %ymm3
	vcvtps2pd	%xmm5, %ymm10
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm6, %ymm10, %ymm2
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm9, %ymm2, %ymm2
	vmulpd	%ymm6, %ymm5, %ymm0
	vmulpd	%ymm6, %ymm3, %ymm3
	vmovups	(%rcx,%rax), %ymm1
	vaddpd	%ymm4, %ymm0, %ymm0
	vmulpd	%ymm7, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vsubpd	%ymm8, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm1, %ymm0, %ymm2
	vmulpd	%ymm7, %ymm10, %ymm0
	vaddpd	%ymm0, %ymm9, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm0, %ymm8, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm3, %ymm0, %ymm0
	vaddps	%ymm0, %ymm1, %ymm0
	vunpcklps	%ymm0, %ymm2, %ymm1
	vunpckhps	%ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm2, (%rdx,%rax,2)
	vmovups	%ymm0, 32(%rdx,%rax,2)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L1484
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195152(%rbp), %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195144(%rbp), %xmm0, %xmm0
	vmulsd	%xmm11, %xmm0, %xmm5
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195148(%rbp), %xmm2, %xmm2
	vmovss	-4194636(%rbp), %xmm1
	vmulsd	%xmm11, %xmm2, %xmm9
	vaddsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm9, %xmm9
	vsubsd	%xmm5, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm1, %xmm3, %xmm3
	vmovss	%xmm3, 200(%r15)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm2, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	-4194632(%rbp), %xmm4
	vmovss	%xmm1, 204(%r15)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195140(%rbp), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm8
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm9, %xmm9
	vaddsd	%xmm2, %xmm8, %xmm2
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm4, %xmm9, %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	-4194628(%rbp), %xmm4
	vmovss	%xmm9, 208(%r15)
	vmovss	%xmm2, 212(%r15)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195136(%rbp), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm9
	vaddsd	%xmm2, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm9, %xmm0
	vsubsd	%xmm9, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm4, %xmm5, %xmm5
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195132(%rbp), %xmm4, %xmm4
	vaddsd	%xmm2, %xmm4, %xmm2
	vmovss	%xmm0, 220(%r15)
	vmulsd	%xmm11, %xmm4, %xmm0
	vaddsd	%xmm4, %xmm9, %xmm9
	vmovss	%xmm5, 216(%r15)
	vmulsd	%xmm3, %xmm4, %xmm4
	vmovss	-4194624(%rbp), %xmm5
	vsubsd	%xmm0, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm0, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm5, %xmm8, %xmm8
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vmovss	-4194620(%rbp), %xmm5
	vmovss	%xmm8, 224(%r15)
	vmovss	%xmm1, 228(%r15)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195128(%rbp), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm8
	vaddsd	%xmm4, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm8, %xmm2
	vsubsd	%xmm8, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm0, %xmm8
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm5, %xmm9, %xmm9
	vmovss	-4194616(%rbp), %xmm5
	vmovss	%xmm2, 236(%r15)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195124(%rbp), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm11
	vmulsd	%xmm13, %xmm2, %xmm13
	vaddsd	%xmm1, %xmm4, %xmm4
	vmovss	%xmm9, 232(%r15)
	vmulsd	%xmm12, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm11, %xmm8, %xmm8
	vaddsd	%xmm3, %xmm11, %xmm11
	vxorps	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm13, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vcvtsd2ss	%xmm11, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vsubss	%xmm5, %xmm8, %xmm8
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm3, 244(%r15)
	vmovss	-4194612(%rbp), %xmm3
	vmovss	%xmm8, 240(%r15)
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm4, 248(%r15)
	vmovss	%xmm0, 252(%r15)
.L1482:
	addq	$1024, %r15
	cmpq	%r12, %r15
	jne	.L1485
	leaq	262144(%r15), %r12
	cmpq	%r13, %r12
	jne	.L1474
	movq	-4195568(%rbp), %rbx
	xorl	%r10d, %r10d
	movq	-4195440(%rbp), %r11
.L1486:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4195472(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$128, %r8d
	je	.L1487
.L1727:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L1488:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$128, %edx
	jne	.L1488
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$128, %r8d
	jne	.L1727
.L1487:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$128, %r10d
	jne	.L1486
	movl	-4195428(%rbp), %r12d
	movq	%r13, -4195632(%rbp)
	vmovapd	.LC38(%rip), %ymm6
	vmovapd	.LC39(%rip), %ymm8
	vmovapd	%ymm6, %ymm7
.L1515:
	leaq	-131072(%rbx), %r15
	movq	%r15, %r14
	.p2align 4,,10
	.p2align 3
.L1501:
	leaq	-256(%r14), %r13
	vmovapd	%ymm7, -4195568(%rbp)
	movl	$256, %edx
	vmovapd	%ymm8, -4195536(%rbp)
	movq	%r13, %rsi
	vmovapd	%ymm6, -4195504(%rbp)
	leaq	-4195376(%rbp), %rdi
	cmpl	$1, %r12d
	jbe	.L1728
	call	memcpy
	movl	$256, %edx
	movq	%r14, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	vxorpd	%xmm4, %xmm4, %xmm4
	xorl	%eax, %eax
	vmovsd	.LC36(%rip), %xmm13
	vcvtss2sd	-4195376(%rbp), %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	.LC35(%rip), %xmm14
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	vmulsd	%xmm13, %xmm4, %xmm1
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195368(%rbp), %xmm3, %xmm3
	vmovsd	.LC34(%rip), %xmm12
	vmulsd	%xmm14, %xmm4, %xmm4
	vmovss	-4194864(%rbp), %xmm2
	vmulsd	%xmm12, %xmm3, %xmm3
	vmovapd	-4195568(%rbp), %ymm7
	leaq	-4195376(%rbp), %rdi
	vmovapd	-4195536(%rbp), %ymm8
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	addq	$8, %rdi
	leaq	-4195376(%rbp), %rsi
	vmovapd	-4195504(%rbp), %ymm6
	leaq	-4194864(%rbp), %rdx
	addq	$4, %rsi
	addq	$4, %rdx
	vsubsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm1, -256(%r14)
	vmovss	%xmm0, -252(%r14)
	.p2align 4,,10
	.p2align 3
.L1500:
	vmovups	(%rsi,%rax), %ymm4
	vmovaps	-4195376(%rbp,%rax), %ymm5
	vcvtps2pd	%xmm4, %ymm10
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vmovups	(%rdi,%rax), %ymm3
	vcvtps2pd	%xmm5, %ymm11
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm6, %ymm11, %ymm2
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm10, %ymm2, %ymm2
	vmulpd	%ymm6, %ymm5, %ymm0
	vcvtps2pd	%xmm3, %ymm9
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm4, %ymm0, %ymm0
	vmulpd	%ymm6, %ymm9, %ymm9
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm6, %ymm3, %ymm3
	vmovups	(%rdx,%rax), %ymm1
	vmulpd	%ymm8, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vsubpd	%ymm9, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm1, %ymm0, %ymm2
	vmulpd	%ymm8, %ymm11, %ymm0
	vaddpd	%ymm0, %ymm10, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm0, %ymm9, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm3, %ymm0, %ymm0
	vaddps	%ymm0, %ymm1, %ymm0
	vunpcklps	%ymm0, %ymm2, %ymm1
	vunpckhps	%ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm2, 8(%r13,%rax,2)
	vmovups	%ymm0, 40(%r13,%rax,2)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L1500
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195152(%rbp), %xmm1, %xmm1
	vmulsd	%xmm12, %xmm1, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195144(%rbp), %xmm0, %xmm0
	vmulsd	%xmm12, %xmm0, %xmm9
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195148(%rbp), %xmm2, %xmm2
	vmovss	-4194636(%rbp), %xmm3
	vmulsd	%xmm12, %xmm2, %xmm10
	vmovss	-4194620(%rbp), %xmm11
	vaddsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm10, %xmm10
	vsubsd	%xmm9, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm3, %xmm4, %xmm4
	vmovss	%xmm4, 200(%r14)
	vmovsd	.LC40(%rip), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm9, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	-4194632(%rbp), %xmm3
	vmovss	%xmm1, 204(%r14)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195140(%rbp), %xmm1, %xmm1
	vmulsd	%xmm12, %xmm1, %xmm5
	vaddsd	%xmm9, %xmm1, %xmm9
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vsubsd	%xmm5, %xmm10, %xmm10
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm3, %xmm10, %xmm10
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195136(%rbp), %xmm3, %xmm3
	vaddsd	%xmm3, %xmm5, %xmm5
	vmovss	%xmm2, 212(%r14)
	vmulsd	%xmm12, %xmm3, %xmm2
	vaddsd	%xmm1, %xmm3, %xmm1
	vmovss	%xmm10, 208(%r14)
	vmovss	-4194628(%rbp), %xmm10
	vsubsd	%xmm2, %xmm9, %xmm9
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm10, %xmm9, %xmm9
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm10, %xmm0
	vmovss	-4194624(%rbp), %xmm10
	vmovss	%xmm9, 216(%r14)
	vmovss	%xmm0, 220(%r14)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195132(%rbp), %xmm0, %xmm0
	vmulsd	%xmm12, %xmm0, %xmm9
	vsubsd	%xmm9, %xmm5, %xmm5
	vaddsd	%xmm1, %xmm9, %xmm1
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm10, %xmm5, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vmovss	%xmm5, 224(%r14)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4195128(%rbp), %xmm5, %xmm5
	vmulsd	%xmm12, %xmm5, %xmm10
	vmovss	%xmm1, 228(%r14)
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195124(%rbp), %xmm2, %xmm2
	vmulsd	%xmm12, %xmm2, %xmm12
	vsubsd	%xmm10, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm11, %xmm1, %xmm1
	vmovss	%xmm1, 232(%r14)
	vmulsd	%xmm4, %xmm3, %xmm1
	vmovss	-4194616(%rbp), %xmm3
	vmulsd	%xmm4, %xmm0, %xmm4
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm4, %xmm5, %xmm0
	vaddsd	%xmm1, %xmm10, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm1
	vmovss	%xmm1, 236(%r14)
	vaddsd	%xmm5, %xmm9, %xmm1
	vsubsd	%xmm12, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm12, %xmm12
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm12, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	-4194612(%rbp), %xmm3
	vmovss	%xmm1, 240(%r14)
	vmulsd	%xmm14, %xmm2, %xmm1
	vmovss	%xmm0, 244(%r14)
	vmulsd	.LC33(%rip), %xmm5, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm9, %xmm9
	vmulsd	%xmm13, %xmm2, %xmm0
	vaddsd	%xmm1, %xmm4, %xmm1
	vaddsd	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm1, 248(%r14)
	vmovss	%xmm0, 252(%r14)
.L1498:
	addq	$1024, %r14
	cmpq	%rbx, %r14
	jne	.L1501
	leaq	-130304(%rbx), %r8
	xorl	%edi, %edi
	leaq	-131328(%rbx), %rsi
	addl	$1, %edi
	cmpl	$128, %edi
	je	.L1729
.L1689:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L1503:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$128, %eax
	jne	.L1503
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$128, %edi
	jne	.L1689
.L1729:
	movzbl	-4195416(%rbp), %r13d
	vmovapd	.LC39(%rip), %ymm9
	.p2align 4,,10
	.p2align 3
.L1502:
	leaq	-256(%r15), %r14
	vmovapd	%ymm7, -4195600(%rbp)
	movl	$256, %edx
	vmovapd	%ymm8, -4195568(%rbp)
	leaq	-4195376(%rbp), %rdi
	movq	%r14, %rsi
	vmovapd	%ymm6, -4195536(%rbp)
	vmovapd	%ymm9, -4195504(%rbp)
	cmpl	$1, %r12d
	jbe	.L1730
	call	memcpy
	movl	$256, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	vxorpd	%xmm4, %xmm4, %xmm4
	xorl	%eax, %eax
	vmovsd	.LC36(%rip), %xmm14
	vcvtss2sd	-4195376(%rbp), %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	.LC35(%rip), %xmm15
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	vmulsd	%xmm14, %xmm4, %xmm1
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195368(%rbp), %xmm3, %xmm3
	vmovsd	.LC34(%rip), %xmm13
	vmulsd	%xmm15, %xmm4, %xmm4
	vmovss	-4194864(%rbp), %xmm2
	vmulsd	%xmm13, %xmm3, %xmm3
	vmovapd	-4195600(%rbp), %ymm7
	leaq	-4195376(%rbp), %rdi
	vmovapd	-4195568(%rbp), %ymm8
	leaq	8(%rdi), %rsi
	vmovapd	-4195536(%rbp), %ymm6
	leaq	4(%rdi), %rcx
	vmovapd	-4195504(%rbp), %ymm9
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	leaq	-4194864(%rbp), %rdi
	leaq	4(%rdi), %rdx
	vmovq	%xmm14, %rdi
	vsubsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm1, -256(%r15)
	vmovss	%xmm0, -252(%r15)
	.p2align 4,,10
	.p2align 3
.L1514:
	vmovups	(%rcx,%rax), %ymm2
	vmovaps	-4195376(%rbp,%rax), %ymm3
	vcvtps2pd	%xmm2, %ymm10
	vextractf128	$0x1, %ymm2, %xmm2
	vcvtps2pd	%xmm2, %ymm2
	vmovups	(%rsi,%rax), %ymm0
	vcvtps2pd	%xmm3, %ymm11
	vextractf128	$0x1, %ymm3, %xmm3
	vmulpd	%ymm7, %ymm11, %ymm4
	vcvtps2pd	%xmm3, %ymm3
	vaddpd	%ymm10, %ymm4, %ymm4
	vmulpd	%ymm7, %ymm3, %ymm12
	vmulpd	%ymm9, %ymm11, %ymm11
	vmulpd	%ymm9, %ymm3, %ymm3
	vaddpd	%ymm2, %ymm12, %ymm12
	vcvtps2pd	%xmm0, %ymm1
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	%ymm7, %ymm1, %ymm1
	vaddpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm7, %ymm0, %ymm0
	vmovups	(%rdx,%rax), %ymm5
	vaddpd	%ymm11, %ymm10, %ymm10
	vsubpd	%ymm1, %ymm4, %ymm4
	vsubpd	%ymm0, %ymm12, %ymm14
	vaddpd	%ymm10, %ymm1, %ymm1
	vaddpd	%ymm2, %ymm0, %ymm0
	vcvtpd2psy	%ymm4, %xmm12
	vcvtpd2psy	%ymm14, %xmm4
	vinsertf128	$0x1, %xmm4, %ymm12, %ymm4
	vsubps	%ymm5, %ymm4, %ymm4
	vcvtpd2psy	%ymm1, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vaddps	%ymm0, %ymm5, %ymm0
	vunpcklps	%ymm0, %ymm4, %ymm1
	vunpckhps	%ymm0, %ymm4, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm2, 8(%r14,%rax,2)
	vmovups	%ymm0, 40(%r14,%rax,2)
	addq	$32, %rax
	cmpq	$224, %rax
	jne	.L1514
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195152(%rbp), %xmm1, %xmm1
	vmulsd	%xmm13, %xmm1, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195144(%rbp), %xmm0, %xmm0
	vmulsd	%xmm13, %xmm0, %xmm10
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195148(%rbp), %xmm2, %xmm2
	vmovss	-4194636(%rbp), %xmm3
	vmulsd	%xmm13, %xmm2, %xmm5
	vmovq	%rdi, %xmm14
	vmovss	-4194620(%rbp), %xmm12
	vaddsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm5, %xmm5
	vsubsd	%xmm10, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm3, %xmm4, %xmm4
	vmovss	%xmm4, 200(%r15)
	vmovsd	.LC40(%rip), %xmm4
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm10, %xmm1
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm3, %xmm1
	vmovss	-4194632(%rbp), %xmm3
	vmovss	%xmm1, 204(%r15)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195140(%rbp), %xmm1, %xmm1
	vmulsd	%xmm13, %xmm1, %xmm11
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm10, %xmm1, %xmm10
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm11, %xmm2
	vsubsd	%xmm11, %xmm5, %xmm5
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm3, %xmm5, %xmm5
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195136(%rbp), %xmm3, %xmm3
	vaddsd	%xmm3, %xmm11, %xmm11
	vmovss	%xmm2, 212(%r15)
	vmulsd	%xmm13, %xmm3, %xmm2
	vaddsd	%xmm1, %xmm3, %xmm1
	vmovss	%xmm5, 208(%r15)
	vmovss	-4194628(%rbp), %xmm5
	vaddsd	%xmm0, %xmm2, %xmm0
	vsubsd	%xmm2, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm5, %xmm0
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm5, %xmm10, %xmm10
	vmovss	%xmm0, 220(%r15)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195132(%rbp), %xmm0, %xmm0
	vmulsd	%xmm13, %xmm0, %xmm5
	vmovss	%xmm10, 216(%r15)
	vmovss	-4194624(%rbp), %xmm10
	vsubsd	%xmm5, %xmm11, %xmm11
	vaddsd	%xmm1, %xmm5, %xmm1
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm10, %xmm11, %xmm11
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm10, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-4195128(%rbp), %xmm10, %xmm10
	vmovss	%xmm11, 224(%r15)
	vmulsd	%xmm13, %xmm10, %xmm11
	vmovss	%xmm1, 228(%r15)
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195124(%rbp), %xmm2, %xmm2
	vmulsd	%xmm13, %xmm2, %xmm13
	vsubsd	%xmm11, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm12, %xmm1, %xmm1
	vmovss	%xmm1, 232(%r15)
	vmulsd	%xmm4, %xmm3, %xmm1
	vmovss	-4194616(%rbp), %xmm3
	vmulsd	%xmm4, %xmm0, %xmm4
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm4, %xmm10, %xmm0
	vaddsd	%xmm1, %xmm11, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm12, %xmm1
	vmovss	%xmm1, 236(%r15)
	vaddsd	%xmm10, %xmm5, %xmm1
	vsubsd	%xmm13, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm13, %xmm13
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm13, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	-4194612(%rbp), %xmm3
	vmovss	%xmm1, 240(%r15)
	vmulsd	%xmm15, %xmm2, %xmm1
	vmovss	%xmm0, 244(%r15)
	vmulsd	.LC33(%rip), %xmm10, %xmm0
	vaddsd	%xmm0, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm5, %xmm5
	vmulsd	%xmm14, %xmm2, %xmm0
	vaddsd	%xmm1, %xmm4, %xmm1
	vaddsd	%xmm0, %xmm5, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm3, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm1, 248(%r15)
	vmovss	%xmm0, 252(%r15)
.L1512:
	addq	$1024, %r15
	cmpq	%rbx, %r15
	jne	.L1502
	leaq	262144(%r15), %rbx
	cmpq	%rbx, -4195632(%rbp)
	jne	.L1515
	movq	-4195424(%rbp), %rax
	movl	-4195428(%rbp), %r14d
	vmovapd	.LC38(%rip), %ymm6
	vmovapd	.LC39(%rip), %ymm7
	vmovapd	.LC31(%rip), %ymm9
	leaq	262680(%rax), %rbx
	leaq	67371544(%rax), %r13
	movq	%rbx, %r12
	movq	%rbx, -4195600(%rbp)
.L1516:
	leaq	-262144(%r12), %r15
	.p2align 4,,10
	.p2align 3
.L1527:
	leaq	-512(%r15), %rbx
	vmovapd	%ymm9, -4195568(%rbp)
	movl	$512, %edx
	vmovapd	%ymm7, -4195536(%rbp)
	movq	%rbx, %rsi
	vmovapd	%ymm6, -4195504(%rbp)
	leaq	-4195376(%rbp), %rdi
	cmpl	$1, %r14d
	jbe	.L1731
	call	memcpy
	movl	$512, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	vxorpd	%xmm3, %xmm3, %xmm3
	xorl	%eax, %eax
	vmovsd	.LC36(%rip), %xmm13
	vcvtss2sd	-4195376(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC35(%rip), %xmm14
	vcvtss2sd	-4195372(%rbp), %xmm2, %xmm2
	vmulsd	%xmm13, %xmm3, %xmm4
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmovsd	.LC34(%rip), %xmm12
	vmulsd	%xmm14, %xmm3, %xmm3
	vmovss	-4194864(%rbp), %xmm0
	vmulsd	%xmm12, %xmm1, %xmm1
	vmovapd	-4195568(%rbp), %ymm9
	leaq	-4195376(%rbp), %rbx
	vmovapd	-4195536(%rbp), %ymm7
	leaq	8(%rbx), %rdi
	vmovapd	-4195504(%rbp), %ymm6
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	leaq	4(%rbx), %rsi
	leaq	-4194864(%rbp), %rbx
	leaq	-504(%r15), %rdx
	leaq	4(%rbx), %rcx
	vsubsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm2, %xmm3
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm3, %xmm1
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm0, %xmm4, %xmm4
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm1
	vmovss	%xmm4, -512(%r15)
	vmovss	%xmm1, -508(%r15)
	.p2align 4,,10
	.p2align 3
.L1526:
	vmovups	(%rsi,%rax), %ymm4
	vmovaps	-4195376(%rbp,%rax), %ymm5
	vcvtps2pd	%xmm4, %ymm10
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vmovups	(%rdi,%rax), %ymm3
	vcvtps2pd	%xmm5, %ymm11
	vextractf128	$0x1, %ymm5, %xmm5
	vmulpd	%ymm6, %ymm11, %ymm2
	vcvtps2pd	%xmm5, %ymm5
	vaddpd	%ymm10, %ymm2, %ymm2
	vmulpd	%ymm6, %ymm5, %ymm0
	vcvtps2pd	%xmm3, %ymm8
	vextractf128	$0x1, %ymm3, %xmm3
	vaddpd	%ymm4, %ymm0, %ymm0
	vmulpd	%ymm6, %ymm8, %ymm8
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm6, %ymm3, %ymm3
	vmovups	(%rcx,%rax), %ymm1
	vmulpd	%ymm7, %ymm5, %ymm5
	vaddpd	%ymm5, %ymm4, %ymm4
	vsubpd	%ymm8, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm1, %ymm0, %ymm2
	vmulpd	%ymm7, %ymm11, %ymm0
	vaddpd	%ymm0, %ymm10, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vaddpd	%ymm0, %ymm8, %ymm0
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm3, %ymm0, %ymm0
	vaddps	%ymm0, %ymm1, %ymm0
	vunpcklps	%ymm0, %ymm2, %ymm1
	vunpckhps	%ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm2, (%rdx,%rax,2)
	vmovups	%ymm0, 32(%rdx,%rax,2)
	addq	$32, %rax
	cmpq	$480, %rax
	jne	.L1526
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194896(%rbp), %xmm4, %xmm4
	vmulsd	%xmm12, %xmm4, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194888(%rbp), %xmm0, %xmm0
	vmulsd	%xmm12, %xmm0, %xmm5
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194892(%rbp), %xmm2, %xmm2
	vmovss	-4194380(%rbp), %xmm1
	vmulsd	%xmm12, %xmm2, %xmm10
	vaddsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm10, %xmm10
	vsubsd	%xmm5, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm1, %xmm3, %xmm3
	vmovss	%xmm3, 456(%r15)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm2, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	-4194376(%rbp), %xmm4
	vmovss	%xmm1, 460(%r15)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194884(%rbp), %xmm1, %xmm1
	vmulsd	%xmm12, %xmm1, %xmm8
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm8, %xmm10, %xmm10
	vaddsd	%xmm2, %xmm8, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm4, %xmm10, %xmm10
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	-4194372(%rbp), %xmm4
	vmovss	%xmm10, 464(%r15)
	vmovss	%xmm2, 468(%r15)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194880(%rbp), %xmm2, %xmm2
	vmulsd	%xmm12, %xmm2, %xmm10
	vaddsd	%xmm2, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm10, %xmm0
	vsubsd	%xmm10, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm4, %xmm5, %xmm5
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194876(%rbp), %xmm4, %xmm4
	vaddsd	%xmm2, %xmm4, %xmm2
	vmovss	%xmm0, 476(%r15)
	vmulsd	%xmm12, %xmm4, %xmm0
	vaddsd	%xmm4, %xmm10, %xmm10
	vmovss	%xmm5, 472(%r15)
	vmulsd	%xmm3, %xmm4, %xmm4
	vmovss	-4194368(%rbp), %xmm5
	vsubsd	%xmm0, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm0, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubss	%xmm5, %xmm8, %xmm8
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vmovss	-4194364(%rbp), %xmm5
	vmovss	%xmm8, 480(%r15)
	vmovss	%xmm1, 484(%r15)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194872(%rbp), %xmm1, %xmm1
	vmulsd	%xmm12, %xmm1, %xmm8
	vaddsd	%xmm4, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm8, %xmm2
	vsubsd	%xmm8, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm0, %xmm8
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm5, %xmm10, %xmm10
	vmovss	-4194360(%rbp), %xmm5
	vmovss	%xmm2, 492(%r15)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194868(%rbp), %xmm2, %xmm2
	vmulsd	%xmm12, %xmm2, %xmm12
	vmulsd	%xmm14, %xmm2, %xmm14
	vaddsd	%xmm1, %xmm4, %xmm4
	vmovss	%xmm10, 488(%r15)
	vmulsd	%xmm13, %xmm2, %xmm2
	vsubsd	%xmm1, %xmm0, %xmm0
	vsubsd	%xmm12, %xmm8, %xmm8
	vaddsd	%xmm3, %xmm12, %xmm12
	vxorps	%xmm3, %xmm3, %xmm3
	vaddsd	%xmm14, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vcvtsd2ss	%xmm12, %xmm3, %xmm3
	vaddss	%xmm3, %xmm5, %xmm3
	vsubss	%xmm5, %xmm8, %xmm8
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm3, 500(%r15)
	vmovss	-4194356(%rbp), %xmm3
	vmovss	%xmm8, 496(%r15)
	vsubss	%xmm3, %xmm4, %xmm4
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm4, 504(%r15)
	vmovss	%xmm0, 508(%r15)
.L1524:
	addq	$1024, %r15
	cmpq	%r12, %r15
	jne	.L1527
	leaq	262144(%r15), %r12
	cmpq	%r13, %r12
	jne	.L1516
	movq	-4195600(%rbp), %rbx
	xorl	%r10d, %r10d
	movq	-4195440(%rbp), %r11
.L1528:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-4195472(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$8, %rdi
	cmpl	$256, %r8d
	je	.L1529
.L1732:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L1530:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$262144, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -262144(%rcx)
	cmpl	$256, %edx
	jne	.L1530
	addl	$1, %r8d
	addq	$262148, %r9
	addq	$262144, %rsi
	cmpl	$256, %r8d
	jne	.L1732
.L1529:
	addl	$1, %r10d
	addq	$1024, %r11
	cmpl	$256, %r10d
	jne	.L1528
	movl	-4195428(%rbp), %r12d
	movq	%r13, -4195424(%rbp)
	vmovapd	.LC38(%rip), %ymm6
	vmovapd	.LC39(%rip), %ymm8
	vmovapd	%ymm6, %ymm7
.L1557:
	leaq	-262144(%rbx), %r15
	movq	%r15, %r14
.L1543:
	leaq	-512(%r14), %r13
	vmovapd	%ymm7, -4195536(%rbp)
	movl	$512, %edx
	vmovapd	%ymm8, -4195504(%rbp)
	movq	%r13, %rsi
	vmovapd	%ymm6, -4195472(%rbp)
	leaq	-4195376(%rbp), %rdi
	cmpl	$1, %r12d
	jbe	.L1733
	call	memcpy
	movl	$512, %edx
	movq	%r14, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	vxorpd	%xmm2, %xmm2, %xmm2
	xorl	%eax, %eax
	vmovsd	.LC36(%rip), %xmm13
	vcvtss2sd	-4195376(%rbp), %xmm2, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovsd	.LC35(%rip), %xmm14
	vcvtss2sd	-4195372(%rbp), %xmm3, %xmm3
	vmulsd	%xmm13, %xmm2, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195368(%rbp), %xmm0, %xmm0
	vmovsd	.LC34(%rip), %xmm5
	vmulsd	%xmm14, %xmm2, %xmm2
	vmovss	-4194864(%rbp), %xmm1
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovapd	-4195536(%rbp), %ymm7
	leaq	-4195376(%rbp), %rdi
	vmovapd	-4195504(%rbp), %ymm8
	vmulsd	.LC33(%rip), %xmm3, %xmm3
	addq	$8, %rdi
	leaq	-4195376(%rbp), %rsi
	vmovapd	-4195472(%rbp), %ymm6
	leaq	-4194864(%rbp), %rdx
	addq	$4, %rsi
	addq	$4, %rdx
	vsubsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm3, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm1, %xmm4, %xmm4
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm4, -512(%r14)
	vmovss	%xmm0, -508(%r14)
	.p2align 4,,10
	.p2align 3
.L1542:
	vmovups	(%rdi,%rax), %ymm0
	vmovups	(%rsi,%rax), %ymm4
	vcvtps2pd	%xmm0, %ymm1
	vextractf128	$0x1, %ymm0, %xmm2
	vmulpd	%ymm6, %ymm1, %ymm1
	vmovaps	-4195376(%rbp,%rax), %ymm9
	vcvtps2pd	%xmm4, %ymm3
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm2, %ymm2
	vcvtps2pd	%xmm9, %ymm12
	vextractf128	$0x1, %ymm9, %xmm9
	vmulpd	%ymm6, %ymm12, %ymm0
	vcvtps2pd	%xmm9, %ymm9
	vaddpd	%ymm3, %ymm0, %ymm0
	vmulpd	%ymm6, %ymm9, %ymm11
	vmulpd	%ymm8, %ymm12, %ymm12
	vmulpd	%ymm8, %ymm9, %ymm9
	vaddpd	%ymm4, %ymm11, %ymm11
	vmulpd	%ymm6, %ymm2, %ymm2
	vmovups	(%rdx,%rax), %ymm10
	vaddpd	%ymm12, %ymm3, %ymm3
	vaddpd	%ymm9, %ymm4, %ymm4
	vsubpd	%ymm1, %ymm0, %ymm0
	vsubpd	%ymm2, %ymm11, %ymm11
	vaddpd	%ymm3, %ymm1, %ymm1
	vaddpd	%ymm4, %ymm2, %ymm2
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm11, %xmm11
	vinsertf128	$0x1, %xmm11, %ymm0, %ymm0
	vsubps	%ymm10, %ymm0, %ymm0
	vcvtpd2psy	%ymm1, %xmm3
	vcvtpd2psy	%ymm2, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm3, %ymm1
	vaddps	%ymm1, %ymm10, %ymm1
	vunpcklps	%ymm1, %ymm0, %ymm2
	vunpckhps	%ymm1, %ymm0, %ymm0
	vinsertf128	$1, %xmm0, %ymm2, %ymm1
	vperm2f128	$49, %ymm0, %ymm2, %ymm0
	vmovups	%ymm1, 8(%r13,%rax,2)
	vmovups	%ymm0, 40(%r13,%rax,2)
	addq	$32, %rax
	cmpq	$480, %rax
	jne	.L1542
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194896(%rbp), %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm1
	vmovsd	.LC40(%rip), %xmm12
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194888(%rbp), %xmm4, %xmm4
	vmulsd	%xmm5, %xmm4, %xmm9
	vxorpd	%xmm3, %xmm3, %xmm3
	vmulsd	%xmm12, %xmm0, %xmm0
	vcvtss2sd	-4194892(%rbp), %xmm3, %xmm3
	vmovss	-4194380(%rbp), %xmm2
	vxorpd	%xmm15, %xmm15, %xmm15
	vmulsd	%xmm5, %xmm3, %xmm11
	vcvtss2sd	-4194880(%rbp), %xmm15, %xmm15
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm3, %xmm0
	vsubsd	%xmm9, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm11, %xmm11
	vaddsd	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm12, %xmm3, %xmm0
	vmovss	-4194372(%rbp), %xmm3
	vmovss	%xmm1, 456(%r14)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194884(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm10
	vaddsd	%xmm9, %xmm1, %xmm9
	vmovss	%xmm2, 460(%r14)
	vmovss	-4194376(%rbp), %xmm2
	vaddsd	%xmm4, %xmm0, %xmm0
	vsubsd	%xmm10, %xmm11, %xmm11
	vaddsd	%xmm0, %xmm10, %xmm0
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm2, %xmm11, %xmm11
	vaddsd	%xmm15, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	-4194368(%rbp), %xmm2
	vmovss	%xmm11, 464(%r14)
	vmulsd	%xmm5, %xmm15, %xmm11
	vmovss	%xmm0, 468(%r14)
	vmulsd	%xmm12, %xmm4, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194876(%rbp), %xmm4, %xmm4
	vsubsd	%xmm11, %xmm9, %xmm9
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm3, %xmm9, %xmm9
	vaddsd	%xmm0, %xmm11, %xmm0
	vmovss	%xmm9, 472(%r14)
	vaddsd	%xmm4, %xmm11, %xmm11
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm12, %xmm1, %xmm0
	vmulsd	%xmm12, %xmm15, %xmm1
	vmovss	%xmm3, 476(%r14)
	vmulsd	%xmm5, %xmm4, %xmm3
	vaddsd	%xmm0, %xmm15, %xmm0
	vaddsd	%xmm1, %xmm4, %xmm1
	vmulsd	%xmm12, %xmm4, %xmm4
	vsubsd	%xmm3, %xmm10, %xmm10
	vaddsd	%xmm0, %xmm3, %xmm0
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm2, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm2
	vmovss	-4194364(%rbp), %xmm0
	vmovss	%xmm10, 480(%r14)
	vmovss	%xmm2, 484(%r14)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194872(%rbp), %xmm2, %xmm2
	vmulsd	%xmm5, %xmm2, %xmm9
	vaddsd	%xmm2, %xmm3, %xmm10
	vsubsd	%xmm9, %xmm11, %xmm11
	vaddsd	%xmm1, %xmm9, %xmm1
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-4194868(%rbp), %xmm9, %xmm9
	vmulsd	%xmm5, %xmm9, %xmm5
	vmulsd	%xmm14, %xmm9, %xmm14
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm0, %xmm11, %xmm11
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	-4194360(%rbp), %xmm1
	vmovss	%xmm11, 488(%r14)
	vsubsd	%xmm5, %xmm10, %xmm10
	vmovss	%xmm0, 492(%r14)
	vaddsd	%xmm4, %xmm2, %xmm0
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm1, %xmm10, %xmm10
	vaddsd	%xmm0, %xmm5, %xmm5
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm10, 496(%r14)
	vaddsd	%xmm2, %xmm4, %xmm4
	vcvtsd2ss	%xmm5, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm13, %xmm9, %xmm0
	vsubsd	%xmm2, %xmm3, %xmm3
	vaddsd	%xmm14, %xmm4, %xmm4
	vmovss	%xmm1, 500(%r14)
	vmovss	-4194356(%rbp), %xmm1
	vaddsd	%xmm0, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vxorps	%xmm0, %xmm0, %xmm0
	vsubss	%xmm1, %xmm4, %xmm4
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm4, 504(%r14)
	vmovss	%xmm0, 508(%r14)
.L1540:
	addq	$1024, %r14
	cmpq	%rbx, %r14
	jne	.L1543
	leaq	-261632(%rbx), %r8
	xorl	%edi, %edi
	leaq	-262656(%rbx), %rsi
	addl	$1, %edi
	cmpl	$256, %edi
	je	.L1734
.L1692:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L1545:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	addq	$1024, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -1024(%rdx)
	cmpl	$256, %eax
	jne	.L1545
	addl	$1, %edi
	addq	$1028, %r8
	addq	$1024, %rsi
	cmpl	$256, %edi
	jne	.L1692
.L1734:
	vmovapd	.LC39(%rip), %ymm9
	vmovapd	.LC31(%rip), %ymm10
.L1544:
	leaq	-512(%r15), %r14
	vmovapd	%ymm7, -4195600(%rbp)
	movl	$512, %edx
	vmovapd	%ymm8, -4195568(%rbp)
	movq	%r14, %rsi
	vmovapd	%ymm6, -4195536(%rbp)
	leaq	-4195376(%rbp), %rdi
	vmovapd	%ymm10, -4195504(%rbp)
	vmovapd	%ymm9, -4195472(%rbp)
	cmpl	$1, %r12d
	jbe	.L1735
	call	memcpy
	movl	$512, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	vxorpd	%xmm3, %xmm3, %xmm3
	xorl	%eax, %eax
	vmovsd	.LC36(%rip), %xmm15
	vcvtss2sd	-4195376(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	movabsq	$4603804719079489536, %rdx
	vcvtss2sd	-4195372(%rbp), %xmm2, %xmm2
	vmovq	%rdx, %xmm7
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovsd	.LC34(%rip), %xmm14
	vcvtss2sd	-4195368(%rbp), %xmm0, %xmm0
	vmulsd	%xmm15, %xmm3, %xmm4
	vmovss	-4194864(%rbp), %xmm1
	vmulsd	%xmm7, %xmm3, %xmm3
	vmovapd	-4195568(%rbp), %ymm8
	vmulsd	%xmm14, %xmm0, %xmm0
	vmovapd	-4195600(%rbp), %ymm7
	leaq	-4195376(%rbp), %rdi
	vmovapd	-4195536(%rbp), %ymm6
	leaq	-4194864(%rbp), %rsi
	vmovapd	-4195504(%rbp), %ymm10
	vmovapd	-4195472(%rbp), %ymm9
	leaq	8(%rdi), %r8
	addq	$4, %rsi
	addq	$4, %rdi
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vsubsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm1, %xmm4, %xmm4
	vcvtsd2ss	%xmm2, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm4, -512(%r15)
	vmovss	%xmm0, -508(%r15)
	.p2align 4,,10
	.p2align 3
.L1556:
	vmovups	(%r8,%rax), %ymm1
	vmovups	(%rdi,%rax), %ymm5
	vmovaps	-4195376(%rbp,%rax), %ymm11
	vcvtps2pd	%xmm1, %ymm2
	vmulpd	%ymm7, %ymm2, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm5, %ymm12
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm11, %ymm13
	vextractf128	$0x1, %ymm11, %xmm11
	vmulpd	%ymm7, %ymm13, %ymm0
	vcvtps2pd	%xmm11, %ymm11
	vaddpd	%ymm12, %ymm0, %ymm0
	vmulpd	%ymm7, %ymm11, %ymm2
	vmulpd	%ymm7, %ymm1, %ymm1
	vmulpd	%ymm9, %ymm11, %ymm11
	vaddpd	%ymm5, %ymm2, %ymm2
	vmovups	(%rsi,%rax), %ymm4
	vaddpd	%ymm11, %ymm5, %ymm5
	vsubpd	%ymm3, %ymm0, %ymm0
	vsubpd	%ymm1, %ymm2, %ymm2
	vaddpd	%ymm5, %ymm1, %ymm1
	vcvtpd2psy	%ymm0, %xmm0
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm0, %ymm0
	vmulpd	%ymm9, %ymm13, %ymm2
	vaddpd	%ymm2, %ymm12, %ymm2
	vsubps	%ymm4, %ymm0, %ymm0
	vcvtpd2psy	%ymm1, %xmm1
	vaddpd	%ymm2, %ymm3, %ymm2
	vcvtpd2psy	%ymm2, %xmm2
	vinsertf128	$0x1, %xmm1, %ymm2, %ymm1
	vaddps	%ymm1, %ymm4, %ymm1
	vunpcklps	%ymm1, %ymm0, %ymm2
	vunpckhps	%ymm1, %ymm0, %ymm0
	vinsertf128	$1, %xmm0, %ymm2, %ymm1
	vperm2f128	$49, %ymm0, %ymm2, %ymm0
	vmovups	%ymm1, 8(%r14,%rax,2)
	vmovups	%ymm0, 40(%r14,%rax,2)
	addq	$32, %rax
	cmpq	$480, %rax
	jne	.L1556
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194896(%rbp), %xmm0, %xmm0
	vmulsd	%xmm14, %xmm0, %xmm1
	vmovsd	.LC40(%rip), %xmm13
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194888(%rbp), %xmm4, %xmm4
	vmulsd	%xmm14, %xmm4, %xmm5
	vxorpd	%xmm3, %xmm3, %xmm3
	vmulsd	%xmm13, %xmm0, %xmm0
	vcvtss2sd	-4194892(%rbp), %xmm3, %xmm3
	vmovss	-4194380(%rbp), %xmm2
	vmulsd	%xmm14, %xmm3, %xmm12
	vaddsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm3, %xmm0
	vsubsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm12, %xmm12
	vaddsd	%xmm0, %xmm5, %xmm0
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm13, %xmm3, %xmm0
	vmovss	%xmm1, 456(%r15)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194884(%rbp), %xmm1, %xmm1
	vmulsd	%xmm14, %xmm1, %xmm11
	vmulsd	%xmm13, %xmm4, %xmm3
	vaddsd	%xmm5, %xmm1, %xmm5
	vmovss	%xmm2, 460(%r15)
	vmovss	-4194376(%rbp), %xmm2
	vaddsd	%xmm4, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4194876(%rbp), %xmm4, %xmm4
	vsubsd	%xmm11, %xmm12, %xmm12
	vaddsd	%xmm1, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm11, %xmm0
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vsubss	%xmm2, %xmm12, %xmm12
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	-4194372(%rbp), %xmm2
	vmovss	%xmm12, 464(%r15)
	vmovss	%xmm0, 468(%r15)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194880(%rbp), %xmm0, %xmm0
	vmulsd	%xmm14, %xmm0, %xmm12
	vaddsd	%xmm0, %xmm11, %xmm11
	vsubsd	%xmm12, %xmm5, %xmm5
	vaddsd	%xmm3, %xmm12, %xmm3
	vaddsd	%xmm4, %xmm12, %xmm12
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm2, %xmm5, %xmm5
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm3
	vmulsd	%xmm13, %xmm1, %xmm2
	vmovss	%xmm5, 472(%r15)
	vmovss	-4194368(%rbp), %xmm5
	vmovss	%xmm3, 476(%r15)
	vmulsd	%xmm14, %xmm4, %xmm3
	vaddsd	%xmm2, %xmm0, %xmm2
	vsubsd	%xmm3, %xmm11, %xmm11
	vaddsd	%xmm2, %xmm3, %xmm2
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm5, %xmm11, %xmm11
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm5, %xmm2
	vmulsd	%xmm13, %xmm0, %xmm5
	vmovss	%xmm11, 480(%r15)
	vmovss	-4194364(%rbp), %xmm11
	vmovss	%xmm2, 484(%r15)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194872(%rbp), %xmm2, %xmm2
	vmulsd	%xmm14, %xmm2, %xmm1
	vaddsd	%xmm5, %xmm4, %xmm5
	vmulsd	%xmm13, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm12, %xmm12
	vaddsd	%xmm5, %xmm1, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-4194868(%rbp), %xmm5, %xmm5
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vsubss	%xmm11, %xmm12, %xmm12
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm0
	vmovss	-4194360(%rbp), %xmm1
	vaddsd	%xmm2, %xmm3, %xmm11
	vmovss	%xmm12, 488(%r15)
	vmovss	%xmm0, 492(%r15)
	vmulsd	%xmm14, %xmm5, %xmm0
	vsubsd	%xmm0, %xmm11, %xmm11
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm1, %xmm11, %xmm11
	vmovss	%xmm11, 496(%r15)
	vaddsd	%xmm4, %xmm2, %xmm11
	vmulsd	.LC33(%rip), %xmm2, %xmm2
	vaddsd	%xmm11, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm4, %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm1
	vmovq	%rdx, %xmm0
	vmulsd	%xmm0, %xmm5, %xmm0
	vsubsd	%xmm2, %xmm3, %xmm3
	vmovss	%xmm1, 500(%r15)
	vmovss	-4194356(%rbp), %xmm1
	vaddsd	%xmm0, %xmm4, %xmm4
	vmulsd	%xmm15, %xmm5, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm1, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm3
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm4, 504(%r15)
	vcvtsd2ss	%xmm3, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vmovss	%xmm0, 508(%r15)
.L1554:
	addq	$1024, %r15
	cmpq	%r15, %rbx
	jne	.L1544
	addq	$262144, %rbx
	cmpq	%rbx, -4195424(%rbp)
	jne	.L1557
	movq	-4195408(%rbp), %rdi
	testq	%rdi, %rdi
	je	.L1353
	call	_ZdlPv
.L1353:
	leaq	-48(%rbp), %rsp
	popq	%rbx
	popq	%r10
	.cfi_remember_state
	.cfi_def_cfa 10, 0
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	leaq	-8(%r10), %rsp
	.cfi_def_cfa 7, 8
	ret
	.p2align 4,,10
	.p2align 3
.L1726:
	.cfi_restore_state
	call	memcpy
	movl	$256, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	cmpb	$0, -4195416(%rbp)
	vmovapd	-4195504(%rbp), %ymm6
	vmovapd	-4195536(%rbp), %ymm7
	je	.L1479
	vmovaps	-4195376(%rbp), %ymm1
	vmovaps	-4194864(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195344(%rbp), %ymm1
	vmovaps	%ymm0, -4195376(%rbp)
	vmovaps	-4194832(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195312(%rbp), %ymm1
	vmovaps	%ymm0, -4195344(%rbp)
	vmovaps	-4194800(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195280(%rbp), %ymm1
	vmovaps	%ymm0, -4195312(%rbp)
	vmovaps	-4194768(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195248(%rbp), %ymm1
	vmovaps	%ymm0, -4195280(%rbp)
	vmovaps	-4194736(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195216(%rbp), %ymm1
	vmovaps	%ymm0, -4195248(%rbp)
	vmovaps	-4194704(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195184(%rbp), %ymm1
	vmovaps	%ymm0, -4195216(%rbp)
	vmovaps	-4194672(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4195152(%rbp), %ymm1
	vmovaps	%ymm0, -4195184(%rbp)
	vmovaps	-4194640(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm2, %ymm2
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195152(%rbp)
.L1479:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1478:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, (%rbx,%rax,2)
	addq	$4, %rax
	cmpq	$256, %rax
	jne	.L1478
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm8
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC21(%rip), %xmm9
	vcvtss2sd	-4195368(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm1, %xmm1
	movl	$1, %edi
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovsd	.LC22(%rip), %xmm4
	vmulsd	%xmm9, %xmm0, %xmm0
	movl	$62, %r8d
	vmovsd	.LC26(%rip), %xmm3
	vxorpd	%xmm12, %xmm12, %xmm12
	vmulsd	%xmm8, %xmm2, %xmm2
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-4195372(%rbp), %xmm10, %xmm10
	vcvtss2sd	-4195368(%rbp), %xmm12, %xmm12
	leaq	-4195376(%rbp), %rax
	vcvtss2sd	-4195376(%rbp), %xmm11, %xmm11
	vmovsd	.LC27(%rip), %xmm5
	leaq	12(%rax), %rcx
	leaq	-4194864(%rbp), %rax
	vaddsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195364(%rbp), %xmm0, %xmm0
	vmulsd	%xmm4, %xmm0, %xmm0
	leaq	4(%rax), %rdx
	leaq	-244(%r15), %rax
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, -252(%r15)
	.p2align 4,,10
	.p2align 3
.L1480:
	vmulsd	%xmm5, %xmm11, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rcx), %xmm0, %xmm0
	addl	$4, %edi
	vmulsd	%xmm3, %xmm10, %xmm2
	movl	%r8d, %esi
	addq	$16, %rcx
	vmulsd	%xmm3, %xmm12, %xmm1
	subl	%edi, %esi
	addq	$16, %rdx
	vmulsd	%xmm4, %xmm0, %xmm13
	addq	$32, %rax
	vmulsd	%xmm5, %xmm10, %xmm10
	vmulsd	%xmm5, %xmm12, %xmm12
	vaddsd	%xmm2, %xmm11, %xmm2
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-12(%rcx), %xmm11, %xmm11
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm10, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8(%rcx), %xmm10, %xmm10
	vsubsd	%xmm13, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm11, %xmm13
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-16(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -32(%rax)
	vmulsd	%xmm3, %xmm0, %xmm2
	vmulsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm12, %xmm2
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-4(%rcx), %xmm12, %xmm12
	vsubsd	%xmm13, %xmm1, %xmm1
	vmulsd	%xmm4, %xmm10, %xmm13
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-12(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -24(%rax)
	vmulsd	%xmm3, %xmm11, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm1
	vsubsd	%xmm13, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-8(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -16(%rax)
	vmulsd	%xmm3, %xmm10, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm12, %xmm2
	vsubsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	$61, %edi
	jne	.L1480
	leaq	236(%r15), %rdx
	addl	$61, %esi
	movl	$61, %eax
	vmovapd	%xmm4, %xmm10
	.p2align 4,,10
	.p2align 3
.L1481:
	leal	-1(%rax), %ecx
	vxorpd	%xmm1, %xmm1, %xmm1
	addq	$8, %rdx
	leal	1(%rax), %edi
	movslq	%ecx, %rcx
	vcvtss2sd	-4195376(%rbp,%rcx,4), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm2
	movslq	%eax, %rcx
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rcx,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm0
	movslq	%edi, %r8
	addl	$2, %eax
	cltq
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp,%r8,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rax,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%edi, %eax
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rcx,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rdx)
	cmpl	%esi, %edi
	jne	.L1481
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195136(%rbp), %xmm3, %xmm3
	vcvtss2sd	-4195132(%rbp), %xmm0, %xmm0
	vmulsd	%xmm10, %xmm3, %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4195128(%rbp), %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm8, %xmm0, %xmm5
	vcvtss2sd	-4195124(%rbp), %xmm1, %xmm1
	vmulsd	%xmm9, %xmm2, %xmm9
	vmulsd	%xmm8, %xmm1, %xmm8
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vmulsd	.LC28(%rip), %xmm3, %xmm3
	vsubsd	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm9, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm3
	vmovsd	.LC30(%rip), %xmm0
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-4194616(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 244(%r15)
	vmulsd	%xmm0, %xmm2, %xmm4
	vmulsd	%xmm0, %xmm1, %xmm0
	vsubsd	%xmm4, %xmm3, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194612(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 252(%r15)
	jmp	.L1482
.L1728:
	call	memcpy
	movl	$256, %edx
	movq	%r14, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	cmpb	$0, -4195416(%rbp)
	vmovapd	-4195504(%rbp), %ymm6
	vmovapd	-4195536(%rbp), %ymm8
	vmovapd	-4195568(%rbp), %ymm7
	je	.L1495
	vmovaps	-4195376(%rbp), %ymm0
	vmovaps	-4194864(%rbp), %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vsubpd	%ymm3, %ymm2, %ymm2
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194832(%rbp), %ymm1
	vmovaps	%ymm0, -4195376(%rbp)
	vmovaps	-4195344(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194800(%rbp), %ymm1
	vmovaps	%ymm0, -4195344(%rbp)
	vmovaps	-4195312(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194768(%rbp), %ymm1
	vmovaps	%ymm0, -4195312(%rbp)
	vmovaps	-4195280(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194736(%rbp), %ymm1
	vmovaps	%ymm0, -4195280(%rbp)
	vmovaps	-4195248(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194704(%rbp), %ymm1
	vmovaps	%ymm0, -4195248(%rbp)
	vmovaps	-4195216(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194672(%rbp), %ymm1
	vmovaps	%ymm0, -4195216(%rbp)
	vmovaps	-4195184(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194640(%rbp), %ymm1
	vmovaps	%ymm0, -4195184(%rbp)
	vmovaps	-4195152(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195152(%rbp)
.L1495:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1494:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, 0(%r13,%rax,2)
	addq	$4, %rax
	cmpq	$256, %rax
	jne	.L1494
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp), %xmm0, %xmm0
	vmovsd	.LC20(%rip), %xmm10
	vcvtss2sd	-4195372(%rbp), %xmm1, %xmm1
	movl	$1, %edi
	vxorpd	%xmm9, %xmm9, %xmm9
	vmovsd	.LC21(%rip), %xmm11
	vmulsd	%xmm10, %xmm0, %xmm0
	movl	$62, %r8d
	vmovsd	.LC22(%rip), %xmm4
	vxorpd	%xmm13, %xmm13, %xmm13
	vmulsd	%xmm11, %xmm1, %xmm1
	vxorpd	%xmm12, %xmm12, %xmm12
	vmovsd	.LC26(%rip), %xmm3
	vcvtss2sd	-4195372(%rbp), %xmm9, %xmm9
	leaq	-4195376(%rbp), %rax
	vcvtss2sd	-4195368(%rbp), %xmm13, %xmm13
	vcvtss2sd	-4195376(%rbp), %xmm12, %xmm12
	vmovsd	.LC27(%rip), %xmm5
	leaq	12(%rax), %rsi
	leaq	-4194864(%rbp), %rax
	leaq	4(%rax), %rdx
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm10, %xmm1, %xmm1
	leaq	-244(%r14), %rax
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195364(%rbp), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, -252(%r14)
	.p2align 4,,10
	.p2align 3
.L1496:
	vmulsd	%xmm5, %xmm12, %xmm12
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rsi), %xmm0, %xmm0
	addl	$4, %edi
	vmulsd	%xmm3, %xmm9, %xmm2
	movl	%r8d, %ecx
	addq	$16, %rsi
	vmulsd	%xmm3, %xmm13, %xmm1
	subl	%edi, %ecx
	addq	$16, %rdx
	vmulsd	%xmm4, %xmm0, %xmm14
	addq	$32, %rax
	vmulsd	%xmm5, %xmm9, %xmm9
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm2, %xmm12, %xmm2
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-12(%rsi), %xmm12, %xmm12
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm9, %xmm1, %xmm1
	vxorpd	%xmm9, %xmm9, %xmm9
	vcvtss2sd	-8(%rsi), %xmm9, %xmm9
	vsubsd	%xmm14, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm12, %xmm14
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-16(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -32(%rax)
	vmulsd	%xmm3, %xmm0, %xmm2
	vmulsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm13, %xmm2
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-4(%rsi), %xmm13, %xmm13
	vsubsd	%xmm14, %xmm1, %xmm1
	vmulsd	%xmm4, %xmm9, %xmm14
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-12(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -24(%rax)
	vmulsd	%xmm3, %xmm12, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm1
	vsubsd	%xmm14, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-8(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -16(%rax)
	vmulsd	%xmm3, %xmm9, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm0
	vmulsd	%xmm4, %xmm13, %xmm2
	vsubsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	$61, %edi
	jne	.L1496
	leaq	236(%r14), %rax
	addl	$61, %ecx
	vmovapd	%xmm4, %xmm9
	.p2align 4,,10
	.p2align 3
.L1497:
	leal	-1(%rdi), %edx
	vxorpd	%xmm1, %xmm1, %xmm1
	addq	$8, %rax
	leal	1(%rdi), %esi
	movslq	%edx, %rdx
	vcvtss2sd	-4195376(%rbp,%rdx,4), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm2
	movslq	%edi, %rdx
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdx,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm0
	movslq	%esi, %r8
	addl	$2, %edi
	movslq	%edi, %rdi
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp,%r8,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%esi, %edi
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rdx,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	%ecx, %esi
	jne	.L1497
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195136(%rbp), %xmm0, %xmm0
	vcvtss2sd	-4195132(%rbp), %xmm4, %xmm4
	vmulsd	%xmm9, %xmm0, %xmm1
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195128(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm10, %xmm4, %xmm5
	vcvtss2sd	-4195124(%rbp), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm3, %xmm11
	vmulsd	%xmm10, %xmm2, %xmm10
	vmulsd	.LC28(%rip), %xmm0, %xmm0
	vmulsd	.LC29(%rip), %xmm4, %xmm4
	vsubsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm11, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm10, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-4194616(%rbp), %xmm1, %xmm1
	vmovss	%xmm1, 244(%r14)
	vmovsd	.LC30(%rip), %xmm1
	vmulsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194612(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 252(%r14)
	jmp	.L1498
.L1730:
	call	memcpy
	movl	$256, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	testb	%r13b, %r13b
	vmovapd	-4195504(%rbp), %ymm9
	vmovapd	-4195536(%rbp), %ymm6
	vmovapd	-4195568(%rbp), %ymm8
	vmovapd	-4195600(%rbp), %ymm7
	je	.L1509
	vmovaps	-4195376(%rbp), %ymm0
	vmovaps	-4194864(%rbp), %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vsubpd	%ymm3, %ymm2, %ymm2
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194832(%rbp), %ymm1
	vmovaps	%ymm0, -4195376(%rbp)
	vmovaps	-4195344(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194800(%rbp), %ymm1
	vmovaps	%ymm0, -4195344(%rbp)
	vmovaps	-4195312(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194768(%rbp), %ymm1
	vmovaps	%ymm0, -4195312(%rbp)
	vmovaps	-4195280(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194736(%rbp), %ymm1
	vmovaps	%ymm0, -4195280(%rbp)
	vmovaps	-4195248(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194704(%rbp), %ymm1
	vmovaps	%ymm0, -4195248(%rbp)
	vmovaps	-4195216(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194672(%rbp), %ymm1
	vmovaps	%ymm0, -4195216(%rbp)
	vmovaps	-4195184(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	-4194640(%rbp), %ymm1
	vmovaps	%ymm0, -4195184(%rbp)
	vmovaps	-4195152(%rbp), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vmulpd	.LC31(%rip), %ymm3, %ymm3
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vsubpd	%ymm3, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm1, %ymm0, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195152(%rbp)
.L1509:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1508:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, (%r14,%rax,2)
	addq	$4, %rax
	cmpq	$256, %rax
	jne	.L1508
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp), %xmm0, %xmm0
	vmovsd	.LC20(%rip), %xmm14
	vcvtss2sd	-4195372(%rbp), %xmm1, %xmm1
	movl	$1, %esi
	movl	$62, %edi
	vmovsd	.LC21(%rip), %xmm15
	vmulsd	%xmm14, %xmm0, %xmm0
	vxorpd	%xmm10, %xmm10, %xmm10
	vmovsd	.LC22(%rip), %xmm4
	vxorpd	%xmm12, %xmm12, %xmm12
	vmulsd	%xmm15, %xmm1, %xmm1
	vxorpd	%xmm11, %xmm11, %xmm11
	vmovsd	.LC26(%rip), %xmm3
	vcvtss2sd	-4195372(%rbp), %xmm10, %xmm10
	leaq	-4195376(%rbp), %rax
	vcvtss2sd	-4195368(%rbp), %xmm12, %xmm12
	vcvtss2sd	-4195376(%rbp), %xmm11, %xmm11
	vmovsd	.LC27(%rip), %xmm5
	leaq	12(%rax), %rcx
	leaq	-4194864(%rbp), %rax
	leaq	4(%rax), %rdx
	vaddsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm14, %xmm1, %xmm1
	leaq	-244(%r15), %rax
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195364(%rbp), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, -252(%r15)
	.p2align 4,,10
	.p2align 3
.L1510:
	vmulsd	%xmm5, %xmm11, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rcx), %xmm0, %xmm0
	addl	$4, %esi
	vmulsd	%xmm3, %xmm10, %xmm2
	movl	%edi, %r8d
	addq	$16, %rcx
	vmulsd	%xmm3, %xmm12, %xmm1
	subl	%esi, %r8d
	addq	$16, %rdx
	vmulsd	%xmm5, %xmm10, %xmm10
	addq	$32, %rax
	vmulsd	%xmm5, %xmm12, %xmm12
	vaddsd	%xmm2, %xmm11, %xmm2
	vmulsd	%xmm4, %xmm0, %xmm11
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm10, %xmm1, %xmm1
	vsubsd	%xmm11, %xmm2, %xmm2
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-12(%rcx), %xmm11, %xmm11
	vmulsd	%xmm4, %xmm11, %xmm10
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-16(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -32(%rax)
	vmulsd	%xmm3, %xmm0, %xmm2
	vmulsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm12, %xmm2
	vsubsd	%xmm10, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	vcvtss2sd	-8(%rcx), %xmm10, %xmm10
	vmulsd	%xmm4, %xmm10, %xmm12
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-12(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -24(%rax)
	vmulsd	%xmm3, %xmm11, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	%xmm3, %xmm10, %xmm1
	vsubsd	%xmm12, %xmm2, %xmm2
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-4(%rcx), %xmm12, %xmm12
	vaddsd	%xmm1, %xmm0, %xmm0
	vmulsd	%xmm4, %xmm12, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-8(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -16(%rax)
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4(%rdx), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	$61, %esi
	jne	.L1510
	leaq	236(%r15), %rdx
	movl	$61, %eax
	vmovapd	%xmm4, %xmm10
	leal	61(%r8), %esi
	.p2align 4,,10
	.p2align 3
.L1511:
	leal	-1(%rax), %edi
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	addq	$8, %rdx
	leal	1(%rax), %ecx
	movslq	%edi, %rdi
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm2
	movslq	%eax, %rdi
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm1
	movslq	%ecx, %r8
	addl	$2, %eax
	vcvtss2sd	-4195376(%rbp,%r8,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	cltq
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rax,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%ecx, %eax
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rdi,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rdx)
	cmpl	%esi, %ecx
	jne	.L1511
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-4195136(%rbp), %xmm0, %xmm0
	vcvtss2sd	-4195132(%rbp), %xmm4, %xmm4
	vmulsd	%xmm10, %xmm0, %xmm1
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-4195128(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vmulsd	%xmm14, %xmm4, %xmm5
	vcvtss2sd	-4195124(%rbp), %xmm2, %xmm2
	vmulsd	%xmm15, %xmm3, %xmm15
	vmulsd	%xmm14, %xmm2, %xmm14
	vmulsd	.LC28(%rip), %xmm0, %xmm0
	vmulsd	.LC29(%rip), %xmm4, %xmm4
	vsubsd	%xmm5, %xmm1, %xmm1
	vaddsd	%xmm15, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vaddsd	%xmm14, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-4194616(%rbp), %xmm1, %xmm1
	vmovss	%xmm1, 244(%r15)
	vmovsd	.LC30(%rip), %xmm1
	vmulsd	%xmm1, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm2, %xmm2
	vsubsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194612(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 252(%r15)
	jmp	.L1512
.L1731:
	call	memcpy
	movl	$512, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	cmpb	$0, -4195416(%rbp)
	vmovapd	-4195504(%rbp), %ymm6
	vmovapd	-4195536(%rbp), %ymm7
	vmovapd	-4195568(%rbp), %ymm9
	je	.L1521
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1519:
	vmovaps	-4195376(%rbp,%rax), %ymm1
	vmovaps	-4194864(%rbp,%rax), %ymm0
	vcvtps2pd	%xmm1, %ymm3
	vextractf128	$0x1, %ymm1, %xmm1
	vcvtps2pd	%xmm1, %ymm1
	vcvtps2pd	%xmm0, %ymm2
	vextractf128	$0x1, %ymm0, %xmm0
	vmulpd	%ymm9, %ymm2, %ymm2
	vcvtps2pd	%xmm0, %ymm0
	vsubpd	%ymm2, %ymm3, %ymm2
	vmulpd	%ymm9, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm2, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195376(%rbp,%rax)
	addq	$32, %rax
	cmpq	$512, %rax
	jne	.L1519
.L1521:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1520:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, (%rbx,%rax,2)
	addq	$4, %rax
	cmpq	$512, %rax
	jne	.L1520
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm8
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vmovsd	.LC21(%rip), %xmm10
	vcvtss2sd	-4195368(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm1, %xmm1
	movl	$1, %edi
	vxorpd	%xmm11, %xmm11, %xmm11
	vmovsd	.LC22(%rip), %xmm4
	vmulsd	%xmm10, %xmm0, %xmm0
	movl	$126, %r8d
	vmovsd	.LC26(%rip), %xmm3
	vxorpd	%xmm13, %xmm13, %xmm13
	vmulsd	%xmm8, %xmm2, %xmm2
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-4195372(%rbp), %xmm11, %xmm11
	vcvtss2sd	-4195368(%rbp), %xmm13, %xmm13
	leaq	-4195376(%rbp), %rax
	vcvtss2sd	-4195376(%rbp), %xmm12, %xmm12
	vmovsd	.LC27(%rip), %xmm5
	leaq	12(%rax), %rcx
	leaq	-4194864(%rbp), %rax
	vaddsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195364(%rbp), %xmm0, %xmm0
	vmulsd	%xmm4, %xmm0, %xmm0
	leaq	4(%rax), %rdx
	leaq	-500(%r15), %rax
	vsubsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, -508(%r15)
	.p2align 4,,10
	.p2align 3
.L1522:
	vmulsd	%xmm5, %xmm12, %xmm12
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	(%rcx), %xmm0, %xmm0
	addl	$4, %edi
	vmulsd	%xmm3, %xmm11, %xmm2
	movl	%r8d, %esi
	addq	$16, %rcx
	vmulsd	%xmm3, %xmm13, %xmm1
	subl	%edi, %esi
	addq	$16, %rdx
	vmulsd	%xmm4, %xmm0, %xmm14
	addq	$32, %rax
	vmulsd	%xmm5, %xmm11, %xmm11
	vmulsd	%xmm5, %xmm13, %xmm13
	vaddsd	%xmm2, %xmm12, %xmm2
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-12(%rcx), %xmm12, %xmm12
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm11, %xmm1, %xmm1
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-8(%rcx), %xmm11, %xmm11
	vsubsd	%xmm14, %xmm2, %xmm2
	vmulsd	%xmm4, %xmm12, %xmm14
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-16(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -32(%rax)
	vmulsd	%xmm3, %xmm0, %xmm2
	vmulsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm13, %xmm2
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-4(%rcx), %xmm13, %xmm13
	vsubsd	%xmm14, %xmm1, %xmm1
	vmulsd	%xmm4, %xmm11, %xmm14
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-12(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -24(%rax)
	vmulsd	%xmm3, %xmm12, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm1
	vsubsd	%xmm14, %xmm2, %xmm2
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-8(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -16(%rax)
	vmulsd	%xmm3, %xmm11, %xmm2
	vaddsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm4, %xmm13, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-4(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -8(%rax)
	cmpl	$125, %edi
	jne	.L1522
	leaq	492(%r15), %rdx
	addl	$125, %esi
	movl	$125, %eax
	vmovapd	%xmm4, %xmm11
	.p2align 4,,10
	.p2align 3
.L1523:
	leal	-1(%rax), %ecx
	vxorpd	%xmm1, %xmm1, %xmm1
	addq	$8, %rdx
	leal	1(%rax), %edi
	movslq	%ecx, %rcx
	vcvtss2sd	-4195376(%rbp,%rcx,4), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm2
	movslq	%eax, %rcx
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rcx,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm0
	movslq	%edi, %r8
	addl	$2, %eax
	cltq
	vaddsd	%xmm0, %xmm2, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp,%r8,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rax,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%edi, %eax
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rcx,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rdx)
	cmpl	%esi, %edi
	jne	.L1523
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194880(%rbp), %xmm3, %xmm3
	vcvtss2sd	-4194876(%rbp), %xmm0, %xmm0
	vmulsd	%xmm11, %xmm3, %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194872(%rbp), %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm8, %xmm0, %xmm5
	vcvtss2sd	-4194868(%rbp), %xmm1, %xmm1
	vmulsd	%xmm10, %xmm2, %xmm10
	vmulsd	%xmm8, %xmm1, %xmm8
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vmulsd	.LC28(%rip), %xmm3, %xmm3
	vsubsd	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm10, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm3
	vmovsd	.LC30(%rip), %xmm0
	vaddsd	%xmm8, %xmm4, %xmm4
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-4194360(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 500(%r15)
	vmulsd	%xmm0, %xmm2, %xmm4
	vmulsd	%xmm0, %xmm1, %xmm0
	vsubsd	%xmm4, %xmm3, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194356(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 508(%r15)
	jmp	.L1524
.L1733:
	call	memcpy
	movl	$512, %edx
	movq	%r14, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	cmpb	$0, -4195416(%rbp)
	vmovapd	-4195472(%rbp), %ymm6
	vmovapd	-4195504(%rbp), %ymm8
	vmovapd	-4195536(%rbp), %ymm7
	je	.L1537
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1535:
	vmovaps	-4195376(%rbp,%rax), %ymm0
	vmovaps	-4194864(%rbp,%rax), %ymm2
	vcvtps2pd	%xmm0, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm2, %ymm1
	vmulpd	.LC31(%rip), %ymm1, %ymm1
	vsubpd	%ymm1, %ymm4, %ymm3
	vcvtps2pd	%xmm0, %ymm1
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	.LC31(%rip), %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195376(%rbp,%rax)
	addq	$32, %rax
	cmpq	$512, %rax
	jne	.L1535
.L1537:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1536:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, 0(%r13,%rax,2)
	addq	$4, %rax
	cmpq	$512, %rax
	jne	.L1536
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm9
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	movl	$1, %esi
	movl	$126, %edi
	vmovsd	.LC21(%rip), %xmm10
	vmulsd	%xmm9, %xmm1, %xmm1
	vxorpd	%xmm13, %xmm13, %xmm13
	vmovsd	.LC22(%rip), %xmm4
	vxorpd	%xmm11, %xmm11, %xmm11
	vmulsd	%xmm10, %xmm0, %xmm0
	vcvtss2sd	-4195372(%rbp), %xmm13, %xmm13
	vmovsd	.LC26(%rip), %xmm3
	vcvtss2sd	-4195368(%rbp), %xmm11, %xmm11
	vmovsd	.LC27(%rip), %xmm5
	leaq	-4195376(%rbp), %rax
	leaq	12(%rax), %rcx
	leaq	-4194864(%rbp), %rax
	leaq	4(%rax), %rdx
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm9, %xmm1, %xmm1
	leaq	-500(%r14), %rax
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195364(%rbp), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, -508(%r14)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp), %xmm0, %xmm0
	.p2align 4,,10
	.p2align 3
.L1538:
	vmulsd	%xmm5, %xmm0, %xmm0
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	(%rcx), %xmm12, %xmm12
	addl	$4, %esi
	vmulsd	%xmm3, %xmm13, %xmm2
	movl	%edi, %r9d
	addq	$16, %rcx
	vmulsd	%xmm3, %xmm11, %xmm1
	subl	%esi, %r9d
	addq	$16, %rdx
	vmulsd	%xmm5, %xmm13, %xmm13
	addq	$32, %rax
	vmulsd	%xmm5, %xmm11, %xmm11
	vaddsd	%xmm2, %xmm0, %xmm2
	vmulsd	%xmm4, %xmm12, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm13, %xmm1, %xmm1
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-12(%rcx), %xmm0, %xmm0
	vmulsd	%xmm4, %xmm0, %xmm13
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-16(%rdx), %xmm2, %xmm2
	vmovss	%xmm2, -32(%rax)
	vmulsd	%xmm3, %xmm12, %xmm2
	vmulsd	%xmm5, %xmm12, %xmm12
	vaddsd	%xmm2, %xmm1, %xmm1
	vaddsd	%xmm2, %xmm11, %xmm2
	vsubsd	%xmm13, %xmm1, %xmm1
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-8(%rcx), %xmm13, %xmm13
	vmulsd	%xmm4, %xmm13, %xmm11
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-12(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -24(%rax)
	vmulsd	%xmm3, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm12, %xmm12
	vmulsd	%xmm3, %xmm13, %xmm1
	vsubsd	%xmm11, %xmm2, %xmm2
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-4(%rcx), %xmm11, %xmm11
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-8(%rdx), %xmm2, %xmm2
	vaddsd	%xmm1, %xmm12, %xmm1
	vmovss	%xmm2, -16(%rax)
	vmulsd	%xmm4, %xmm11, %xmm2
	vsubsd	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-4(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -8(%rax)
	cmpl	$125, %esi
	jne	.L1538
	leaq	492(%r14), %rdx
	addl	$125, %r9d
	movl	$125, %eax
	vmovapd	%xmm4, %xmm11
	.p2align 4,,10
	.p2align 3
.L1539:
	leal	-1(%rax), %esi
	movslq	%eax, %rdi
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	leal	1(%rax), %ecx
	movslq	%esi, %rsi
	addl	$2, %eax
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm1
	movslq	%ecx, %rsi
	cltq
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	addq	$8, %rdx
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rax,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm1
	movl	%ecx, %eax
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rdi,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rdx)
	cmpl	%r9d, %ecx
	jne	.L1539
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4194880(%rbp), %xmm3, %xmm3
	vcvtss2sd	-4194876(%rbp), %xmm0, %xmm0
	vmulsd	%xmm11, %xmm3, %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194872(%rbp), %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm9, %xmm0, %xmm5
	vcvtss2sd	-4194868(%rbp), %xmm1, %xmm1
	vmulsd	.LC29(%rip), %xmm0, %xmm0
	vmulsd	.LC28(%rip), %xmm3, %xmm3
	vmulsd	%xmm10, %xmm2, %xmm10
	vmulsd	%xmm9, %xmm1, %xmm9
	vsubsd	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm3
	vmovsd	.LC30(%rip), %xmm0
	vaddsd	%xmm10, %xmm4, %xmm4
	vmulsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm9, %xmm4, %xmm4
	vsubsd	%xmm2, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-4194360(%rbp), %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm0
	vmovss	%xmm4, 500(%r14)
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194356(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 508(%r14)
	jmp	.L1540
.L1735:
	call	memcpy
	movl	$512, %edx
	movq	%r15, %rsi
	leaq	-4194864(%rbp), %rdi
	call	memcpy
	cmpb	$0, -4195416(%rbp)
	vmovapd	-4195472(%rbp), %ymm9
	vmovapd	-4195504(%rbp), %ymm10
	vmovapd	-4195536(%rbp), %ymm6
	vmovapd	-4195568(%rbp), %ymm8
	vmovapd	-4195600(%rbp), %ymm7
	je	.L1551
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1549:
	vmovaps	-4195376(%rbp,%rax), %ymm0
	vmovaps	-4194864(%rbp,%rax), %ymm2
	vcvtps2pd	%xmm0, %ymm4
	vextractf128	$0x1, %ymm0, %xmm0
	vcvtps2pd	%xmm2, %ymm1
	vmulpd	%ymm10, %ymm1, %ymm1
	vsubpd	%ymm1, %ymm4, %ymm3
	vcvtps2pd	%xmm0, %ymm1
	vextractf128	$0x1, %ymm2, %xmm0
	vcvtps2pd	%xmm0, %ymm0
	vmulpd	%ymm10, %ymm0, %ymm0
	vsubpd	%ymm0, %ymm1, %ymm0
	vcvtpd2psy	%ymm3, %xmm1
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmovaps	%ymm0, -4195376(%rbp,%rax)
	addq	$32, %rax
	cmpq	$512, %rax
	jne	.L1549
.L1551:
	xorl	%eax, %eax
	.p2align 4,,10
	.p2align 3
.L1550:
	vmovss	-4195376(%rbp,%rax), %xmm0
	vmovss	%xmm0, (%r14,%rax,2)
	addq	$4, %rax
	cmpq	$512, %rax
	jne	.L1550
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195376(%rbp), %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm11
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	movl	$1, %edi
	vxorpd	%xmm15, %xmm15, %xmm15
	vmovsd	.LC21(%rip), %xmm12
	vmulsd	%xmm11, %xmm1, %xmm1
	movl	$126, %r8d
	vmovsd	.LC22(%rip), %xmm3
	vxorpd	%xmm14, %xmm14, %xmm14
	vmulsd	%xmm12, %xmm0, %xmm0
	vcvtss2sd	-4195368(%rbp), %xmm15, %xmm15
	vmovsd	.LC26(%rip), %xmm2
	vcvtss2sd	-4195376(%rbp), %xmm14, %xmm14
	vmovsd	.LC27(%rip), %xmm4
	leaq	-4195376(%rbp), %rax
	leaq	12(%rax), %rsi
	leaq	-4194864(%rbp), %rax
	leaq	4(%rax), %rdx
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm11, %xmm1, %xmm1
	leaq	-500(%r15), %rax
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195364(%rbp), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, -508(%r15)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-4195372(%rbp), %xmm0, %xmm0
	.p2align 4,,10
	.p2align 3
.L1552:
	vmulsd	%xmm2, %xmm0, %xmm13
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	(%rsi), %xmm5, %xmm5
	addl	$4, %edi
	vmulsd	%xmm4, %xmm14, %xmm14
	movl	%r8d, %ecx
	addq	$16, %rsi
	vmulsd	%xmm2, %xmm15, %xmm1
	subl	%edi, %ecx
	addq	$16, %rdx
	vmulsd	%xmm4, %xmm0, %xmm0
	addq	$32, %rax
	vmulsd	%xmm4, %xmm15, %xmm15
	vaddsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm3, %xmm5, %xmm13
	vaddsd	%xmm1, %xmm14, %xmm14
	vaddsd	%xmm0, %xmm1, %xmm1
	vsubsd	%xmm13, %xmm14, %xmm14
	vmulsd	%xmm2, %xmm5, %xmm13
	vmulsd	%xmm4, %xmm5, %xmm5
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	-16(%rdx), %xmm14, %xmm14
	vaddsd	%xmm13, %xmm1, %xmm1
	vmovss	%xmm14, -32(%rax)
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-12(%rsi), %xmm14, %xmm14
	vmulsd	%xmm3, %xmm14, %xmm0
	vaddsd	%xmm13, %xmm15, %xmm15
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-8(%rsi), %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm13
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-12(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -24(%rax)
	vmulsd	%xmm2, %xmm14, %xmm1
	vaddsd	%xmm1, %xmm15, %xmm15
	vaddsd	%xmm1, %xmm5, %xmm5
	vmulsd	%xmm2, %xmm0, %xmm1
	vsubsd	%xmm13, %xmm15, %xmm15
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vaddss	-8(%rdx), %xmm15, %xmm15
	vaddsd	%xmm1, %xmm5, %xmm1
	vmovss	%xmm15, -16(%rax)
	vxorpd	%xmm15, %xmm15, %xmm15
	vcvtss2sd	-4(%rsi), %xmm15, %xmm15
	vmulsd	%xmm3, %xmm15, %xmm5
	vsubsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-4(%rdx), %xmm1, %xmm1
	vmovss	%xmm1, -8(%rax)
	cmpl	$125, %edi
	jne	.L1552
	leaq	492(%r15), %rax
	addl	$125, %ecx
	vmovapd	%xmm3, %xmm13
	.p2align 4,,10
	.p2align 3
.L1553:
	leal	-1(%rdi), %esi
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	addq	$8, %rax
	leal	1(%rdi), %edx
	movslq	%esi, %rsi
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm1, %xmm1
	vmulsd	%xmm4, %xmm1, %xmm5
	movslq	%edi, %rsi
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rsi,4), %xmm1, %xmm1
	vmulsd	%xmm2, %xmm1, %xmm1
	movslq	%edx, %r8
	addl	$2, %edi
	vcvtss2sd	-4195376(%rbp,%r8,4), %xmm0, %xmm0
	vmulsd	%xmm2, %xmm0, %xmm0
	movslq	%edi, %rdi
	vaddsd	%xmm1, %xmm5, %xmm1
	vaddsd	%xmm0, %xmm1, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4195376(%rbp,%rdi,4), %xmm1, %xmm1
	vmulsd	%xmm3, %xmm1, %xmm1
	movl	%edx, %edi
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194864(%rbp,%rsi,4), %xmm0, %xmm0
	vmovss	%xmm0, -8(%rax)
	cmpl	%ecx, %edx
	jne	.L1553
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-4194880(%rbp), %xmm3, %xmm3
	vcvtss2sd	-4194876(%rbp), %xmm2, %xmm2
	vmulsd	%xmm13, %xmm3, %xmm4
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-4194872(%rbp), %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vmulsd	%xmm11, %xmm2, %xmm5
	vcvtss2sd	-4194868(%rbp), %xmm0, %xmm0
	vmulsd	.LC29(%rip), %xmm2, %xmm2
	vmulsd	.LC28(%rip), %xmm3, %xmm3
	vmulsd	%xmm12, %xmm1, %xmm12
	vmulsd	%xmm11, %xmm0, %xmm11
	vsubsd	%xmm5, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm3, %xmm3
	vmovsd	.LC30(%rip), %xmm2
	vaddsd	%xmm12, %xmm4, %xmm4
	vmulsd	%xmm2, %xmm1, %xmm1
	vmulsd	%xmm2, %xmm0, %xmm0
	vaddsd	%xmm11, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-4194360(%rbp), %xmm4, %xmm4
	vaddsd	%xmm0, %xmm3, %xmm0
	vmovss	%xmm4, 500(%r15)
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	-4194356(%rbp), %xmm0, %xmm0
	vmovss	%xmm0, 508(%r15)
	jmp	.L1554
	.cfi_endproc
.LFE1395:
	.size	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi, .-_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,comdat
.LCOLDE73:
	.section	.text._ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi,comdat
.LHOTE73:
	.section	.rodata.str1.1
.LC74:
	.string	"ZLIB DECOMPRESSION FAILURE!!"
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,comdat
	.align 2
.LCOLDB75:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,comdat
.LHOTB75:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi
	.type	_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi, @function
_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi:
.LFB1402:
	.cfi_startproc
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	xorl	%eax, %eax
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	movl	%ecx, %r12d
	movl	$14, %ecx
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	movl	%esi, %ebp
	movl	$.LC59, %esi
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	movq	%rdi, %rbx
	subq	$120, %rsp
	.cfi_def_cfa_offset 160
	movq	%rsp, %rdi
	rep; stosq
	leaq	136314920(%rbx), %rax
	movl	%edx, 8(%rsp)
	movq	%rsp, %rdi
	movq	%rax, (%rsp)
	movl	$112, %edx
	leaq	67108896(%rbx), %rax
	movl	$69206016, 32(%rsp)
	movq	%rax, 24(%rsp)
	call	inflateInit_
	testl	%eax, %eax
	je	.L1737
.L1738:
	movl	$.LC74, %edi
	call	puts
	call	abort
	.p2align 4,,10
	.p2align 3
.L1737:
	movl	$4, %esi
	movq	%rsp, %rdi
	call	inflate
	testl	%eax, %eax
	je	.L1738
	movq	40(%rsp), %r13
	movq	%rsp, %rdi
	call	inflateEnd
	movzbl	%bpl, %esi
	movl	%r12d, %ecx
	movq	%rbx, %rdi
	movslq	%r13d, %rdx
	call	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi
	addq	$120, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1402:
	.size	_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi, .-_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,comdat
.LCOLDE75:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi,comdat
.LHOTE75:
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,comdat
	.align 2
.LCOLDB76:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,comdat
.LHOTB76:
	.align 2
	.p2align 4,,15
	.weak	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi
	.type	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi, @function
_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi:
.LFB1383:
	.cfi_startproc
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	leaq	16(%rsp), %r13
	.cfi_def_cfa 13, 0
	andq	$-32, %rsp
	pushq	-8(%r13)
	pushq	%rbp
	.cfi_escape 0x10,0x6,0x2,0x76,0
	movq	%rsp, %rbp
	pushq	%r15
	.cfi_escape 0x10,0xf,0x2,0x76,0x78
	movl	%esi, %r15d
	xorl	%esi, %esi
	pushq	%r14
	pushq	%r13
	.cfi_escape 0xf,0x3,0x76,0x68,0x6
	.cfi_escape 0x10,0xe,0x2,0x76,0x70
	pushq	%r12
	pushq	%rbx
	.cfi_escape 0x10,0xc,0x2,0x76,0x60
	.cfi_escape 0x10,0x3,0x2,0x76,0x58
	movq	%rdi, %rbx
	leaq	131104(%rbx), %r13
	subq	$13672, %rsp
	movq	%rdi, -13664(%rbp)
	movl	$1, %ebx
	movq	%rdx, -13496(%rbp)
	leaq	-12336(%rbp), %rdi
	movl	$4096, %edx
	movl	%ecx, -13684(%rbp)
	call	memset
	movl	$2, %r11d
	movl	$6, %edi
	movl	$3, %r10d
	movl	$4, %r9d
	movl	$5, %r8d
	movl	$7, %esi
	xorl	%ecx, %ecx
	movl	$1, %edx
	jmp	.L1760
	.p2align 4,,10
	.p2align 3
.L1744:
	andn	-12336(%rbp,%r14,8), %r12, %r12
.L1745:
	movq	%r12, -12336(%rbp,%r14,8)
	movl	%ebx, %r12d
	movq	%rbx, %r14
	andl	$63, %r12d
	shrq	$6, %r14
	shlx	%r12, %rdx, %r12
	testb	$2, %al
	jne	.L1991
	andn	-12336(%rbp,%r14,8), %r12, %r12
.L1747:
	movq	%r12, -12336(%rbp,%r14,8)
	movl	%r11d, %r12d
	movq	%r11, %r14
	andl	$63, %r12d
	shrq	$6, %r14
	shlx	%r12, %rdx, %r12
	testb	$4, %al
	jne	.L1992
	andn	-12336(%rbp,%r14,8), %r12, %r12
.L1749:
	movq	%r12, -12336(%rbp,%r14,8)
	movl	%r10d, %r12d
	movq	%r10, %r14
	andl	$63, %r12d
	shrq	$6, %r14
	shlx	%r12, %rdx, %r12
	testb	$8, %al
	jne	.L1993
	andn	-12336(%rbp,%r14,8), %r12, %r12
.L1751:
	movq	%r12, -12336(%rbp,%r14,8)
	movl	%r9d, %r12d
	movq	%r9, %r14
	andl	$63, %r12d
	shrq	$6, %r14
	shlx	%r12, %rdx, %r12
	testb	$16, %al
	jne	.L1994
	andn	-12336(%rbp,%r14,8), %r12, %r12
.L1753:
	movq	%r12, -12336(%rbp,%r14,8)
	movl	%r8d, %r12d
	movq	%r8, %r14
	andl	$63, %r12d
	shrq	$6, %r14
	shlx	%r12, %rdx, %r12
	testb	$32, %al
	jne	.L1995
	andn	-12336(%rbp,%r14,8), %r12, %r12
.L1755:
	movq	%r12, -12336(%rbp,%r14,8)
	movl	%edi, %r12d
	movq	%rdi, %r14
	andl	$63, %r12d
	shrq	$6, %r14
	shlx	%r12, %rdx, %r12
	testb	$64, %al
	jne	.L1996
	andn	-12336(%rbp,%r14,8), %r12, %r12
.L1757:
	movq	%r12, -12336(%rbp,%r14,8)
	movq	%rsi, %r12
	movl	%esi, %r14d
	andl	$63, %r14d
	shrq	$6, %r12
	testb	%al, %al
	shlx	%r14, %rdx, %rax
	js	.L1997
	andn	-12336(%rbp,%r12,8), %rax, %rax
.L1759:
	addq	$8, %rcx
	movq	%rax, -12336(%rbp,%r12,8)
	addq	$1, %r13
	addq	$8, %rsi
	addq	$8, %rdi
	addq	$8, %r8
	addq	$8, %r9
	addq	$8, %r10
	addq	$8, %r11
	addq	$8, %rbx
	cmpq	$32768, %rcx
	je	.L1998
.L1760:
	movzbl	0(%r13), %eax
	movl	%ecx, %r12d
	movq	%rcx, %r14
	andl	$63, %r12d
	shrq	$6, %r14
	shlx	%r12, %rdx, %r12
	testb	$1, %al
	je	.L1744
	orq	-12336(%rbp,%r14,8), %r12
	jmp	.L1745
.L1997:
	orq	-12336(%rbp,%r12,8), %rax
	jmp	.L1759
.L1996:
	orq	-12336(%rbp,%r14,8), %r12
	jmp	.L1757
.L1995:
	orq	-12336(%rbp,%r14,8), %r12
	jmp	.L1755
.L1994:
	orq	-12336(%rbp,%r14,8), %r12
	jmp	.L1753
.L1993:
	orq	-12336(%rbp,%r14,8), %r12
	jmp	.L1751
.L1992:
	orq	-12336(%rbp,%r14,8), %r12
	jmp	.L1749
.L1991:
	orq	-12336(%rbp,%r14,8), %r12
	jmp	.L1747
.L1998:
	movq	-13496(%rbp), %rax
	subq	$4096, %rax
	cmpb	$1, %r15b
	sbbq	%rcx, %rcx
	xorl	%edx, %edx
	andl	$2, %ecx
	addq	$2, %rcx
	divq	%rcx
	movslq	%eax, %rbx
	movq	%rax, %r13
	movl	%eax, -13496(%rbp)
	testq	%rbx, %rbx
	jne	.L1999
	testb	%r15b, %r15b
	je	.L1926
	movq	%rsp, %r12
	xorl	%r14d, %r14d
	movq	$0, -13656(%rbp)
.L1776:
	movq	%r12, %rsp
.L1765:
	movl	$4096, %edx
	movl	$1, %r12d
	leaq	-12336(%rbp), %rsi
	leaq	-8240(%rbp), %rdi
	call	memcpy
	movq	-13664(%rbp), %rax
	movl	$4096, %edx
	leaq	-8240(%rbp), %rsi
	leaq	-4144(%rbp), %rdi
	addq	$20, %rax
	movq	%rax, -13648(%rbp)
	call	memcpy
	movl	$7, -13616(%rbp)
.L1784:
	movl	-13616(%rbp), %eax
	leal	0(,%rax,8), %ecx
	movl	%eax, %esi
	movl	%eax, %edx
	sarl	$2, %esi
	andl	$16, %ecx
	andl	$1, %edx
	leal	15(%rcx), %ebx
	movl	%esi, %edi
	sall	$4, %edx
	sall	$4, %edi
	sall	$9, %esi
	movl	%ebx, -13584(%rbp)
	leal	15(%rdi), %eax
	movslq	%edi, %r10
	movslq	%edx, %rdi
	leal	15(%rdx), %r13d
	sall	$5, %eax
	addq	$15, %r10
	leal	134217727(%rcx,%rax), %r8d
	addl	%ebx, %eax
	salq	$12, %r10
	addq	-13648(%rbp), %r10
	sall	$5, %eax
	cltq
	sall	$5, %r8d
	leaq	15(%rdi,%rax), %rax
	movl	%r8d, -13496(%rbp)
	movq	%rax, -13512(%rbp)
	leal	134217695(%rcx,%rsi), %eax
	sall	$5, %eax
	movl	%eax, -13552(%rbp)
	leal	-1(%rdx), %eax
	movl	%eax, -13504(%rbp)
	movl	%r8d, %eax
.L1777:
	movl	-13584(%rbp), %ebx
	leal	512(%rax), %edi
	movq	-13512(%rbp), %r11
	.p2align 4,,10
	.p2align 3
.L1783:
	movl	-13504(%rbp), %eax
	leal	0(%r13,%rdi), %ecx
	movslq	%ebx, %r8
	movq	%r11, %rdx
	salq	$5, %r8
	leal	(%rax,%rdi), %r9d
	jmp	.L1780
	.p2align 4,,10
	.p2align 3
.L2001:
	vmovss	-4(%r14), %xmm0
	cltq
	subl	$1, %ecx
	subq	$4, %r14
	addq	%r8, %rax
	subq	$1, %rdx
	vmovss	%xmm0, (%r10,%rax,4)
	cmpl	%ecx, %r9d
	je	.L2000
.L1780:
	movq	%rdx, %rsi
	movl	%ecx, %eax
	movq	%r12, %r15
	shrq	$6, %rsi
	subl	%edi, %eax
	salq	%cl, %r15
	testq	%r15, -4144(%rbp,%rsi,8)
	jne	.L2001
	cltq
	subl	$1, %ecx
	addq	%r8, %rax
	subq	$1, %rdx
	movl	$0x00000000, (%r10,%rax,4)
	cmpl	%ecx, %r9d
	jne	.L1780
.L2000:
	subl	$1, %ebx
	subq	$32, %r11
	subl	$32, %edi
	cmpl	-13496(%rbp), %edi
	jne	.L1783
	leal	-1024(%rdi), %eax
	subq	$4096, %r10
	movl	%eax, -13496(%rbp)
	subq	$1024, -13512(%rbp)
	cmpl	-13552(%rbp), %eax
	jne	.L1777
	subl	$1, -13616(%rbp)
	jne	.L1784
	movl	$64, %ecx
	xorl	%eax, %eax
	xorl	%r15d, %r15d
	leaq	-13360(%rbp), %rdi
	xorl	%r13d, %r13d
	rep; stosq
	movl	$1, %edi
.L1785:
	movl	%r13d, %esi
	movl	%r15d, %r10d
	movq	%r15, %r9
	leal	512(%r13), %r11d
	movq	%r13, %r8
.L1791:
	xorl	%eax, %eax
	jmp	.L1788
	.p2align 4,,10
	.p2align 3
.L1786:
	andn	-13360(%rbp,%rdx,8), %rcx, %rcx
.L1787:
	addq	$1, %rax
	movq	%rcx, -13360(%rbp,%rdx,8)
	cmpq	$16, %rax
	je	.L2002
.L1788:
	leal	(%rsi,%rax), %ecx
	movq	%rdi, %r12
	leaq	(%rax,%r9), %rdx
	salq	%cl, %r12
	leaq	(%rax,%r8), %rcx
	shrq	$6, %rdx
	leal	(%r10,%rax), %ebx
	shrq	$6, %rcx
	andl	$63, %ebx
	testq	%r12, -4144(%rbp,%rcx,8)
	shlx	%rbx, %rdi, %rcx
	je	.L1786
	orq	-13360(%rbp,%rdx,8), %rcx
	jmp	.L1787
.L1926:
	movq	$0, -13656(%rbp)
	xorl	%r14d, %r14d
	xorl	%r12d, %r12d
.L1916:
	movq	-13664(%rbp), %rax
	movq	%r12, %rdx
	movq	-13656(%rbp), %rdi
	leaq	135200(%rax), %rsi
	call	memcpy
	jmp	.L1765
.L1999:
	movabsq	$4611686018427387903, %rax
	cmpq	%rax, %rbx
	ja	.L2003
	leaq	0(,%rbx,4), %r12
	movq	%r12, %rdi
	call	_Znwm
	xorl	%esi, %esi
	movq	%r12, %rdx
	movq	%rax, -13656(%rbp)
	leaq	(%rax,%r12), %r14
	movq	%rax, %rdi
	call	memset
	testb	%r15b, %r15b
	je	.L1916
	leaq	(%rbx,%rbx), %rdx
	movq	%rsp, %r12
	leaq	16(%rdx), %rax
	andq	$-16, %rax
	subq	%rax, %rsp
	movq	-13664(%rbp), %rax
	movq	%rsp, %rdi
	leaq	135200(%rax), %rsi
	call	memcpy
	movq	%rsp, %r8
	testl	%r13d, %r13d
	jle	.L1776
	movq	-13656(%rbp), %rsi
	andl	$31, %esi
	shrq	$2, %rsi
	negq	%rsi
	andl	$7, %esi
	cmpl	%r13d, %esi
	cmova	%r13d, %esi
	cmpl	$16, %r13d
	cmovbe	%r13d, %esi
	testl	%esi, %esi
	je	.L1920
	movq	-13656(%rbp), %rbx
	xorl	%ecx, %ecx
.L1769:
	movzwl	(%r8,%rcx,2), %edx
	movl	%edx, %eax
	andl	$1023, %eax
	sall	$13, %eax
	movl	%eax, %edi
	movl	%edx, %eax
	andl	$31744, %edx
	andw	$-32768, %ax
	sall	$13, %edx
	movzwl	%ax, %eax
	addl	$939524096, %edx
	sall	$16, %eax
	orl	%edi, %eax
	orl	%edx, %eax
	movl	%eax, (%rbx,%rcx,4)
	leal	1(%rcx), %eax
	addq	$1, %rcx
	cmpl	%ecx, %esi
	ja	.L1769
	cmpl	%r13d, %esi
	je	.L1776
.L1768:
	movl	%r13d, %edi
	subl	$1, %r13d
	movl	%esi, %ecx
	subl	%esi, %edi
	subl	%esi, %r13d
	leal	-16(%rdi), %edx
	shrl	$4, %edx
	addl	$1, %edx
	movl	%edx, %r9d
	sall	$4, %r9d
	cmpl	$14, %r13d
	jbe	.L1771
	movq	-13656(%rbp), %rbx
	leaq	(%r8,%rcx,2), %r11
	xorl	%esi, %esi
	vmovdqa	.LC69(%rip), %ymm6
	vmovdqa	.LC70(%rip), %ymm5
	vmovdqa	.LC71(%rip), %ymm4
	vmovdqa	.LC72(%rip), %ymm3
	leaq	(%rbx,%rcx,4), %r10
	xorl	%ecx, %ecx
.L1772:
	vmovdqu	(%r11,%rcx), %ymm0
	addl	$1, %esi
	vpand	%ymm0, %ymm6, %ymm8
	vpand	%ymm0, %ymm5, %ymm1
	vpmovzxwd	%xmm8, %ymm7
	vpand	%ymm0, %ymm4, %ymm0
	vpmovzxwd	%xmm0, %ymm2
	vpslld	$16, %ymm7, %ymm7
	vextracti128	$0x1, %ymm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0
	vpslld	$13, %ymm2, %ymm2
	vpor	%ymm7, %ymm2, %ymm2
	vpmovzxwd	%xmm1, %ymm7
	vextracti128	$0x1, %ymm1, %xmm1
	vpmovzxwd	%xmm1, %ymm1
	vpsrad	$10, %ymm7, %ymm7
	vpaddd	%ymm3, %ymm7, %ymm7
	vpsrad	$10, %ymm1, %ymm1
	vpslld	$23, %ymm7, %ymm7
	vpaddd	%ymm3, %ymm1, %ymm1
	vpor	%ymm7, %ymm2, %ymm2
	vmovaps	%ymm2, (%r10,%rcx,2)
	vextracti128	$0x1, %ymm8, %xmm2
	vpmovzxwd	%xmm2, %ymm2
	vpslld	$13, %ymm0, %ymm0
	vpslld	$16, %ymm2, %ymm2
	vpslld	$23, %ymm1, %ymm1
	vpor	%ymm2, %ymm0, %ymm0
	vpor	%ymm1, %ymm0, %ymm0
	vmovaps	%ymm0, 32(%r10,%rcx,2)
	addq	$32, %rcx
	cmpl	%edx, %esi
	jb	.L1772
	addl	%r9d, %eax
	cmpl	%edi, %r9d
	je	.L1776
.L1771:
	movq	-13656(%rbp), %rbx
	movslq	%eax, %rdx
	leaq	(%rbx,%rdx,4), %rsi
	jmp	.L1774
	.p2align 4,,10
	.p2align 3
.L2004:
	movslq	%eax, %rdx
.L1774:
	movzwl	(%r8,%rdx,2), %ecx
	addl	$1, %eax
	addq	$4, %rsi
	movl	%ecx, %edx
	andl	$1023, %edx
	movl	%edx, %edi
	movl	%ecx, %edx
	andl	$31744, %ecx
	sall	$13, %edi
	andw	$-32768, %dx
	sall	$13, %ecx
	movzwl	%dx, %edx
	addl	$939524096, %ecx
	sall	$16, %edx
	orl	%edi, %edx
	orl	%ecx, %edx
	movl	%edx, -4(%rsi)
	cmpl	%eax, -13496(%rbp)
	jg	.L2004
	jmp	.L1776
	.p2align 4,,10
	.p2align 3
.L2002:
	addl	$32, %esi
	addl	$16, %r10d
	addq	$32, %r8
	addq	$16, %r9
	cmpl	%esi, %r11d
	jne	.L1791
	addq	$1024, %r13
	addq	$256, %r15
	cmpq	$16384, %r13
	jne	.L1785
	movl	$512, %edx
	movl	$1, %ebx
	leaq	-13360(%rbp), %rsi
	leaq	-12848(%rbp), %rdi
	call	memcpy
	movl	$7, -13676(%rbp)
.L1813:
	movl	-13676(%rbp), %eax
	leal	0(,%rax,4), %edx
	movl	%eax, %r8d
	sarl	$2, %eax
	sall	$3, %eax
	andl	$8, %edx
	andl	$1, %r8d
	leal	7(%rdx), %esi
	movslq	%eax, %rdi
	addl	$7, %eax
	sall	$4, %eax
	sall	$3, %r8d
	movl	%esi, -13504(%rbp)
	addq	$7, %rdi
	addl	%esi, %eax
	movslq	%r8d, %r10
	salq	$12, %rdi
	addq	-13648(%rbp), %rdi
	sall	$4, %eax
	leal	7(%r8), %r9d
	movslq	%eax, %rdx
	leaq	7(%rdx,%r10), %rsi
	addl	%r9d, %eax
	movslq	%r9d, %r9
	movl	%eax, -13680(%rbp)
	leaq	-2041(%rdx,%r10), %rax
	movq	%rax, -13696(%rbp)
	leal	4(%r8), %eax
	leal	3(%r8), %ecx
	cltq
	movq	%rsi, -13672(%rbp)
	movq	%rax, -13624(%rbp)
	leal	6(%r8), %esi
	subq	%r9, %rax
	movq	%rax, -13512(%rbp)
	leal	5(%r8), %edx
	movslq	%esi, %rsi
	movslq	%ecx, %rax
	leal	2(%r8), %r11d
	movq	%rsi, -13616(%rbp)
	subq	%r9, %rsi
	addl	$1, %r8d
	movq	%rax, -13632(%rbp)
	subq	%r9, %rax
	movq	%rsi, %r13
	movslq	%edx, %rsi
	movq	%rax, -13552(%rbp)
	movslq	%r11d, %rax
	movq	%rsi, -13520(%rbp)
	subq	%r9, %rsi
	movq	%rax, -13640(%rbp)
	subq	%r9, %rax
	movq	%rsi, %r12
	movl	%r8d, -13496(%rbp)
	movq	%rax, -13584(%rbp)
.L1792:
	movl	-13680(%rbp), %esi
	xorl	%r11d, %r11d
	movq	-13672(%rbp), %rdx
	jmp	.L1812
.L2006:
	cltq
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	%r9, %rcx
	vmovss	%xmm0, (%rdi,%rcx,4)
.L1796:
	leal	-1(%rsi), %ecx
	movq	%rbx, %r8
	salq	%cl, %r8
	leaq	0(%r13,%rdx), %rcx
	shrq	$6, %rcx
	testq	%r8, -12848(%rbp,%rcx,8)
	je	.L1797
	movq	%rax, %rcx
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	salq	$5, %rcx
	addq	-13616(%rbp), %rcx
	vmovss	%xmm0, (%rdi,%rcx,4)
.L1798:
	leal	-2(%rsi), %ecx
	movq	%rbx, %r8
	salq	%cl, %r8
	leaq	(%r12,%rdx), %rcx
	shrq	$6, %rcx
	testq	%r8, -12848(%rbp,%rcx,8)
	je	.L1799
	movq	%rax, %rcx
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	salq	$5, %rcx
	addq	-13520(%rbp), %rcx
	vmovss	%xmm0, (%rdi,%rcx,4)
.L1800:
	leal	-3(%rsi), %ecx
	movq	%rbx, %r8
	salq	%cl, %r8
	movq	-13512(%rbp), %rcx
	addq	%rdx, %rcx
	shrq	$6, %rcx
	testq	%r8, -12848(%rbp,%rcx,8)
	je	.L1801
	movq	%rax, %rcx
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	salq	$5, %rcx
	addq	-13624(%rbp), %rcx
	vmovss	%xmm0, (%rdi,%rcx,4)
.L1802:
	leal	-4(%rsi), %ecx
	movq	%rbx, %r8
	salq	%cl, %r8
	movq	-13552(%rbp), %rcx
	addq	%rdx, %rcx
	shrq	$6, %rcx
	testq	%r8, -12848(%rbp,%rcx,8)
	je	.L1803
	movq	%rax, %rcx
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	salq	$5, %rcx
	addq	-13632(%rbp), %rcx
	vmovss	%xmm0, (%rdi,%rcx,4)
.L1804:
	leal	-5(%rsi), %ecx
	movq	%rbx, %r8
	salq	%cl, %r8
	movq	-13584(%rbp), %rcx
	addq	%rdx, %rcx
	shrq	$6, %rcx
	testq	%r8, -12848(%rbp,%rcx,8)
	je	.L1805
	movq	%rax, %rcx
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	salq	$5, %rcx
	addq	-13640(%rbp), %rcx
	vmovss	%xmm0, (%rdi,%rcx,4)
.L1806:
	movslq	-13496(%rbp), %r8
	leal	-6(%rsi), %ecx
	movq	%rbx, %r15
	salq	%cl, %r15
	movq	%r8, %rcx
	subq	%r9, %rcx
	addq	%rdx, %rcx
	shrq	$6, %rcx
	testq	%r15, -12848(%rbp,%rcx,8)
	je	.L1807
	movq	%rax, %rcx
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	salq	$5, %rcx
	addq	%r8, %rcx
	vmovss	%xmm0, (%rdi,%rcx,4)
.L1808:
	leal	-7(%rsi), %ecx
	movq	%rbx, %r8
	salq	%cl, %r8
	movq	%r10, %rcx
	subq	%r9, %rcx
	addq	%rdx, %rcx
	shrq	$6, %rcx
	testq	%r8, -12848(%rbp,%rcx,8)
	je	.L2005
	vmovss	-4(%r14), %xmm0
	salq	$5, %rax
	subq	$4, %r14
	addq	%r10, %rax
	vmovss	%xmm0, (%rdi,%rax,4)
.L1793:
	addq	$1, %r11
	subq	$16, %rdx
	subl	$16, %esi
	cmpq	$8, %r11
	je	.L1794
.L1812:
	movl	-13504(%rbp), %eax
	movl	%esi, %ecx
	movq	%rbx, %r8
	salq	%cl, %r8
	movq	%rdx, %rcx
	shrq	$6, %rcx
	subl	%r11d, %eax
	testq	%r8, -12848(%rbp,%rcx,8)
	jne	.L2006
	cltq
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	%r9, %rcx
	movl	$0x00000000, (%rdi,%rcx,4)
	jmp	.L1796
.L2005:
	salq	$5, %rax
	addq	%r10, %rax
	movl	$0x00000000, (%rdi,%rax,4)
	jmp	.L1793
.L1807:
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	%r8, %rcx
	movl	$0x00000000, (%rdi,%rcx,4)
	jmp	.L1808
.L1805:
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	-13640(%rbp), %rcx
	movl	$0x00000000, (%rdi,%rcx,4)
	jmp	.L1806
.L1803:
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	-13632(%rbp), %rcx
	movl	$0x00000000, (%rdi,%rcx,4)
	jmp	.L1804
.L1801:
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	-13624(%rbp), %rcx
	movl	$0x00000000, (%rdi,%rcx,4)
	jmp	.L1802
.L1799:
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	-13520(%rbp), %rcx
	movl	$0x00000000, (%rdi,%rcx,4)
	jmp	.L1800
.L1797:
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	-13616(%rbp), %rcx
	movl	$0x00000000, (%rdi,%rcx,4)
	jmp	.L1798
.L1794:
	subq	$256, -13672(%rbp)
	subq	$4096, %rdi
	movq	-13672(%rbp), %rax
	subl	$256, -13680(%rbp)
	cmpq	-13696(%rbp), %rax
	jne	.L1792
	subl	$1, -13676(%rbp)
	jne	.L1813
	leaq	-13424(%rbp), %r15
	xorl	%eax, %eax
	movq	%r11, %rcx
	movq	%r15, %rdi
	xorl	%ebx, %ebx
	xorl	%r13d, %r13d
	rep; stosq
	movl	$1, %edi
.L1814:
	leal	128(%r13), %eax
	movl	%r13d, %r8d
	movl	%ebx, %r11d
	movl	%eax, -13496(%rbp)
	movq	%rbx, %r10
	movq	%r13, %r9
.L1820:
	xorl	%eax, %eax
	jmp	.L1817
.L1815:
	andn	-13424(%rbp,%rdx,8), %rcx, %rcx
.L1816:
	addq	$1, %rax
	movq	%rcx, -13424(%rbp,%rdx,8)
	cmpq	$8, %rax
	je	.L2007
.L1817:
	leal	(%r8,%rax), %ecx
	movq	%rdi, %r12
	leaq	(%rax,%r10), %rdx
	salq	%cl, %r12
	leaq	(%r9,%rax), %rcx
	shrq	$6, %rdx
	leal	(%r11,%rax), %esi
	shrq	$6, %rcx
	andl	$63, %esi
	testq	%r12, -12848(%rbp,%rcx,8)
	shlx	%rsi, %rdi, %rcx
	je	.L1815
	orq	-13424(%rbp,%rdx,8), %rcx
	jmp	.L1816
.L2007:
	addl	$16, %r8d
	addl	$8, %r11d
	addq	$16, %r9
	addq	$8, %r10
	cmpl	-13496(%rbp), %r8d
	jne	.L1820
	addq	$256, %r13
	addq	$64, %rbx
	cmpq	$2048, %r13
	jne	.L1814
	movq	-13424(%rbp), %rax
	vxorps	%xmm1, %xmm1, %xmm1
	movl	$1, %r13d
	movl	$7, -13584(%rbp)
	vmovaps	%xmm1, %xmm3
	vmovaps	%xmm1, %xmm2
	movq	%rax, -13488(%rbp)
	movq	-13416(%rbp), %rax
	movq	%rax, -13480(%rbp)
	movq	-13408(%rbp), %rax
	movq	%rax, -13472(%rbp)
	movq	-13400(%rbp), %rax
	movq	%rax, -13464(%rbp)
	movq	-13392(%rbp), %rax
	movq	%rax, -13456(%rbp)
	movq	-13384(%rbp), %rax
	movq	%rax, -13448(%rbp)
	movq	-13376(%rbp), %rax
	movq	%rax, -13440(%rbp)
	movq	-13368(%rbp), %rax
	movq	%rax, -13432(%rbp)
.L1829:
	movl	-13584(%rbp), %ebx
	leal	(%rbx,%rbx), %r10d
	movl	%ebx, %esi
	movl	%ebx, %eax
	sarl	$2, %esi
	andl	$4, %r10d
	andl	$1, %eax
	leal	0(,%rsi,4), %edi
	sall	$5, %esi
	movslq	%edi, %rdx
	sall	$2, %eax
	addl	%r10d, %esi
	leal	27(%r10,%rdi,8), %edi
	movslq	%eax, %r9
	addq	$3, %rdx
	sall	$3, %edi
	salq	$12, %rdx
	addq	-13648(%rbp), %rdx
	movslq	%edi, %r8
	addl	%eax, %edi
	leal	3(%rax), %ecx
	addq	%r9, %r8
	movl	%edi, -13496(%rbp)
	leal	2(%rax), %ebx
	movq	%r8, -13616(%rbp)
	leal	1(%rax), %r11d
	movslq	%ebx, %rbx
	leal	-40(%rax,%rsi,8), %eax
	movslq	%r11d, %r11
	movl	%eax, -13624(%rbp)
	movslq	%ecx, %rax
	movq	%rax, -13520(%rbp)
	subq	%r9, %rax
	movq	%rax, -13504(%rbp)
	movq	%rbx, %rax
	subq	%r9, %rax
	movq	%rax, -13512(%rbp)
	movq	%r11, %rax
	subq	%r9, %rax
	movq	%rax, -13552(%rbp)
.L1821:
	movl	-13496(%rbp), %edi
	movl	$3, %r8d
	movq	-13616(%rbp), %rsi
.L1827:
	leal	3(%rdi), %ecx
	movq	%r13, %r12
	leal	(%r10,%r8), %eax
	salq	%cl, %r12
	movq	-13504(%rbp), %rcx
	addq	%rsi, %rcx
	shrq	$6, %rcx
	testq	%r12, -13488(%rbp,%rcx,8)
	je	.L1822
	cltq
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	-13520(%rbp), %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
.L1823:
	leal	2(%rdi), %ecx
	movq	%r13, %r12
	vmovaps	%xmm1, %xmm0
	salq	%cl, %r12
	movq	-13512(%rbp), %rcx
	addq	%rsi, %rcx
	shrq	$6, %rcx
	testq	%r12, -13488(%rbp,%rcx,8)
	je	.L1824
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
.L1824:
	movq	%rax, %rcx
	movq	%r13, %r12
	salq	$5, %rcx
	addq	%rbx, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	leal	1(%rdi), %ecx
	vmovaps	%xmm3, %xmm0
	salq	%cl, %r12
	movq	-13552(%rbp), %rcx
	addq	%rsi, %rcx
	shrq	$6, %rcx
	testq	%r12, -13488(%rbp,%rcx,8)
	je	.L1825
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
.L1825:
	movq	%rax, %rcx
	movq	%r13, %r12
	salq	$5, %rcx
	addq	%r11, %rcx
	vmovss	%xmm0, (%rdx,%rcx,4)
	movl	%edi, %ecx
	vmovaps	%xmm2, %xmm0
	salq	%cl, %r12
	movq	%rsi, %rcx
	shrq	$6, %rcx
	testq	%r12, -13488(%rbp,%rcx,8)
	je	.L1826
	vmovss	-4(%r14), %xmm0
	subq	$4, %r14
.L1826:
	salq	$5, %rax
	subl	$1, %r8d
	subq	$8, %rsi
	addq	%r9, %rax
	subl	$8, %edi
	vmovss	%xmm0, (%rdx,%rax,4)
	cmpl	$-1, %r8d
	jne	.L1827
	subl	$64, -13496(%rbp)
	subq	$4096, %rdx
	subq	$64, -13616(%rbp)
	movl	-13496(%rbp), %eax
	cmpl	-13624(%rbp), %eax
	jne	.L1821
	subl	$1, -13584(%rbp)
	jne	.L1829
	movq	-13648(%rbp), %rax
	xorl	%ecx, %ecx
	movq	-13656(%rbp), %rdx
	movq	%rax, -13520(%rbp)
.L1830:
	vmovss	(%rdx), %xmm0
	addl	$1, %ecx
	addq	$64, %rdx
	addq	$4096, %rax
	vmovss	%xmm0, -4096(%rax)
	vmovss	-60(%rdx), %xmm0
	vmovss	%xmm0, -4092(%rax)
	vmovss	-56(%rdx), %xmm0
	vmovss	%xmm0, -4088(%rax)
	vmovss	-52(%rdx), %xmm0
	vmovss	%xmm0, -4084(%rax)
	vmovss	-48(%rdx), %xmm0
	vmovss	%xmm0, -3968(%rax)
	vmovss	-44(%rdx), %xmm0
	vmovss	%xmm0, -3964(%rax)
	vmovss	-40(%rdx), %xmm0
	vmovss	%xmm0, -3960(%rax)
	vmovss	-36(%rdx), %xmm0
	vmovss	%xmm0, -3956(%rax)
	vmovss	-32(%rdx), %xmm0
	vmovss	%xmm0, -3840(%rax)
	vmovss	-28(%rdx), %xmm0
	vmovss	%xmm0, -3836(%rax)
	vmovss	-24(%rdx), %xmm0
	vmovss	%xmm0, -3832(%rax)
	vmovss	-20(%rdx), %xmm0
	vmovss	%xmm0, -3828(%rax)
	vmovss	-16(%rdx), %xmm0
	vmovss	%xmm0, -3712(%rax)
	vmovss	-12(%rdx), %xmm0
	vmovss	%xmm0, -3708(%rax)
	vmovss	-8(%rdx), %xmm0
	vmovss	%xmm0, -3704(%rax)
	vmovss	-4(%rdx), %xmm0
	vmovss	%xmm0, -3700(%rax)
	cmpl	$4, %ecx
	jne	.L1830
	movl	-13684(%rbp), %eax
	leal	-1(%rax), %ebx
	cmpl	$2, %eax
	movq	-13664(%rbp), %rax
	sete	-13504(%rbp)
	movl	%ebx, -13496(%rbp)
	leaq	1060(%rax), %r13
	leaq	33828(%rax), %rbx
.L1831:
	leaq	-1024(%r13), %r12
	jmp	.L1838
.L1832:
	call	memcpy
	vxorpd	%xmm4, %xmm4, %xmm4
	vmovsd	.LC36(%rip), %xmm3
	vcvtss2sd	-13424(%rbp), %xmm4, %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13420(%rbp), %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm2, %xmm7
	vmulsd	%xmm3, %xmm4, %xmm5
	vcvtss2sd	-13416(%rbp), %xmm0, %xmm0
	subq	$-128, %r12
	vmovsd	.LC34(%rip), %xmm8
	vmovss	-13360(%rbp), %xmm6
	vmulsd	%xmm8, %xmm0, %xmm1
	vsubsd	%xmm7, %xmm5, %xmm5
	vaddsd	%xmm1, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -144(%r12)
	vmovsd	.LC35(%rip), %xmm5
	vmulsd	%xmm5, %xmm4, %xmm9
	vaddsd	%xmm9, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vmovss	-13356(%rbp), %xmm7
	vmovss	%xmm6, -140(%r12)
	vmulsd	%xmm8, %xmm4, %xmm6
	vaddsd	%xmm6, %xmm2, %xmm6
	vsubsd	%xmm1, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm6
	vmovss	%xmm6, -136(%r12)
	vmovsd	.LC40(%rip), %xmm6
	vmulsd	%xmm6, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm2, %xmm6
	vaddsd	%xmm4, %xmm2, %xmm4
	vaddsd	%xmm4, %xmm1, %xmm1
	vmulsd	%xmm8, %xmm2, %xmm4
	vaddsd	%xmm6, %xmm0, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vmovss	-13352(%rbp), %xmm7
	vaddsd	%xmm4, %xmm0, %xmm9
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vmovss	%xmm1, -132(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13412(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm0, %xmm6
	vsubsd	%xmm8, %xmm9, %xmm9
	vaddsd	%xmm2, %xmm8, %xmm8
	vxorps	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm4, %xmm0
	vaddsd	%xmm5, %xmm6, %xmm5
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubss	%xmm7, %xmm9, %xmm9
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm2, -124(%r12)
	vmovss	-13348(%rbp), %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm2, %xmm5, %xmm5
	vmovss	%xmm9, -128(%r12)
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm5, -120(%r12)
	vmovss	%xmm0, -116(%r12)
	cmpq	%r13, %r12
	je	.L2008
.L1838:
	movq	-16(%r12), %rax
	movl	$16, %edx
	movq	%r12, %rsi
	cmpl	$1, -13496(%rbp)
	leaq	-13360(%rbp), %rdi
	movq	%rax, (%r15)
	movq	-8(%r12), %rax
	movq	%rax, 8(%r15)
	ja	.L1832
	call	memcpy
	cmpb	$0, -13504(%rbp)
	jne	.L1834
	vmovss	-13424(%rbp), %xmm2
	vmovss	-13420(%rbp), %xmm1
	vmovss	-13416(%rbp), %xmm0
	vmovss	-13412(%rbp), %xmm3
	vmovss	-13360(%rbp), %xmm11
	vmovss	-13356(%rbp), %xmm9
	vmovss	-13352(%rbp), %xmm6
	vmovss	-13348(%rbp), %xmm4
.L1835:
	vmovsd	.LC20(%rip), %xmm5
	vmovss	%xmm2, -16(%r12)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	subq	$-128, %r12
	vmovsd	.LC21(%rip), %xmm7
	vmovss	%xmm0, -128(%r12)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm2, %xmm13
	vmovss	%xmm3, -120(%r12)
	vmovsd	.LC22(%rip), %xmm8
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vmovss	%xmm1, -136(%r12)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm12
	vmulsd	%xmm8, %xmm3, %xmm10
	vmulsd	%xmm8, %xmm2, %xmm8
	vmulsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm12, %xmm13, %xmm12
	vmulsd	%xmm5, %xmm0, %xmm13
	vsubsd	%xmm13, %xmm12, %xmm12
	vmulsd	.LC27(%rip), %xmm2, %xmm13
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm10, %xmm12, %xmm12
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vaddss	%xmm11, %xmm12, %xmm12
	vmovsd	.LC26(%rip), %xmm11
	vmovss	%xmm12, -140(%r12)
	vmulsd	%xmm11, %xmm1, %xmm12
	vmulsd	%xmm11, %xmm0, %xmm11
	vaddsd	%xmm12, %xmm13, %xmm12
	vaddsd	%xmm11, %xmm12, %xmm11
	vsubsd	%xmm10, %xmm11, %xmm10
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vaddss	%xmm9, %xmm10, %xmm10
	vmulsd	%xmm5, %xmm1, %xmm9
	vmulsd	%xmm5, %xmm3, %xmm5
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vmovss	%xmm10, -132(%r12)
	vsubsd	%xmm9, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm7, %xmm8, %xmm7
	vaddsd	%xmm5, %xmm7, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -124(%r12)
	vmovsd	.LC30(%rip), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vmovss	%xmm0, -116(%r12)
	cmpq	%r13, %r12
	jne	.L1838
.L2008:
	leaq	4096(%r12), %r13
	cmpq	%rbx, %r13
	jne	.L1831
	movq	-13664(%rbp), %rax
	xorl	%r14d, %r14d
	leaq	24(%rax), %r13
.L1842:
	movq	-13648(%rbp), %rdx
	movslq	%r14d, %rdi
	movq	%r13, %rax
	movl	$6, %ebx
	movq	%rdi, %rcx
	movl	$5, %r11d
	salq	$7, %rdi
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	movl	$7, %esi
	salq	$5, %rcx
.L1841:
	cmpl	$14, %esi
	je	.L1840
	vmovss	(%rax), %xmm0
	vmovss	4092(%rax), %xmm1
	vmovss	%xmm0, 4092(%rax)
	vmovss	%xmm1, (%rax)
	cmpl	$7, %r8d
	ja	.L1840
	movslq	%r8d, %r12
	vmovss	8188(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 8188(%rax)
	cmpl	$7, %r9d
	ja	.L1840
	movslq	%r9d, %r12
	vmovss	12284(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 12284(%rax)
	cmpl	$7, %r10d
	ja	.L1840
	movslq	%r10d, %r12
	vmovss	16380(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 16380(%rax)
	cmpl	$7, %r11d
	ja	.L1840
	movslq	%r11d, %r12
	vmovss	20476(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 20476(%rax)
	cmpl	$7, %ebx
	ja	.L1840
	movslq	%ebx, %r12
	vmovss	24572(%rax), %xmm1
	addq	%rcx, %r12
	vmovss	(%rdx,%r12,4), %xmm0
	vmovss	%xmm1, (%rdx,%r12,4)
	vmovss	%xmm0, 24572(%rax)
	cmpl	$7, %esi
	jne	.L1840
	leaq	(%rdx,%rdi), %r12
	vmovss	28668(%rax), %xmm1
	vmovss	28(%r12), %xmm0
	vmovss	%xmm1, 28(%r12)
	vmovss	%xmm0, 28668(%rax)
.L1840:
	addl	$1, %esi
	addq	$4096, %rdx
	addq	$4100, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	cmpl	$15, %esi
	jne	.L1841
	addl	$1, %r14d
	subq	$-128, %r13
	cmpl	$8, %r14d
	jne	.L1842
	movq	-13664(%rbp), %rax
	movq	-13648(%rbp), %r12
	leaq	32788(%rax), %r13
.L1855:
	leaq	16(%r12), %r14
	leaq	1040(%r12), %rbx
	jmp	.L1849
.L1843:
	call	memcpy
	vxorpd	%xmm4, %xmm4, %xmm4
	vmovsd	.LC36(%rip), %xmm3
	vcvtss2sd	-13424(%rbp), %xmm4, %xmm4
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13420(%rbp), %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vmulsd	.LC33(%rip), %xmm2, %xmm7
	vmulsd	%xmm3, %xmm4, %xmm5
	vcvtss2sd	-13416(%rbp), %xmm0, %xmm0
	subq	$-128, %r14
	vmovsd	.LC34(%rip), %xmm8
	vmovss	-13360(%rbp), %xmm6
	vmulsd	%xmm8, %xmm0, %xmm1
	vsubsd	%xmm7, %xmm5, %xmm5
	vaddsd	%xmm1, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -144(%r14)
	vmovsd	.LC35(%rip), %xmm5
	vmulsd	%xmm5, %xmm4, %xmm9
	vaddsd	%xmm9, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm7, %xmm7
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vaddss	%xmm7, %xmm6, %xmm6
	vmovss	-13356(%rbp), %xmm7
	vmovss	%xmm6, -140(%r14)
	vmulsd	%xmm8, %xmm4, %xmm6
	vaddsd	%xmm6, %xmm2, %xmm6
	vsubsd	%xmm1, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm7, %xmm6, %xmm6
	vmovss	%xmm6, -136(%r14)
	vmovsd	.LC40(%rip), %xmm6
	vmulsd	%xmm6, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm2, %xmm6
	vaddsd	%xmm4, %xmm2, %xmm4
	vaddsd	%xmm4, %xmm1, %xmm1
	vmulsd	%xmm8, %xmm2, %xmm4
	vaddsd	%xmm6, %xmm0, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm7, %xmm1
	vmovss	-13352(%rbp), %xmm7
	vaddsd	%xmm4, %xmm0, %xmm9
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vmovss	%xmm1, -132(%r14)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13412(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm8
	vmulsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm6, %xmm0, %xmm6
	vsubsd	%xmm8, %xmm9, %xmm9
	vaddsd	%xmm2, %xmm8, %xmm8
	vxorps	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm4, %xmm0
	vaddsd	%xmm5, %xmm6, %xmm5
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm2, %xmm2
	vaddss	%xmm2, %xmm7, %xmm2
	vaddsd	%xmm1, %xmm0, %xmm0
	vsubss	%xmm7, %xmm9, %xmm9
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovss	%xmm2, -124(%r14)
	vmovss	-13348(%rbp), %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubss	%xmm2, %xmm5, %xmm5
	vmovss	%xmm9, -128(%r14)
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm5, -120(%r14)
	vmovss	%xmm0, -116(%r14)
	cmpq	%r14, %rbx
	je	.L2009
.L1849:
	movq	-16(%r14), %rax
	movl	$16, %edx
	movq	%r14, %rsi
	cmpl	$1, -13496(%rbp)
	leaq	-13360(%rbp), %rdi
	movq	%rax, (%r15)
	movq	-8(%r14), %rax
	movq	%rax, 8(%r15)
	ja	.L1843
	call	memcpy
	cmpb	$0, -13504(%rbp)
	jne	.L1845
	vmovss	-13424(%rbp), %xmm2
	vmovss	-13420(%rbp), %xmm1
	vmovss	-13416(%rbp), %xmm0
	vmovss	-13412(%rbp), %xmm3
	vmovss	-13360(%rbp), %xmm11
	vmovss	-13356(%rbp), %xmm9
	vmovss	-13352(%rbp), %xmm6
	vmovss	-13348(%rbp), %xmm4
.L1846:
	vmovsd	.LC20(%rip), %xmm5
	vmovss	%xmm2, -16(%r14)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	subq	$-128, %r14
	vmovsd	.LC21(%rip), %xmm7
	vmovss	%xmm0, -128(%r14)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm2, %xmm13
	vmovss	%xmm3, -120(%r14)
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vmovsd	.LC22(%rip), %xmm8
	vmovss	%xmm1, -136(%r14)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm12
	vmulsd	%xmm8, %xmm3, %xmm10
	vmulsd	%xmm8, %xmm2, %xmm8
	vmulsd	%xmm7, %xmm0, %xmm7
	vaddsd	%xmm12, %xmm13, %xmm12
	vmulsd	%xmm5, %xmm0, %xmm13
	vsubsd	%xmm13, %xmm12, %xmm12
	vmulsd	.LC27(%rip), %xmm2, %xmm13
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm10, %xmm12, %xmm12
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vaddss	%xmm11, %xmm12, %xmm12
	vmovsd	.LC26(%rip), %xmm11
	vmovss	%xmm12, -140(%r14)
	vmulsd	%xmm11, %xmm1, %xmm12
	vmulsd	%xmm11, %xmm0, %xmm11
	vaddsd	%xmm12, %xmm13, %xmm12
	vaddsd	%xmm11, %xmm12, %xmm11
	vsubsd	%xmm10, %xmm11, %xmm10
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vaddss	%xmm9, %xmm10, %xmm10
	vmulsd	%xmm5, %xmm1, %xmm9
	vmulsd	%xmm5, %xmm3, %xmm5
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vmovss	%xmm10, -132(%r14)
	vsubsd	%xmm9, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm7, %xmm8, %xmm7
	vaddsd	%xmm5, %xmm7, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -124(%r14)
	vmovsd	.LC30(%rip), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm4, %xmm0, %xmm0
	vmovss	%xmm0, -116(%r14)
	cmpq	%r14, %rbx
	jne	.L1849
.L2009:
	leaq	896(%r12), %rax
	movq	%r12, %rdx
	movl	$6, %r10d
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %edi
	movl	$2, %esi
	movl	$7, %ecx
.L1851:
	cmpl	$14, %ecx
	je	.L1853
	vmovss	-892(%rax), %xmm0
	vmovss	-768(%rax), %xmm1
	vmovss	%xmm0, -768(%rax)
	vmovss	%xmm1, -892(%rax)
	cmpl	$7, %esi
	ja	.L1853
	movslq	%esi, %r11
	vmovss	-640(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -640(%rax)
	cmpl	$7, %edi
	ja	.L1853
	movslq	%edi, %r11
	vmovss	-512(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -512(%rax)
	cmpl	$7, %r8d
	ja	.L1853
	movslq	%r8d, %r11
	vmovss	-384(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -384(%rax)
	cmpl	$7, %r9d
	ja	.L1853
	movslq	%r9d, %r11
	vmovss	-256(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -256(%rax)
	cmpl	$7, %r10d
	ja	.L1853
	movslq	%r10d, %r11
	vmovss	-128(%rax), %xmm1
	leaq	(%rdx,%r11,4), %r11
	vmovss	(%r11), %xmm0
	vmovss	%xmm1, (%r11)
	vmovss	%xmm0, -128(%rax)
	cmpl	$7, %ecx
	jne	.L1853
	vmovss	28(%rdx), %xmm0
	vmovss	(%rax), %xmm1
	vmovss	%xmm1, 28(%rdx)
	vmovss	%xmm0, (%rax)
.L1853:
	addl	$1, %ecx
	subq	$-128, %rdx
	addq	$132, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	cmpl	$15, %ecx
	jne	.L1851
	leaq	1024(%r12), %r14
	movq	%r12, %rbx
.L1854:
	movl	-13684(%rbp), %esi
	movq	%rbx, %rdi
	subq	$-128, %rbx
	call	_ZN18WaveletsOnInterval3WI49transformILi8ELb0EEEvPfi
	cmpq	%r14, %rbx
	jne	.L1854
	addq	$4096, %r12
	cmpq	%r13, %r12
	jne	.L1855
	movq	-13664(%rbp), %rbx
	movzbl	-13504(%rbp), %r14d
	movl	-13496(%rbp), %r13d
	movq	%rbx, %rax
	addq	$67636, %rbx
	addq	$2100, %rax
	movq	%rbx, -13616(%rbp)
	movq	%rax, -13584(%rbp)
	movq	%rax, %r12
.L1856:
	leaq	-2048(%r12), %rbx
	jmp	.L1863
.L1857:
	call	memcpy
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC36(%rip), %xmm4
	vcvtss2sd	-13424(%rbp), %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13420(%rbp), %xmm0, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	.LC33(%rip), %xmm0, %xmm3
	vmulsd	%xmm4, %xmm1, %xmm6
	vcvtss2sd	-13416(%rbp), %xmm5, %xmm5
	subq	$-128, %rbx
	vmovsd	.LC34(%rip), %xmm7
	vmovss	-13360(%rbp), %xmm2
	vmulsd	%xmm7, %xmm5, %xmm10
	vsubsd	%xmm3, %xmm6, %xmm6
	vaddsd	%xmm10, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm2, %xmm6, %xmm6
	vmovss	%xmm6, -160(%rbx)
	vmovsd	.LC35(%rip), %xmm6
	vmulsd	%xmm6, %xmm1, %xmm8
	vaddsd	%xmm8, %xmm3, %xmm3
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-13412(%rbp), %xmm8, %xmm8
	vmulsd	%xmm7, %xmm8, %xmm9
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm3
	vmovss	%xmm2, -156(%rbx)
	vmovss	-13356(%rbp), %xmm2
	vaddsd	%xmm3, %xmm0, %xmm3
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm2, %xmm3, %xmm3
	vmovss	%xmm3, -152(%rbx)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm10, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm0, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vmovss	%xmm1, -148(%rbx)
	vmovss	-13352(%rbp), %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vaddsd	%xmm0, %xmm5, %xmm0
	vsubsd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm1, %xmm2, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm8, %xmm10, %xmm1
	vmovss	%xmm2, -144(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13408(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm11
	vmovss	%xmm0, -140(%rbx)
	vmovss	-13348(%rbp), %xmm0
	vaddsd	%xmm2, %xmm9, %xmm9
	vsubsd	%xmm11, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -136(%rbx)
	vmulsd	%xmm3, %xmm5, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13404(%rbp), %xmm5, %xmm5
	vaddsd	%xmm5, %xmm11, %xmm10
	vaddsd	%xmm1, %xmm8, %xmm1
	vmulsd	%xmm3, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm11, %xmm1
	vaddsd	%xmm8, %xmm2, %xmm8
	vmulsd	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	-13344(%rbp), %xmm1
	vmovss	%xmm0, -132(%rbx)
	vmulsd	%xmm7, %xmm5, %xmm0
	vaddsd	%xmm2, %xmm5, %xmm2
	vmulsd	%xmm3, %xmm5, %xmm5
	vsubsd	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm8, %xmm0, %xmm8
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm1, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vaddss	%xmm8, %xmm1, %xmm1
	vmovss	-13340(%rbp), %xmm8
	vmovss	%xmm9, -128(%rbx)
	vmovss	%xmm1, -124(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13400(%rbp), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm5, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm9, %xmm2
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm0, %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm8, %xmm10, %xmm10
	vmovss	-13336(%rbp), %xmm8
	vmovss	%xmm2, -116(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13396(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm7
	vmovss	%xmm10, -120(%rbx)
	vmulsd	%xmm6, %xmm2, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm3, %xmm7, %xmm7
	vxorps	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm8, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	%xmm9, -112(%rbx)
	vmovss	%xmm3, -108(%rbx)
	vmovss	-13332(%rbp), %xmm3
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm5, -104(%rbx)
	vmovss	%xmm0, -100(%rbx)
	cmpq	%rbx, %r12
	je	.L2010
.L1863:
	movq	-32(%rbx), %rax
	movl	$32, %edx
	movq	%rbx, %rsi
	leaq	-13360(%rbp), %rdi
	movq	%rax, (%r15)
	movq	-24(%rbx), %rax
	movq	%rax, 8(%r15)
	movq	-16(%rbx), %rax
	movq	%rax, 16(%r15)
	movq	-8(%rbx), %rax
	movq	%rax, 24(%r15)
	cmpl	$1, %r13d
	ja	.L1857
	call	memcpy
	testb	%r14b, %r14b
	jne	.L1859
	movl	-13360(%rbp), %r9d
	movl	-13356(%rbp), %r8d
	movl	-13352(%rbp), %edi
	movl	-13348(%rbp), %esi
	movl	-13344(%rbp), %ecx
	movl	-13340(%rbp), %edx
	movl	-13336(%rbp), %eax
	vmovss	-13424(%rbp), %xmm2
	vmovss	-13420(%rbp), %xmm14
	vmovss	-13416(%rbp), %xmm8
	vmovss	-13412(%rbp), %xmm10
	vmovss	-13408(%rbp), %xmm12
	vmovss	-13404(%rbp), %xmm9
	vmovss	-13400(%rbp), %xmm4
	vmovss	-13396(%rbp), %xmm3
	vmovss	-13332(%rbp), %xmm15
.L1860:
	vmovss	%xmm2, -32(%rbx)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm11
	vmovss	%xmm14, -24(%rbx)
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	subq	$-128, %rbx
	vmovsd	.LC22(%rip), %xmm6
	vmulsd	%xmm11, %xmm2, %xmm7
	vmovss	%xmm8, -144(%rbx)
	vcvtss2sd	%xmm8, %xmm8, %xmm8
	vmulsd	.LC21(%rip), %xmm14, %xmm5
	vmovss	%xmm10, -136(%rbx)
	vcvtss2sd	%xmm10, %xmm10, %xmm10
	vmulsd	%xmm11, %xmm8, %xmm0
	vmovss	%xmm3, -104(%rbx)
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm10, %xmm13
	vmovss	%xmm12, -128(%rbx)
	vmovss	%xmm4, -112(%rbx)
	vmovss	%xmm9, -120(%rbx)
	vaddsd	%xmm5, %xmm7, %xmm5
	vmovd	%r9d, %xmm7
	vsubsd	%xmm0, %xmm5, %xmm5
	vaddsd	%xmm13, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm5
	vmovsd	.LC27(%rip), %xmm7
	vmulsd	%xmm5, %xmm14, %xmm0
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovss	%xmm1, -156(%rbx)
	vmulsd	%xmm5, %xmm8, %xmm1
	vmulsd	%xmm7, %xmm14, %xmm14
	vmulsd	%xmm7, %xmm8, %xmm8
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovd	%r8d, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovd	%edi, %xmm1
	vsubsd	%xmm13, %xmm0, %xmm13
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm13, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm12, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm2, %xmm13
	vmovss	%xmm0, -148(%rbx)
	vmulsd	%xmm5, %xmm10, %xmm0
	vmulsd	%xmm7, %xmm10, %xmm10
	vmulsd	%xmm7, %xmm2, %xmm7
	vaddsd	%xmm0, %xmm14, %xmm14
	vaddsd	%xmm8, %xmm0, %xmm8
	vsubsd	%xmm13, %xmm14, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	%xmm1, %xmm14, %xmm14
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm9, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm1, %xmm0
	vmovss	%xmm14, -140(%rbx)
	vmulsd	%xmm5, %xmm2, %xmm14
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm14, %xmm8, %xmm8
	vaddsd	%xmm10, %xmm14, %xmm10
	vsubsd	%xmm0, %xmm8, %xmm8
	vmovd	%esi, %xmm0
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vaddss	%xmm0, %xmm8, %xmm8
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm4, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm14
	vmovd	%ecx, %xmm4
	vmulsd	%xmm6, %xmm3, %xmm6
	vmovss	%xmm8, -132(%rbx)
	vmulsd	%xmm5, %xmm1, %xmm8
	vmulsd	%xmm5, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm0, %xmm12
	vaddsd	%xmm8, %xmm10, %xmm10
	vaddsd	%xmm7, %xmm8, %xmm8
	vsubsd	%xmm14, %xmm10, %xmm10
	vaddsd	%xmm5, %xmm8, %xmm5
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm10
	vsubsd	%xmm6, %xmm5, %xmm5
	vmovd	%edx, %xmm6
	vmovss	%xmm10, -124(%rbx)
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovd	%eax, %xmm6
	vmovss	%xmm5, -116(%rbx)
	vmulsd	%xmm11, %xmm1, %xmm5
	vmulsd	%xmm11, %xmm3, %xmm11
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vsubsd	%xmm5, %xmm13, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm12, %xmm5, %xmm5
	vaddsd	%xmm11, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -108(%rbx)
	vmovsd	.LC30(%rip), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -100(%rbx)
	cmpq	%rbx, %r12
	jne	.L1863
.L2010:
	addq	$4096, %r12
	cmpq	-13616(%rbp), %r12
	jne	.L1856
	movq	-13648(%rbp), %rax
	movl	$0, -13552(%rbp)
	movq	%rax, -13624(%rbp)
.L1864:
	movslq	-13552(%rbp), %rdi
	movl	$15, %r14d
	movl	$6, %ebx
	xorl	%esi, %esi
	movq	-13624(%rbp), %rax
	movl	$8, %r13d
	movl	$7, %r12d
	movl	$5, %r11d
	movq	-13648(%rbp), %rdx
	movl	$4, %r10d
	movl	$3, %r9d
	movl	$2, %r8d
	movq	%rdi, %rcx
	salq	$7, %rdi
	salq	$5, %rcx
	movq	%rdi, -13512(%rbp)
	jmp	.L1867
.L2011:
	vmovss	4(%rax), %xmm0
	vmovss	4096(%rax), %xmm1
	vmovss	%xmm0, 4096(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %r8d
	ja	.L1866
	movslq	%r8d, %rdi
	vmovss	8192(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 8192(%rax)
	cmpl	$15, %r9d
	ja	.L1866
	movslq	%r9d, %rdi
	vmovss	12288(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 12288(%rax)
	cmpl	$15, %r10d
	ja	.L1866
	movslq	%r10d, %rdi
	vmovss	16384(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 16384(%rax)
	cmpl	$15, %r11d
	ja	.L1866
	movslq	%r11d, %rdi
	vmovss	20480(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 20480(%rax)
	cmpl	$15, %ebx
	ja	.L1866
	movslq	%ebx, %rdi
	vmovss	24576(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 24576(%rax)
	cmpl	$15, %r12d
	ja	.L1866
	movslq	%r12d, %rdi
	vmovss	28672(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 28672(%rax)
	cmpl	$15, %r13d
	ja	.L1866
	movslq	%r13d, %rdi
	vmovss	32768(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	8(%rsi), %edi
	vmovss	%xmm0, 32768(%rax)
	cmpl	$15, %edi
	ja	.L1866
	movslq	%edi, %rdi
	vmovss	36864(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	9(%rsi), %edi
	vmovss	%xmm0, 36864(%rax)
	cmpl	$15, %edi
	ja	.L1866
	movslq	%edi, %rdi
	vmovss	40960(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	10(%rsi), %edi
	vmovss	%xmm0, 40960(%rax)
	cmpl	$15, %edi
	ja	.L1866
	movslq	%edi, %rdi
	vmovss	45056(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	11(%rsi), %edi
	vmovss	%xmm0, 45056(%rax)
	cmpl	$15, %edi
	ja	.L1866
	movslq	%edi, %rdi
	vmovss	49152(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	12(%rsi), %edi
	vmovss	%xmm0, 49152(%rax)
	cmpl	$15, %edi
	ja	.L1866
	movslq	%edi, %rdi
	vmovss	53248(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	leal	13(%rsi), %edi
	vmovss	%xmm0, 53248(%rax)
	cmpl	$15, %edi
	ja	.L1866
	movslq	%edi, %rdi
	vmovss	57344(%rax), %xmm1
	addq	%rcx, %rdi
	vmovss	(%rdx,%rdi,4), %xmm0
	vmovss	%xmm1, (%rdx,%rdi,4)
	vmovss	%xmm0, 57344(%rax)
	cmpl	$15, %r14d
	jne	.L1866
	movq	-13512(%rbp), %rdi
	vmovss	61440(%rax), %xmm1
	addq	%rdx, %rdi
	vmovss	60(%rdi), %xmm0
	vmovss	%xmm1, 60(%rdi)
	vmovss	%xmm0, 61440(%rax)
.L1866:
	addl	$1, %r14d
	addq	$4096, %rdx
	addq	$4100, %rax
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %ebx
	addl	$1, %r12d
	addl	$1, %r13d
.L1867:
	addl	$1, %esi
	cmpl	$16, %esi
	jne	.L2011
	addl	$1, -13552(%rbp)
	movl	-13552(%rbp), %eax
	subq	$-128, -13624(%rbp)
	cmpl	$16, %eax
	jne	.L1864
.L1883:
	movq	-13584(%rbp), %rax
	movl	-13496(%rbp), %r14d
	leaq	-2048(%rax), %rbx
	movq	%rax, %r13
	movq	%rbx, %r12
	jmp	.L1874
.L1868:
	call	memcpy
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC36(%rip), %xmm4
	vcvtss2sd	-13424(%rbp), %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13420(%rbp), %xmm0, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	.LC33(%rip), %xmm0, %xmm3
	vmulsd	%xmm4, %xmm1, %xmm6
	vcvtss2sd	-13416(%rbp), %xmm5, %xmm5
	subq	$-128, %r12
	vmovsd	.LC34(%rip), %xmm7
	vmovss	-13360(%rbp), %xmm2
	vmulsd	%xmm7, %xmm5, %xmm10
	vsubsd	%xmm3, %xmm6, %xmm6
	vaddsd	%xmm10, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm2, %xmm6, %xmm6
	vmovss	%xmm6, -160(%r12)
	vmovsd	.LC35(%rip), %xmm6
	vmulsd	%xmm6, %xmm1, %xmm8
	vaddsd	%xmm8, %xmm3, %xmm3
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-13412(%rbp), %xmm8, %xmm8
	vmulsd	%xmm7, %xmm8, %xmm9
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm3
	vmovss	%xmm2, -156(%r12)
	vmovss	-13356(%rbp), %xmm2
	vaddsd	%xmm3, %xmm0, %xmm3
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm2, %xmm3, %xmm3
	vmovss	%xmm3, -152(%r12)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm10, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm0, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vmovss	%xmm1, -148(%r12)
	vmovss	-13352(%rbp), %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vaddsd	%xmm0, %xmm5, %xmm0
	vsubsd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm1, %xmm2, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm8, %xmm10, %xmm1
	vmovss	%xmm2, -144(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13408(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm11
	vmovss	%xmm0, -140(%r12)
	vaddsd	%xmm2, %xmm9, %xmm9
	vmovss	-13348(%rbp), %xmm0
	vsubsd	%xmm11, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -136(%r12)
	vmulsd	%xmm3, %xmm5, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13404(%rbp), %xmm5, %xmm5
	vaddsd	%xmm5, %xmm11, %xmm10
	vaddsd	%xmm1, %xmm8, %xmm1
	vmulsd	%xmm3, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm11, %xmm1
	vaddsd	%xmm8, %xmm2, %xmm8
	vmulsd	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	-13344(%rbp), %xmm1
	vmovss	%xmm0, -132(%r12)
	vmulsd	%xmm7, %xmm5, %xmm0
	vaddsd	%xmm2, %xmm5, %xmm2
	vmulsd	%xmm3, %xmm5, %xmm5
	vsubsd	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm8, %xmm0, %xmm8
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm1, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vaddss	%xmm8, %xmm1, %xmm1
	vmovss	-13340(%rbp), %xmm8
	vmovss	%xmm9, -128(%r12)
	vmovss	%xmm1, -124(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13400(%rbp), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm5, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm9, %xmm2
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm0, %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm8, %xmm10, %xmm10
	vmovss	-13336(%rbp), %xmm8
	vmovss	%xmm2, -116(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13396(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm7
	vmovss	%xmm10, -120(%r12)
	vmulsd	%xmm6, %xmm2, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm3, %xmm7, %xmm7
	vxorps	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm8, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	%xmm9, -112(%r12)
	vmovss	%xmm3, -108(%r12)
	vmovss	-13332(%rbp), %xmm3
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm5, -104(%r12)
	vmovss	%xmm0, -100(%r12)
	cmpq	%r12, %r13
	je	.L2012
.L1874:
	movq	-32(%r12), %rax
	movl	$32, %edx
	movq	%r12, %rsi
	leaq	-13360(%rbp), %rdi
	movq	%rax, (%r15)
	movq	-24(%r12), %rax
	movq	%rax, 8(%r15)
	movq	-16(%r12), %rax
	movq	%rax, 16(%r15)
	movq	-8(%r12), %rax
	movq	%rax, 24(%r15)
	cmpl	$1, %r14d
	ja	.L1868
	call	memcpy
	cmpb	$0, -13504(%rbp)
	jne	.L1870
	movl	-13360(%rbp), %r9d
	movl	-13356(%rbp), %r8d
	movl	-13352(%rbp), %edi
	movl	-13348(%rbp), %esi
	movl	-13344(%rbp), %ecx
	movl	-13340(%rbp), %edx
	movl	-13336(%rbp), %eax
	vmovss	-13424(%rbp), %xmm2
	vmovss	-13420(%rbp), %xmm14
	vmovss	-13416(%rbp), %xmm8
	vmovss	-13412(%rbp), %xmm10
	vmovss	-13408(%rbp), %xmm12
	vmovss	-13404(%rbp), %xmm9
	vmovss	-13400(%rbp), %xmm4
	vmovss	-13396(%rbp), %xmm3
	vmovss	-13332(%rbp), %xmm15
.L1871:
	vmovss	%xmm2, -32(%r12)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm11
	vmovss	%xmm14, -24(%r12)
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	subq	$-128, %r12
	vmovsd	.LC22(%rip), %xmm6
	vmulsd	%xmm11, %xmm2, %xmm7
	vmovss	%xmm8, -144(%r12)
	vcvtss2sd	%xmm8, %xmm8, %xmm8
	vmulsd	.LC21(%rip), %xmm14, %xmm5
	vmovss	%xmm3, -104(%r12)
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vmulsd	%xmm11, %xmm8, %xmm0
	vmovss	%xmm12, -128(%r12)
	vmovss	%xmm10, -136(%r12)
	vcvtss2sd	%xmm10, %xmm10, %xmm10
	vmulsd	%xmm6, %xmm10, %xmm13
	vmovss	%xmm4, -112(%r12)
	vmovss	%xmm9, -120(%r12)
	vaddsd	%xmm5, %xmm7, %xmm5
	vmovd	%r9d, %xmm7
	vsubsd	%xmm0, %xmm5, %xmm5
	vaddsd	%xmm13, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm5
	vmovsd	.LC27(%rip), %xmm7
	vmulsd	%xmm5, %xmm14, %xmm0
	vmovss	%xmm1, -156(%r12)
	vmulsd	%xmm7, %xmm2, %xmm2
	vmulsd	%xmm5, %xmm8, %xmm1
	vmulsd	%xmm7, %xmm14, %xmm14
	vmulsd	%xmm7, %xmm8, %xmm8
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovd	%r8d, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovd	%edi, %xmm1
	vsubsd	%xmm13, %xmm0, %xmm13
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm13, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm12, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm2, %xmm13
	vmovss	%xmm0, -148(%r12)
	vmulsd	%xmm5, %xmm10, %xmm0
	vmulsd	%xmm7, %xmm10, %xmm10
	vmulsd	%xmm7, %xmm2, %xmm7
	vaddsd	%xmm0, %xmm14, %xmm14
	vaddsd	%xmm8, %xmm0, %xmm8
	vsubsd	%xmm13, %xmm14, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	%xmm1, %xmm14, %xmm14
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm9, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm1, %xmm0
	vmovss	%xmm14, -140(%r12)
	vmulsd	%xmm5, %xmm2, %xmm14
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm14, %xmm8, %xmm8
	vaddsd	%xmm10, %xmm14, %xmm10
	vsubsd	%xmm0, %xmm8, %xmm8
	vmovd	%esi, %xmm0
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vaddss	%xmm0, %xmm8, %xmm8
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm4, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm14
	vmovd	%ecx, %xmm4
	vmulsd	%xmm6, %xmm3, %xmm6
	vmovss	%xmm8, -132(%r12)
	vmulsd	%xmm5, %xmm1, %xmm8
	vmulsd	%xmm5, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm0, %xmm12
	vaddsd	%xmm8, %xmm10, %xmm10
	vaddsd	%xmm7, %xmm8, %xmm8
	vsubsd	%xmm14, %xmm10, %xmm10
	vaddsd	%xmm5, %xmm8, %xmm5
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm10
	vsubsd	%xmm6, %xmm5, %xmm5
	vmovd	%edx, %xmm6
	vmovss	%xmm10, -124(%r12)
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovd	%eax, %xmm6
	vmovss	%xmm5, -116(%r12)
	vmulsd	%xmm11, %xmm1, %xmm5
	vmulsd	%xmm11, %xmm3, %xmm11
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vsubsd	%xmm5, %xmm13, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm12, %xmm5, %xmm5
	vaddsd	%xmm11, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -108(%r12)
	vmovsd	.LC30(%rip), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -100(%r12)
	cmpq	%r12, %r13
	jne	.L1874
.L2012:
	movq	-13584(%rbp), %rax
	movl	$15, %r14d
	xorl	%ecx, %ecx
	movl	$9, %r13d
	movl	$8, %r12d
	movl	$7, %r11d
	movl	$6, %r10d
	movq	%rbx, -13512(%rbp)
	movl	$5, %r9d
	movl	$4, %r8d
	movl	$3, %edi
	movl	$2, %esi
	leaq	-2080(%rax), %rdx
	movq	%rdx, %rax
	jmp	.L1917
.L1986:
	vmovss	4(%rax), %xmm0
	vmovss	128(%rax), %xmm1
	vmovss	%xmm0, 128(%rax)
	vmovss	%xmm1, 4(%rax)
	cmpl	$15, %esi
	ja	.L1876
	movslq	%esi, %rbx
	vmovss	256(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 256(%rax)
	cmpl	$15, %edi
	ja	.L1876
	movslq	%edi, %rbx
	vmovss	384(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 384(%rax)
	cmpl	$15, %r8d
	ja	.L1876
	movslq	%r8d, %rbx
	vmovss	512(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 512(%rax)
	cmpl	$15, %r9d
	ja	.L1876
	movslq	%r9d, %rbx
	vmovss	640(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 640(%rax)
	cmpl	$15, %r10d
	ja	.L1876
	movslq	%r10d, %rbx
	vmovss	768(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 768(%rax)
	cmpl	$15, %r11d
	ja	.L1876
	movslq	%r11d, %rbx
	vmovss	896(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 896(%rax)
	cmpl	$15, %r12d
	ja	.L1876
	movslq	%r12d, %rbx
	vmovss	1024(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 1024(%rax)
	cmpl	$15, %r13d
	ja	.L1876
	movslq	%r13d, %rbx
	vmovss	1152(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	leal	9(%rcx), %ebx
	vmovss	%xmm0, 1152(%rax)
	cmpl	$15, %ebx
	ja	.L1876
	movslq	%ebx, %rbx
	vmovss	1280(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	leal	10(%rcx), %ebx
	vmovss	%xmm0, 1280(%rax)
	cmpl	$15, %ebx
	ja	.L1876
	movslq	%ebx, %rbx
	vmovss	1408(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	leal	11(%rcx), %ebx
	vmovss	%xmm0, 1408(%rax)
	cmpl	$15, %ebx
	ja	.L1876
	movslq	%ebx, %rbx
	vmovss	1536(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	leal	12(%rcx), %ebx
	vmovss	%xmm0, 1536(%rax)
	cmpl	$15, %ebx
	ja	.L1876
	movslq	%ebx, %rbx
	vmovss	1664(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	leal	13(%rcx), %ebx
	vmovss	%xmm0, 1664(%rax)
	cmpl	$15, %ebx
	ja	.L1876
	movslq	%ebx, %rbx
	vmovss	1792(%rax), %xmm1
	leaq	(%rdx,%rbx,4), %rbx
	vmovss	(%rbx), %xmm0
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, 1792(%rax)
	cmpl	$15, %r14d
	jne	.L1876
	vmovss	60(%rdx), %xmm0
	vmovss	1920(%rax), %xmm1
	vmovss	%xmm1, 60(%rdx)
	vmovss	%xmm0, 1920(%rax)
.L1876:
	addl	$1, %r14d
	subq	$-128, %rdx
	addq	$132, %rax
	addl	$1, %esi
	addl	$1, %edi
	addl	$1, %r8d
	addl	$1, %r9d
	addl	$1, %r10d
	addl	$1, %r11d
	addl	$1, %r12d
	addl	$1, %r13d
.L1917:
	addl	$1, %ecx
	cmpl	$16, %ecx
	jne	.L1986
	movq	-13512(%rbp), %rbx
	movq	-13584(%rbp), %r12
	movzbl	-13504(%rbp), %r14d
	movl	-13496(%rbp), %r13d
	jmp	.L1875
.L1877:
	call	memcpy
	vxorpd	%xmm1, %xmm1, %xmm1
	vmovsd	.LC36(%rip), %xmm4
	vcvtss2sd	-13424(%rbp), %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13420(%rbp), %xmm0, %xmm0
	vxorpd	%xmm5, %xmm5, %xmm5
	vmulsd	.LC33(%rip), %xmm0, %xmm3
	vmulsd	%xmm4, %xmm1, %xmm6
	vcvtss2sd	-13416(%rbp), %xmm5, %xmm5
	subq	$-128, %rbx
	vmovsd	.LC34(%rip), %xmm7
	vmovss	-13360(%rbp), %xmm2
	vmulsd	%xmm7, %xmm5, %xmm10
	vsubsd	%xmm3, %xmm6, %xmm6
	vaddsd	%xmm10, %xmm6, %xmm6
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm2, %xmm6, %xmm6
	vmovss	%xmm6, -160(%rbx)
	vmovsd	.LC35(%rip), %xmm6
	vmulsd	%xmm6, %xmm1, %xmm8
	vaddsd	%xmm8, %xmm3, %xmm3
	vxorpd	%xmm8, %xmm8, %xmm8
	vcvtss2sd	-13412(%rbp), %xmm8, %xmm8
	vmulsd	%xmm7, %xmm8, %xmm9
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	%xmm3, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm3
	vmovss	%xmm2, -156(%rbx)
	vmovss	-13356(%rbp), %xmm2
	vaddsd	%xmm3, %xmm0, %xmm3
	vsubsd	%xmm10, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm2, %xmm3, %xmm3
	vmovss	%xmm3, -152(%rbx)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm1
	vaddsd	%xmm1, %xmm10, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm7, %xmm0, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vmovss	%xmm1, -148(%rbx)
	vmovss	-13352(%rbp), %xmm1
	vaddsd	%xmm2, %xmm5, %xmm2
	vaddsd	%xmm0, %xmm5, %xmm0
	vsubsd	%xmm9, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm9, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubss	%xmm1, %xmm2, %xmm2
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm8, %xmm10, %xmm1
	vmovss	%xmm2, -144(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13408(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm11
	vmovss	%xmm0, -140(%rbx)
	vmovss	-13348(%rbp), %xmm0
	vaddsd	%xmm2, %xmm9, %xmm9
	vsubsd	%xmm11, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm0, %xmm1, %xmm1
	vmovss	%xmm1, -136(%rbx)
	vmulsd	%xmm3, %xmm5, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13404(%rbp), %xmm5, %xmm5
	vaddsd	%xmm5, %xmm11, %xmm10
	vaddsd	%xmm1, %xmm8, %xmm1
	vmulsd	%xmm3, %xmm8, %xmm8
	vaddsd	%xmm1, %xmm11, %xmm1
	vaddsd	%xmm8, %xmm2, %xmm8
	vmulsd	%xmm3, %xmm2, %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	-13344(%rbp), %xmm1
	vmovss	%xmm0, -132(%rbx)
	vmulsd	%xmm7, %xmm5, %xmm0
	vaddsd	%xmm2, %xmm5, %xmm2
	vmulsd	%xmm3, %xmm5, %xmm5
	vsubsd	%xmm0, %xmm9, %xmm9
	vaddsd	%xmm8, %xmm0, %xmm8
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm1, %xmm9, %xmm9
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vaddss	%xmm8, %xmm1, %xmm1
	vmovss	-13340(%rbp), %xmm8
	vmovss	%xmm9, -128(%rbx)
	vmovss	%xmm1, -124(%rbx)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13400(%rbp), %xmm1, %xmm1
	vmulsd	%xmm7, %xmm1, %xmm9
	vaddsd	%xmm5, %xmm1, %xmm3
	vaddsd	%xmm2, %xmm9, %xmm2
	vsubsd	%xmm9, %xmm10, %xmm10
	vaddsd	%xmm1, %xmm0, %xmm9
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm8, %xmm2
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vsubss	%xmm8, %xmm10, %xmm10
	vmovss	-13336(%rbp), %xmm8
	vmovss	%xmm2, -116(%rbx)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13396(%rbp), %xmm2, %xmm2
	vmulsd	%xmm7, %xmm2, %xmm7
	vmovss	%xmm10, -120(%rbx)
	vmulsd	%xmm6, %xmm2, %xmm6
	vmulsd	%xmm4, %xmm2, %xmm2
	vsubsd	%xmm7, %xmm9, %xmm9
	vaddsd	%xmm3, %xmm7, %xmm7
	vxorps	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm9, %xmm9, %xmm9
	vsubss	%xmm8, %xmm9, %xmm9
	vcvtsd2ss	%xmm7, %xmm3, %xmm3
	vaddss	%xmm3, %xmm8, %xmm3
	vmovss	%xmm9, -112(%rbx)
	vmovss	%xmm3, -108(%rbx)
	vmovss	-13332(%rbp), %xmm3
	vmulsd	.LC33(%rip), %xmm1, %xmm1
	vaddsd	%xmm5, %xmm1, %xmm5
	vsubsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm6, %xmm5, %xmm5
	vaddsd	%xmm2, %xmm0, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm5, -104(%rbx)
	vmovss	%xmm0, -100(%rbx)
	cmpq	%r12, %rbx
	je	.L2013
.L1875:
	movq	-32(%rbx), %rax
	movl	$32, %edx
	movq	%rbx, %rsi
	leaq	-13360(%rbp), %rdi
	movq	%rax, (%r15)
	movq	-24(%rbx), %rax
	movq	%rax, 8(%r15)
	movq	-16(%rbx), %rax
	movq	%rax, 16(%r15)
	movq	-8(%rbx), %rax
	movq	%rax, 24(%r15)
	cmpl	$1, %r13d
	ja	.L1877
	call	memcpy
	testb	%r14b, %r14b
	jne	.L1879
	movl	-13360(%rbp), %r9d
	movl	-13356(%rbp), %r8d
	movl	-13352(%rbp), %edi
	movl	-13348(%rbp), %esi
	movl	-13344(%rbp), %ecx
	movl	-13340(%rbp), %edx
	movl	-13336(%rbp), %eax
	vmovss	-13424(%rbp), %xmm2
	vmovss	-13420(%rbp), %xmm14
	vmovss	-13416(%rbp), %xmm8
	vmovss	-13412(%rbp), %xmm10
	vmovss	-13408(%rbp), %xmm12
	vmovss	-13404(%rbp), %xmm9
	vmovss	-13400(%rbp), %xmm4
	vmovss	-13396(%rbp), %xmm3
	vmovss	-13332(%rbp), %xmm15
.L1880:
	vmovss	%xmm2, -32(%rbx)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vmovsd	.LC20(%rip), %xmm11
	vmovss	%xmm14, -24(%rbx)
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	subq	$-128, %rbx
	vmovsd	.LC22(%rip), %xmm6
	vmulsd	%xmm11, %xmm2, %xmm7
	vmovss	%xmm8, -144(%rbx)
	vcvtss2sd	%xmm8, %xmm8, %xmm8
	vmulsd	.LC21(%rip), %xmm14, %xmm5
	vmovss	%xmm10, -136(%rbx)
	vcvtss2sd	%xmm10, %xmm10, %xmm10
	vmulsd	%xmm11, %xmm8, %xmm0
	vmovss	%xmm3, -104(%rbx)
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vmulsd	%xmm6, %xmm10, %xmm13
	vmovss	%xmm12, -128(%rbx)
	vmovss	%xmm4, -112(%rbx)
	vmovss	%xmm9, -120(%rbx)
	vaddsd	%xmm5, %xmm7, %xmm5
	vmovd	%r9d, %xmm7
	vsubsd	%xmm0, %xmm5, %xmm5
	vaddsd	%xmm13, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm7, %xmm1, %xmm1
	vmovsd	.LC26(%rip), %xmm5
	vmovsd	.LC27(%rip), %xmm7
	vmulsd	%xmm5, %xmm14, %xmm0
	vmulsd	%xmm7, %xmm2, %xmm2
	vmovss	%xmm1, -156(%rbx)
	vmulsd	%xmm5, %xmm8, %xmm1
	vmulsd	%xmm7, %xmm14, %xmm14
	vmulsd	%xmm7, %xmm8, %xmm8
	vaddsd	%xmm0, %xmm2, %xmm0
	vmovd	%r8d, %xmm2
	vaddsd	%xmm14, %xmm1, %xmm14
	vaddsd	%xmm1, %xmm0, %xmm0
	vmovd	%edi, %xmm1
	vsubsd	%xmm13, %xmm0, %xmm13
	vxorps	%xmm0, %xmm0, %xmm0
	vcvtsd2ss	%xmm13, %xmm0, %xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm12, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm2, %xmm13
	vmovss	%xmm0, -148(%rbx)
	vmulsd	%xmm5, %xmm10, %xmm0
	vmulsd	%xmm7, %xmm10, %xmm10
	vmulsd	%xmm7, %xmm2, %xmm7
	vaddsd	%xmm0, %xmm14, %xmm14
	vaddsd	%xmm8, %xmm0, %xmm8
	vsubsd	%xmm13, %xmm14, %xmm14
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	%xmm1, %xmm14, %xmm14
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm9, %xmm1, %xmm1
	vmulsd	%xmm6, %xmm1, %xmm0
	vmovss	%xmm14, -140(%rbx)
	vmulsd	%xmm5, %xmm2, %xmm14
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vaddsd	%xmm14, %xmm8, %xmm8
	vaddsd	%xmm10, %xmm14, %xmm10
	vsubsd	%xmm0, %xmm8, %xmm8
	vmovd	%esi, %xmm0
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vaddss	%xmm0, %xmm8, %xmm8
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm4, %xmm0, %xmm0
	vmulsd	%xmm6, %xmm0, %xmm14
	vmovd	%ecx, %xmm4
	vmulsd	%xmm6, %xmm3, %xmm6
	vmovss	%xmm8, -132(%rbx)
	vmulsd	%xmm5, %xmm1, %xmm8
	vmulsd	%xmm5, %xmm0, %xmm5
	vmulsd	.LC21(%rip), %xmm0, %xmm12
	vaddsd	%xmm8, %xmm10, %xmm10
	vaddsd	%xmm7, %xmm8, %xmm8
	vsubsd	%xmm14, %xmm10, %xmm10
	vaddsd	%xmm5, %xmm8, %xmm5
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vaddss	%xmm4, %xmm10, %xmm10
	vsubsd	%xmm6, %xmm5, %xmm5
	vmovd	%edx, %xmm6
	vmovss	%xmm10, -124(%rbx)
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovd	%eax, %xmm6
	vmovss	%xmm5, -116(%rbx)
	vmulsd	%xmm11, %xmm1, %xmm5
	vmulsd	%xmm11, %xmm3, %xmm11
	vmulsd	.LC29(%rip), %xmm1, %xmm1
	vsubsd	%xmm5, %xmm13, %xmm5
	vaddsd	%xmm1, %xmm2, %xmm1
	vaddsd	%xmm12, %xmm5, %xmm5
	vaddsd	%xmm11, %xmm5, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm5, -108(%rbx)
	vmovsd	.LC30(%rip), %xmm5
	vmulsd	%xmm5, %xmm0, %xmm6
	vmulsd	%xmm5, %xmm3, %xmm3
	vsubsd	%xmm6, %xmm1, %xmm0
	vaddsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm15, %xmm0, %xmm0
	vmovss	%xmm0, -100(%rbx)
	cmpq	%r12, %rbx
	jne	.L1875
.L2013:
	leaq	4096(%rbx), %rax
	movq	%rax, -13584(%rbp)
	cmpq	%rax, -13616(%rbp)
	jne	.L1883
	movq	-13664(%rbp), %rax
	vmovsd	.LC33(%rip), %xmm11
	vmovsd	.LC34(%rip), %xmm8
	vmovapd	.LC38(%rip), %ymm9
	vmovapd	.LC39(%rip), %ymm10
	addq	$131092, %rax
	movq	%rax, -13632(%rbp)
	movq	-13648(%rbp), %rax
	movq	%rax, -13624(%rbp)
.L1884:
	leaq	64(%rax), %r13
	movq	%rax, %r12
	movl	$32, %r14d
	jmp	.L1891
	.p2align 4,,10
	.p2align 3
.L1885:
	subq	%r13, %rdx
	movq	%rdx, %rax
	shrq	$2, %rax
	jne	.L2014
.L1890:
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13424(%rbp), %xmm3, %xmm3
	vmovsd	.LC36(%rip), %xmm12
	vcvtss2sd	-13420(%rbp), %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm2
	vmovsd	.LC35(%rip), %xmm13
	vxorpd	%xmm1, %xmm1, %xmm1
	vmulsd	%xmm12, %xmm3, %xmm4
	vcvtss2sd	-13416(%rbp), %xmm1, %xmm1
	subq	$-128, %r13
	vmovss	-13360(%rbp), %xmm0
	vmulsd	%xmm13, %xmm3, %xmm3
	vmovaps	-13424(%rbp), %ymm5
	vmulsd	%xmm8, %xmm1, %xmm1
	vcvtps2pd	%xmm5, %ymm14
	vextractf128	$0x1, %ymm5, %xmm5
	vcvtps2pd	%xmm5, %ymm5
	vsubsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm2, %xmm1
	vmulpd	%ymm9, %ymm14, %ymm2
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm0, %xmm4, %xmm4
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	vmovups	-13356(%rbp), %ymm1
	vmovss	%xmm4, (%r12)
	vmovups	-13420(%rbp), %ymm4
	vmovss	%xmm0, 4(%r12)
	vmovups	-13416(%rbp), %ymm0
	vcvtps2pd	%xmm4, %ymm7
	vextractf128	$0x1, %ymm4, %xmm4
	vaddpd	%ymm7, %ymm2, %ymm2
	vcvtps2pd	%xmm4, %ymm4
	vcvtps2pd	%xmm0, %ymm6
	vextractf128	$0x1, %ymm0, %xmm3
	vmulpd	%ymm9, %ymm5, %ymm0
	vaddpd	%ymm4, %ymm0, %ymm0
	vmulpd	%ymm9, %ymm6, %ymm6
	vcvtps2pd	%xmm3, %ymm3
	vmulpd	%ymm9, %ymm3, %ymm3
	vmulpd	%ymm10, %ymm5, %ymm5
	vsubpd	%ymm6, %ymm2, %ymm2
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm5, %ymm4, %ymm4
	vcvtpd2psy	%ymm2, %xmm2
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm2, %ymm0
	vsubps	%ymm1, %ymm0, %ymm2
	vmulpd	%ymm10, %ymm14, %ymm0
	vaddpd	%ymm0, %ymm7, %ymm0
	vaddpd	%ymm4, %ymm3, %ymm3
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13392(%rbp), %xmm4, %xmm4
	vaddpd	%ymm0, %ymm6, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vcvtpd2psy	%ymm0, %xmm0
	vinsertf128	$0x1, %xmm3, %ymm0, %ymm0
	vmulsd	%xmm8, %xmm4, %xmm3
	vaddps	%ymm0, %ymm1, %ymm0
	vunpcklps	%ymm0, %ymm2, %ymm1
	vunpckhps	%ymm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vmovups	%ymm0, 40(%r12)
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13384(%rbp), %xmm0, %xmm0
	vmulsd	%xmm8, %xmm0, %xmm5
	vmovups	%ymm2, 8(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13388(%rbp), %xmm2, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm3
	vmovss	-13324(%rbp), %xmm1
	vmulsd	%xmm8, %xmm2, %xmm7
	vsubsd	%xmm5, %xmm3, %xmm3
	vaddsd	%xmm0, %xmm7, %xmm7
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm1, %xmm3, %xmm3
	vmovss	%xmm3, 72(%r12)
	vmovsd	.LC40(%rip), %xmm3
	vmulsd	%xmm3, %xmm4, %xmm4
	vaddsd	%xmm4, %xmm2, %xmm4
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm4, %xmm5, %xmm4
	vaddsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	%xmm4, %xmm1, %xmm1
	vmovss	-13320(%rbp), %xmm4
	vmovss	%xmm1, 76(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13380(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm6
	vaddsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm5, %xmm1, %xmm5
	vmulsd	%xmm3, %xmm1, %xmm1
	vsubsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm6, %xmm2
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm4, %xmm7, %xmm7
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vmovss	-13316(%rbp), %xmm4
	vmovss	%xmm7, 80(%r12)
	vmovss	%xmm2, 84(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13376(%rbp), %xmm2, %xmm2
	vmulsd	%xmm8, %xmm2, %xmm7
	vaddsd	%xmm2, %xmm6, %xmm6
	vaddsd	%xmm1, %xmm2, %xmm1
	vmulsd	%xmm3, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm7, %xmm0
	vsubsd	%xmm7, %xmm5, %xmm5
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vsubss	%xmm4, %xmm5, %xmm5
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13372(%rbp), %xmm4, %xmm4
	vaddsd	%xmm4, %xmm7, %xmm7
	vmovss	%xmm0, 92(%r12)
	vmulsd	%xmm8, %xmm4, %xmm0
	vaddsd	%xmm2, %xmm4, %xmm2
	vmovss	%xmm5, 88(%r12)
	vmovss	-13312(%rbp), %xmm5
	vmulsd	%xmm3, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm6, %xmm6
	vaddsd	%xmm1, %xmm0, %xmm1
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm5, %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm5, %xmm1
	vmovss	%xmm6, 96(%r12)
	vmovss	%xmm1, 100(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm6
	vmovss	-13308(%rbp), %xmm5
	vaddsd	%xmm4, %xmm1, %xmm3
	vsubsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm2, %xmm6, %xmm6
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13364(%rbp), %xmm2, %xmm2
	vmulsd	%xmm13, %xmm2, %xmm13
	vmulsd	%xmm12, %xmm2, %xmm12
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm5, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vaddss	%xmm6, %xmm5, %xmm5
	vmulsd	%xmm8, %xmm2, %xmm6
	vmovss	%xmm7, 104(%r12)
	vaddsd	%xmm1, %xmm0, %xmm7
	vmulsd	%xmm11, %xmm1, %xmm1
	vmovss	%xmm5, 108(%r12)
	vmovss	-13304(%rbp), %xmm5
	vsubsd	%xmm6, %xmm7, %xmm7
	vaddsd	%xmm3, %xmm6, %xmm6
	vmovss	-13300(%rbp), %xmm3
	vaddsd	%xmm1, %xmm4, %xmm4
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vsubss	%xmm5, %xmm7, %xmm7
	vaddsd	%xmm13, %xmm4, %xmm4
	vaddsd	%xmm12, %xmm0, %xmm0
	vaddss	%xmm6, %xmm5, %xmm5
	vmovss	%xmm7, 112(%r12)
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm3, %xmm4, %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vmovss	%xmm5, 116(%r12)
	vmovss	%xmm4, 120(%r12)
	vmovss	%xmm0, 124(%r12)
	movq	%rbx, %r12
	subl	$1, %r14d
	je	.L2015
.L1891:
	movq	(%r12), %rax
	leaq	128(%r12), %rbx
	cmpl	$1, -13496(%rbp)
	movq	%rbx, %rdx
	movq	%rax, (%r15)
	movq	8(%r12), %rax
	movq	%rax, 8(%r15)
	movq	16(%r12), %rax
	movq	%rax, 16(%r15)
	movq	24(%r12), %rax
	movq	%rax, 24(%r15)
	movq	32(%r12), %rax
	movq	%rax, 32(%r15)
	movq	40(%r12), %rax
	movq	%rax, 40(%r15)
	movq	48(%r12), %rax
	movq	%rax, 48(%r15)
	movq	56(%r12), %rax
	movq	%rax, 56(%r15)
	ja	.L1885
	subq	%r13, %rdx
	movq	%rdx, %rax
	shrq	$2, %rax
	jne	.L2016
.L1886:
	cmpb	$0, -13504(%rbp)
	jne	.L1887
	movl	-13408(%rbp), %r10d
	movl	-13404(%rbp), %edi
	movl	-13400(%rbp), %r8d
	movl	-13396(%rbp), %r9d
	movl	-13392(%rbp), %r11d
	movl	-13376(%rbp), %edx
	movl	-13372(%rbp), %eax
	movl	-13304(%rbp), %esi
	movl	-13300(%rbp), %ecx
	vmovss	-13424(%rbp), %xmm4
	vmovss	-13420(%rbp), %xmm3
	vmovss	-13416(%rbp), %xmm2
	vmovss	-13412(%rbp), %xmm1
	vmovss	-13388(%rbp), %xmm15
	vmovss	-13384(%rbp), %xmm13
	vmovss	-13380(%rbp), %xmm12
	vmovss	-13368(%rbp), %xmm6
	vmovss	-13364(%rbp), %xmm0
	vmovss	-13360(%rbp), %xmm5
.L1888:
	vmovss	%xmm4, (%r12)
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	%xmm2, %xmm7, %xmm7
	vmovss	%xmm3, 8(%r12)
	vcvtss2sd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm14, %xmm14, %xmm14
	vmulsd	.LC21(%rip), %xmm3, %xmm3
	vmovss	%xmm2, 16(%r12)
	vcvtss2sd	-13412(%rbp), %xmm14, %xmm14
	subq	$-128, %r13
	vmovss	%xmm1, 24(%r12)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	movl	%r10d, 32(%r12)
	movl	%edi, 40(%r12)
	movl	%r8d, 48(%r12)
	movl	%r9d, 56(%r12)
	movl	%r11d, -128(%r13)
	vmovss	%xmm12, 88(%r12)
	vmovsd	.LC20(%rip), %xmm12
	vmovss	%xmm15, 72(%r12)
	vxorpd	%xmm15, %xmm15, %xmm15
	vcvtss2sd	-13420(%rbp), %xmm15, %xmm15
	vmulsd	%xmm12, %xmm4, %xmm4
	vmovss	%xmm13, 80(%r12)
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-13416(%rbp), %xmm13, %xmm13
	vmulsd	%xmm12, %xmm7, %xmm7
	vmovss	%xmm6, 112(%r12)
	vmovss	%xmm0, 120(%r12)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	movl	%edx, 96(%r12)
	movl	%eax, 104(%r12)
	vaddsd	%xmm3, %xmm4, %xmm3
	vsubsd	%xmm7, %xmm3, %xmm2
	vmovsd	.LC22(%rip), %xmm7
	vmulsd	%xmm7, %xmm1, %xmm1
	vaddsd	%xmm1, %xmm2, %xmm1
	vmovsd	.LC27(%rip), %xmm2
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm5, %xmm1, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13424(%rbp), %xmm5, %xmm5
	vmulsd	%xmm2, %xmm5, %xmm5
	vmovss	%xmm1, 4(%r12)
	vmovsd	.LC26(%rip), %xmm1
	vmulsd	%xmm1, %xmm15, %xmm3
	vmulsd	%xmm1, %xmm13, %xmm4
	vmulsd	%xmm2, %xmm15, %xmm15
	vmulsd	%xmm2, %xmm13, %xmm13
	vaddsd	%xmm3, %xmm5, %xmm3
	vmulsd	%xmm7, %xmm14, %xmm5
	vaddsd	%xmm4, %xmm15, %xmm15
	vaddsd	%xmm4, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm14, %xmm5
	vmulsd	%xmm2, %xmm14, %xmm14
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13356(%rbp), %xmm3, %xmm3
	vaddsd	%xmm5, %xmm15, %xmm4
	vmovss	%xmm3, 12(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13408(%rbp), %xmm3, %xmm3
	vmulsd	%xmm7, %xmm3, %xmm15
	vaddsd	%xmm5, %xmm13, %xmm13
	vsubsd	%xmm15, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm3, %xmm15
	vmulsd	%xmm2, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13352(%rbp), %xmm4, %xmm4
	vaddsd	%xmm15, %xmm13, %xmm5
	vmovss	%xmm4, 20(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13404(%rbp), %xmm4, %xmm4
	vmulsd	%xmm7, %xmm4, %xmm13
	vaddsd	%xmm15, %xmm14, %xmm14
	vsubsd	%xmm13, %xmm5, %xmm5
	vmulsd	%xmm1, %xmm4, %xmm13
	vmulsd	%xmm2, %xmm4, %xmm4
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13348(%rbp), %xmm5, %xmm5
	vaddsd	%xmm13, %xmm14, %xmm14
	vmovss	%xmm5, 28(%r12)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13400(%rbp), %xmm5, %xmm5
	vmulsd	%xmm7, %xmm5, %xmm15
	vaddsd	%xmm13, %xmm3, %xmm3
	vsubsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm1, %xmm5, %xmm15
	vmulsd	%xmm2, %xmm5, %xmm5
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	-13344(%rbp), %xmm14, %xmm14
	vaddsd	%xmm15, %xmm3, %xmm3
	vmovss	%xmm14, 36(%r12)
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-13396(%rbp), %xmm14, %xmm14
	vmulsd	%xmm7, %xmm14, %xmm13
	vaddsd	%xmm15, %xmm4, %xmm4
	vsubsd	%xmm13, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm14, %xmm13
	vmulsd	%xmm2, %xmm14, %xmm14
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13340(%rbp), %xmm3, %xmm3
	vaddsd	%xmm13, %xmm4, %xmm4
	vmovss	%xmm3, 44(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13392(%rbp), %xmm3, %xmm3
	vmulsd	%xmm7, %xmm3, %xmm15
	vaddsd	%xmm13, %xmm5, %xmm5
	vsubsd	%xmm15, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm3, %xmm15
	vmulsd	%xmm2, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13336(%rbp), %xmm4, %xmm4
	vaddsd	%xmm15, %xmm5, %xmm5
	vmovss	%xmm4, 52(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13388(%rbp), %xmm4, %xmm4
	vmulsd	%xmm7, %xmm4, %xmm13
	vaddsd	%xmm15, %xmm14, %xmm14
	vsubsd	%xmm13, %xmm5, %xmm5
	vmulsd	%xmm1, %xmm4, %xmm13
	vmulsd	%xmm2, %xmm4, %xmm4
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13332(%rbp), %xmm5, %xmm5
	vaddsd	%xmm13, %xmm14, %xmm14
	vmovss	%xmm5, 60(%r12)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13384(%rbp), %xmm5, %xmm5
	vmulsd	%xmm7, %xmm5, %xmm15
	vaddsd	%xmm13, %xmm3, %xmm3
	vsubsd	%xmm15, %xmm14, %xmm14
	vmulsd	%xmm1, %xmm5, %xmm15
	vmulsd	%xmm2, %xmm5, %xmm5
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vaddss	-13328(%rbp), %xmm14, %xmm14
	vaddsd	%xmm15, %xmm3, %xmm3
	vmovss	%xmm14, 68(%r12)
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-13380(%rbp), %xmm14, %xmm14
	vmulsd	%xmm7, %xmm14, %xmm13
	vaddsd	%xmm15, %xmm4, %xmm4
	vsubsd	%xmm13, %xmm3, %xmm3
	vmulsd	%xmm1, %xmm14, %xmm13
	vmulsd	%xmm2, %xmm14, %xmm14
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13324(%rbp), %xmm3, %xmm3
	vaddsd	%xmm13, %xmm4, %xmm4
	vmovss	%xmm3, 76(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13376(%rbp), %xmm3, %xmm3
	vmulsd	%xmm7, %xmm3, %xmm15
	vaddsd	%xmm13, %xmm5, %xmm5
	vmulsd	%xmm2, %xmm3, %xmm2
	vsubsd	%xmm15, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm3, %xmm15
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13320(%rbp), %xmm4, %xmm4
	vaddsd	%xmm15, %xmm5, %xmm5
	vmovss	%xmm4, 84(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13372(%rbp), %xmm4, %xmm4
	vmulsd	%xmm7, %xmm4, %xmm13
	vmulsd	%xmm1, %xmm4, %xmm4
	vaddsd	%xmm15, %xmm14, %xmm14
	vsubsd	%xmm13, %xmm5, %xmm5
	vaddsd	%xmm4, %xmm14, %xmm13
	vaddsd	%xmm4, %xmm2, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm6, %xmm4, %xmm4
	vmovd	%esi, %xmm6
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13316(%rbp), %xmm5, %xmm5
	vmovss	%xmm5, 92(%r12)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13368(%rbp), %xmm5, %xmm5
	vmulsd	%xmm7, %xmm5, %xmm14
	vmulsd	%xmm1, %xmm5, %xmm1
	vmovd	%edx, %xmm5
	vsubsd	%xmm14, %xmm13, %xmm13
	vaddsd	%xmm1, %xmm2, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm5, %xmm2, %xmm2
	vmovd	%eax, %xmm5
	vcvtss2sd	%xmm5, %xmm3, %xmm3
	vmulsd	%xmm7, %xmm2, %xmm5
	vmulsd	.LC28(%rip), %xmm2, %xmm2
	vcvtsd2ss	%xmm13, %xmm13, %xmm13
	vaddss	-13312(%rbp), %xmm13, %xmm13
	vmovss	%xmm13, 100(%r12)
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-13364(%rbp), %xmm13, %xmm13
	vmulsd	%xmm7, %xmm13, %xmm13
	vsubsd	%xmm13, %xmm1, %xmm1
	vmulsd	%xmm12, %xmm3, %xmm13
	vmulsd	%xmm12, %xmm0, %xmm12
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	-13308(%rbp), %xmm1, %xmm1
	vsubsd	%xmm13, %xmm5, %xmm5
	vmovss	%xmm1, 108(%r12)
	vmulsd	.LC21(%rip), %xmm4, %xmm1
	vaddsd	%xmm1, %xmm5, %xmm1
	vaddsd	%xmm12, %xmm1, %xmm1
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm6, %xmm1, %xmm1
	vmovd	%ecx, %xmm6
	vmovss	%xmm1, 116(%r12)
	vmulsd	.LC29(%rip), %xmm3, %xmm3
	vmovsd	.LC30(%rip), %xmm1
	vmulsd	%xmm1, %xmm4, %xmm4
	vmulsd	%xmm1, %xmm0, %xmm0
	vaddsd	%xmm3, %xmm2, %xmm2
	vsubsd	%xmm4, %xmm2, %xmm2
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, 124(%r12)
	movq	%rbx, %r12
	subl	$1, %r14d
	jne	.L1891
.L2015:
	addq	$4096, -13624(%rbp)
	movq	-13624(%rbp), %rax
	cmpq	-13632(%rbp), %rax
	jne	.L1884
	movq	-13664(%rbp), %r11
	xorl	%r10d, %r10d
	addq	$4116, %r11
.L1897:
	xorl	%r8d, %r8d
	movslq	%r10d, %rdi
	movq	-13648(%rbp), %rsi
	movq	%r11, %r9
	addl	$1, %r8d
	salq	$5, %rdi
	cmpl	$32, %r8d
	je	.L1893
	.p2align 4,,10
	.p2align 3
.L2017:
	movq	%r9, %rcx
	movl	%r8d, %edx
	.p2align 4,,10
	.p2align 3
.L1894:
	movslq	%edx, %rax
	vmovss	(%rcx), %xmm1
	addl	$1, %edx
	addq	$4096, %rcx
	addq	%rdi, %rax
	vmovss	(%rsi,%rax,4), %xmm0
	vmovss	%xmm1, (%rsi,%rax,4)
	vmovss	%xmm0, -4096(%rcx)
	cmpl	$32, %edx
	jne	.L1894
	addl	$1, %r8d
	addq	$4100, %r9
	addq	$4096, %rsi
	cmpl	$32, %r8d
	jne	.L2017
.L1893:
	addl	$1, %r10d
	subq	$-128, %r11
	cmpl	$32, %r10d
	jne	.L1897
	movq	-13520(%rbp), %rax
	vmovapd	.LC38(%rip), %ymm10
	vmovapd	%ymm10, %ymm9
.L1914:
	leaq	64(%rax), %r13
	movl	$32, %r10d
	movq	%rax, %r12
	vmovsd	.LC34(%rip), %xmm8
	movq	%r13, %r14
	movl	%r10d, %ebx
	movq	%r13, -13616(%rbp)
	jmp	.L1904
	.p2align 4,,10
	.p2align 3
.L1898:
	subq	%r14, %rdx
	movq	%rdx, %rax
	shrq	$2, %rax
	jne	.L2018
.L1903:
	vxorpd	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13424(%rbp), %xmm4, %xmm4
	vmovsd	.LC36(%rip), %xmm6
	vcvtss2sd	-13420(%rbp), %xmm0, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovsd	.LC35(%rip), %xmm5
	vcvtss2sd	-13416(%rbp), %xmm3, %xmm3
	vmulsd	%xmm6, %xmm4, %xmm1
	vmovss	-13360(%rbp), %xmm2
	subq	$-128, %r14
	vmulsd	%xmm5, %xmm4, %xmm4
	vmovaps	-13424(%rbp), %ymm7
	vmulsd	%xmm8, %xmm3, %xmm3
	vmovups	-13356(%rbp), %ymm11
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtps2pd	%xmm7, %ymm13
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmulpd	%ymm10, %ymm7, %ymm14
	vsubsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vmovups	-13420(%rbp), %ymm4
	vaddsd	%xmm3, %xmm1, %xmm1
	vcvtps2pd	%xmm4, %ymm12
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vsubsd	%xmm3, %xmm0, %xmm0
	vaddpd	%ymm4, %ymm14, %ymm14
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm1, (%r12)
	vmovss	%xmm0, 4(%r12)
	vmovups	-13416(%rbp), %ymm0
	vcvtps2pd	%xmm0, %ymm3
	vextractf128	$0x1, %ymm0, %xmm2
	vmulpd	%ymm10, %ymm13, %ymm0
	vaddpd	%ymm12, %ymm0, %ymm0
	vmulpd	%ymm10, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm10, %ymm2, %ymm2
	vsubpd	%ymm2, %ymm14, %ymm14
	vmulpd	.LC39(%rip), %ymm13, %ymm13
	vaddpd	%ymm13, %ymm12, %ymm12
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm12, %ymm3, %ymm3
	vcvtpd2psy	%ymm0, %xmm1
	vcvtpd2psy	%ymm14, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmulpd	.LC39(%rip), %ymm7, %ymm1
	vmovsd	.LC40(%rip), %xmm7
	vaddpd	%ymm1, %ymm4, %ymm1
	vsubps	%ymm11, %ymm0, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vmovss	-13324(%rbp), %xmm4
	vaddpd	%ymm1, %ymm2, %ymm1
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm3, %ymm1
	vaddps	%ymm1, %ymm11, %ymm11
	vunpcklps	%ymm11, %ymm0, %ymm1
	vunpckhps	%ymm11, %ymm0, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13392(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm3
	vmovups	%ymm2, 8(%r12)
	vmulsd	%xmm7, %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13384(%rbp), %xmm2, %xmm2
	vmovups	%ymm0, 40(%r12)
	vmulsd	%xmm8, %xmm2, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13388(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm0, %xmm1
	vsubsd	%xmm11, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm11, %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vmulsd	%xmm8, %xmm0, %xmm4
	vmulsd	%xmm7, %xmm0, %xmm0
	vmovss	%xmm3, 72(%r12)
	vmovss	-13320(%rbp), %xmm3
	vmovss	%xmm1, 76(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13380(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm12
	vaddsd	%xmm11, %xmm1, %xmm11
	vaddsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm12, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm12, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm3, %xmm4, %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm1
	vmovss	%xmm4, 80(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13376(%rbp), %xmm4, %xmm4
	vmulsd	%xmm8, %xmm4, %xmm3
	vmovss	%xmm0, 84(%r12)
	vmovss	-13316(%rbp), %xmm0
	vaddsd	%xmm4, %xmm12, %xmm12
	vaddsd	%xmm1, %xmm4, %xmm1
	vmulsd	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm3, %xmm2
	vsubsd	%xmm3, %xmm11, %xmm11
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm2
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm0, %xmm11, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13372(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm7, %xmm0, %xmm7
	vmovss	%xmm2, 92(%r12)
	vmulsd	%xmm8, %xmm0, %xmm2
	vaddsd	%xmm4, %xmm0, %xmm4
	vmovss	%xmm11, 88(%r12)
	vmovss	-13312(%rbp), %xmm11
	vaddsd	%xmm1, %xmm2, %xmm1
	vsubsd	%xmm2, %xmm12, %xmm12
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm1
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vsubss	%xmm11, %xmm12, %xmm12
	vmovss	%xmm1, 100(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm11
	vmovss	%xmm12, 96(%r12)
	vmovss	-13308(%rbp), %xmm12
	vaddsd	%xmm7, %xmm1, %xmm0
	vsubsd	%xmm11, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm11, %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm12, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	%xmm4, %xmm12, %xmm4
	vmovss	-13304(%rbp), %xmm12
	vmovss	%xmm3, 104(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13364(%rbp), %xmm3, %xmm3
	vmulsd	%xmm8, %xmm3, %xmm11
	vmovss	%xmm4, 108(%r12)
	vaddsd	%xmm1, %xmm2, %xmm4
	vmulsd	%xmm5, %xmm3, %xmm5
	vsubsd	%xmm11, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm11, %xmm11
	vmulsd	.LC33(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm12, %xmm4, %xmm4
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddsd	%xmm0, %xmm7, %xmm7
	vmovss	%xmm4, 112(%r12)
	vmovss	-13300(%rbp), %xmm4
	vsubsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm3, %xmm0
	vmovss	%xmm11, 116(%r12)
	vaddsd	%xmm5, %xmm7, %xmm7
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm4, %xmm7, %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm7, 120(%r12)
	vmovss	%xmm0, 124(%r12)
	movq	%r13, %r12
	subl	$1, %ebx
	je	.L2019
.L1904:
	movq	(%r12), %rax
	leaq	128(%r12), %r13
	cmpl	$1, -13496(%rbp)
	movq	%r13, %rdx
	movq	%rax, (%r15)
	movq	8(%r12), %rax
	movq	%rax, 8(%r15)
	movq	16(%r12), %rax
	movq	%rax, 16(%r15)
	movq	24(%r12), %rax
	movq	%rax, 24(%r15)
	movq	32(%r12), %rax
	movq	%rax, 32(%r15)
	movq	40(%r12), %rax
	movq	%rax, 40(%r15)
	movq	48(%r12), %rax
	movq	%rax, 48(%r15)
	movq	56(%r12), %rax
	movq	%rax, 56(%r15)
	ja	.L1898
	subq	%r14, %rdx
	movq	%rdx, %rax
	shrq	$2, %rax
	jne	.L2020
.L1899:
	cmpb	$0, -13504(%rbp)
	jne	.L1900
	movl	-13404(%rbp), %r8d
	movl	-13400(%rbp), %r9d
	movl	-13396(%rbp), %r10d
	movl	-13392(%rbp), %r11d
	movl	-13372(%rbp), %ecx
	movl	-13368(%rbp), %edx
	movl	-13364(%rbp), %eax
	movl	-13304(%rbp), %edi
	movl	-13300(%rbp), %esi
	vmovss	-13424(%rbp), %xmm0
	vmovss	-13420(%rbp), %xmm4
	vmovss	-13416(%rbp), %xmm3
	vmovss	-13412(%rbp), %xmm2
	vmovss	-13408(%rbp), %xmm15
	vmovss	-13388(%rbp), %xmm12
	vmovss	-13384(%rbp), %xmm7
	vmovss	-13380(%rbp), %xmm6
	vmovss	-13376(%rbp), %xmm1
	vmovss	-13360(%rbp), %xmm13
.L1901:
	vmovsd	.LC20(%rip), %xmm11
	vmovss	%xmm0, (%r12)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	subq	$-128, %r14
	vmovss	%xmm4, 8(%r12)
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmovss	%xmm3, 16(%r12)
	vmulsd	%xmm11, %xmm0, %xmm0
	vmovss	%xmm2, 24(%r12)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vmovss	%xmm15, 32(%r12)
	movl	%r8d, 40(%r12)
	movl	%r9d, 48(%r12)
	movl	%r10d, 56(%r12)
	movl	%r11d, -128(%r14)
	vmovss	%xmm12, 72(%r12)
	vmovsd	.LC21(%rip), %xmm12
	vmovss	%xmm7, 80(%r12)
	vmovsd	.LC27(%rip), %xmm7
	vmulsd	%xmm12, %xmm4, %xmm4
	vmovss	%xmm6, 88(%r12)
	vmovsd	.LC26(%rip), %xmm6
	vmovss	%xmm1, 96(%r12)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	movl	%ecx, 104(%r12)
	movl	%edx, 112(%r12)
	movl	%eax, 120(%r12)
	vaddsd	%xmm4, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm4
	vsubsd	%xmm4, %xmm0, %xmm3
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13420(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm5
	vmovsd	.LC22(%rip), %xmm0
	vmulsd	%xmm7, %xmm4, %xmm4
	vmulsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13416(%rbp), %xmm3, %xmm3
	vmulsd	%xmm6, %xmm3, %xmm14
	vmulsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm13, %xmm2, %xmm2
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-13424(%rbp), %xmm13, %xmm13
	vmulsd	%xmm7, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm4
	vmovss	%xmm2, 4(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13412(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm15
	vaddsd	%xmm5, %xmm13, %xmm5
	vmulsd	%xmm6, %xmm2, %xmm13
	vmulsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm5, %xmm5
	vaddsd	%xmm13, %xmm4, %xmm4
	vsubsd	%xmm15, %xmm5, %xmm5
	vaddsd	%xmm13, %xmm3, %xmm3
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13356(%rbp), %xmm5, %xmm5
	vmovss	%xmm5, 12(%r12)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13408(%rbp), %xmm5, %xmm5
	vmulsd	%xmm0, %xmm5, %xmm15
	vmulsd	%xmm6, %xmm5, %xmm14
	vmulsd	%xmm7, %xmm5, %xmm5
	vsubsd	%xmm15, %xmm4, %xmm4
	vaddsd	%xmm14, %xmm3, %xmm3
	vaddsd	%xmm14, %xmm2, %xmm2
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13352(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 20(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13404(%rbp), %xmm4, %xmm4
	vmulsd	%xmm0, %xmm4, %xmm15
	vmulsd	%xmm6, %xmm4, %xmm13
	vmulsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm15, %xmm3, %xmm3
	vaddsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm13, %xmm5, %xmm5
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13348(%rbp), %xmm3, %xmm3
	vmovss	%xmm3, 28(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13400(%rbp), %xmm3, %xmm3
	vmulsd	%xmm0, %xmm3, %xmm15
	vmulsd	%xmm6, %xmm3, %xmm14
	vcvtss2sd	-13392(%rbp), %xmm13, %xmm13
	vmulsd	%xmm7, %xmm3, %xmm3
	vsubsd	%xmm15, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm5, %xmm5
	vaddsd	%xmm14, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm13, %xmm14
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13344(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 36(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13396(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm15
	vsubsd	%xmm15, %xmm5, %xmm5
	vmulsd	%xmm6, %xmm2, %xmm15
	vmulsd	%xmm7, %xmm2, %xmm2
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13340(%rbp), %xmm5, %xmm5
	vaddsd	%xmm15, %xmm4, %xmm4
	vmovss	%xmm5, 44(%r12)
	vmulsd	%xmm0, %xmm13, %xmm5
	vaddsd	%xmm15, %xmm3, %xmm3
	vaddsd	%xmm14, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm4, %xmm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13388(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm15
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13336(%rbp), %xmm4, %xmm4
	vaddsd	%xmm15, %xmm2, %xmm2
	vmovss	%xmm4, 52(%r12)
	vmulsd	%xmm0, %xmm5, %xmm4
	vaddsd	%xmm15, %xmm13, %xmm13
	vmulsd	%xmm7, %xmm5, %xmm5
	vsubsd	%xmm4, %xmm3, %xmm3
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13384(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm14
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13332(%rbp), %xmm3, %xmm3
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm3, 60(%r12)
	vmulsd	%xmm0, %xmm4, %xmm3
	vaddsd	%xmm14, %xmm5, %xmm5
	vmulsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm3, %xmm2, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13380(%rbp), %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13328(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 68(%r12)
	vmulsd	%xmm0, %xmm3, %xmm2
	vsubsd	%xmm2, %xmm13, %xmm2
	vmulsd	%xmm6, %xmm3, %xmm13
	vmulsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13324(%rbp), %xmm2, %xmm2
	vaddsd	%xmm13, %xmm5, %xmm5
	vmovss	%xmm2, 76(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13376(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm15
	vmulsd	%xmm6, %xmm2, %xmm14
	vaddsd	%xmm13, %xmm4, %xmm4
	vmulsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm15, %xmm5, %xmm5
	vaddsd	%xmm14, %xmm4, %xmm4
	vaddsd	%xmm3, %xmm14, %xmm14
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13320(%rbp), %xmm5, %xmm5
	vmovss	%xmm5, 84(%r12)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13372(%rbp), %xmm5, %xmm5
	vmulsd	%xmm0, %xmm5, %xmm15
	vmulsd	%xmm6, %xmm5, %xmm5
	vsubsd	%xmm15, %xmm4, %xmm4
	vaddsd	%xmm5, %xmm14, %xmm14
	vaddsd	%xmm5, %xmm2, %xmm5
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13316(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 92(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13368(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm2
	vmulsd	%xmm0, %xmm4, %xmm13
	vmovd	%ecx, %xmm6
	vxorpd	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm5, %xmm5
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13364(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm2
	vsubsd	%xmm13, %xmm14, %xmm3
	vmulsd	%xmm0, %xmm1, %xmm0
	vmulsd	.LC28(%rip), %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13312(%rbp), %xmm3, %xmm3
	vsubsd	%xmm2, %xmm5, %xmm2
	vmovss	%xmm3, 100(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13308(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 108(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm5
	vmovd	%edx, %xmm6
	vcvtss2sd	%xmm6, %xmm3, %xmm3
	vmulsd	%xmm12, %xmm3, %xmm12
	vmovd	%eax, %xmm6
	vcvtss2sd	%xmm6, %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm11
	vmovd	%edi, %xmm6
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm12, %xmm0, %xmm0
	vaddsd	%xmm11, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vmovd	%esi, %xmm6
	vmovss	%xmm0, 116(%r12)
	vmulsd	.LC29(%rip), %xmm2, %xmm0
	vmovsd	.LC30(%rip), %xmm2
	vmulsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm2, %xmm3, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm4, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, 124(%r12)
	movq	%r13, %r12
	subl	$1, %ebx
	jne	.L1904
.L2019:
	movq	-13520(%rbp), %rax
	xorl	%edi, %edi
	addl	$1, %edi
	movq	-13616(%rbp), %r13
	leaq	128(%rax), %r8
	movq	%rax, %rsi
	cmpl	$32, %edi
	je	.L1925
	.p2align 4,,10
	.p2align 3
.L2021:
	movq	%r8, %rdx
	movl	%edi, %eax
	.p2align 4,,10
	.p2align 3
.L1906:
	movslq	%eax, %rcx
	vmovss	(%rdx), %xmm1
	addl	$1, %eax
	subq	$-128, %rdx
	leaq	(%rsi,%rcx,4), %rcx
	vmovss	(%rcx), %xmm0
	vmovss	%xmm1, (%rcx)
	vmovss	%xmm0, -128(%rdx)
	cmpl	$32, %eax
	jne	.L1906
	addl	$1, %edi
	addq	$132, %r8
	subq	$-128, %rsi
	cmpl	$32, %edi
	jne	.L2021
.L1925:
	movq	-13520(%rbp), %r12
	movl	$32, %r14d
	vmovsd	.LC34(%rip), %xmm8
	jmp	.L1905
	.p2align 4,,10
	.p2align 3
.L1908:
	subq	%r13, %rdx
	movq	%rdx, %rax
	shrq	$2, %rax
	jne	.L2022
.L1913:
	vxorpd	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13424(%rbp), %xmm4, %xmm4
	vmovsd	.LC36(%rip), %xmm6
	vcvtss2sd	-13420(%rbp), %xmm0, %xmm0
	vxorpd	%xmm3, %xmm3, %xmm3
	vmovsd	.LC35(%rip), %xmm5
	vcvtss2sd	-13416(%rbp), %xmm3, %xmm3
	vmulsd	%xmm6, %xmm4, %xmm1
	vmovss	-13360(%rbp), %xmm2
	subq	$-128, %r13
	vmulsd	%xmm5, %xmm4, %xmm4
	vmovaps	-13424(%rbp), %ymm7
	vmulsd	%xmm8, %xmm3, %xmm3
	vmovups	-13356(%rbp), %ymm11
	vmulsd	.LC33(%rip), %xmm0, %xmm0
	vcvtps2pd	%xmm7, %ymm13
	vextractf128	$0x1, %ymm7, %xmm7
	vcvtps2pd	%xmm7, %ymm7
	vmulpd	%ymm9, %ymm7, %ymm14
	vsubsd	%xmm0, %xmm1, %xmm1
	vaddsd	%xmm4, %xmm0, %xmm0
	vmovups	-13420(%rbp), %ymm4
	vaddsd	%xmm3, %xmm1, %xmm1
	vcvtps2pd	%xmm4, %ymm12
	vextractf128	$0x1, %ymm4, %xmm4
	vcvtps2pd	%xmm4, %ymm4
	vsubsd	%xmm3, %xmm0, %xmm0
	vaddpd	%ymm4, %ymm14, %ymm14
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vsubss	%xmm2, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm2, %xmm0
	vmovss	%xmm1, (%r12)
	vmovss	%xmm0, 4(%r12)
	vmovups	-13416(%rbp), %ymm0
	vcvtps2pd	%xmm0, %ymm3
	vextractf128	$0x1, %ymm0, %xmm2
	vmulpd	%ymm9, %ymm13, %ymm0
	vaddpd	%ymm12, %ymm0, %ymm0
	vmulpd	%ymm9, %ymm3, %ymm3
	vcvtps2pd	%xmm2, %ymm2
	vmulpd	%ymm9, %ymm2, %ymm2
	vsubpd	%ymm2, %ymm14, %ymm14
	vmulpd	.LC39(%rip), %ymm13, %ymm13
	vaddpd	%ymm13, %ymm12, %ymm12
	vsubpd	%ymm3, %ymm0, %ymm0
	vaddpd	%ymm12, %ymm3, %ymm3
	vcvtpd2psy	%ymm0, %xmm1
	vcvtpd2psy	%ymm14, %xmm0
	vinsertf128	$0x1, %xmm0, %ymm1, %ymm0
	vmulpd	.LC39(%rip), %ymm7, %ymm1
	vmovsd	.LC40(%rip), %xmm7
	vaddpd	%ymm1, %ymm4, %ymm1
	vsubps	%ymm11, %ymm0, %ymm0
	vcvtpd2psy	%ymm3, %xmm3
	vmovss	-13324(%rbp), %xmm4
	vaddpd	%ymm1, %ymm2, %ymm1
	vcvtpd2psy	%ymm1, %xmm1
	vinsertf128	$0x1, %xmm1, %ymm3, %ymm1
	vaddps	%ymm1, %ymm11, %ymm11
	vunpcklps	%ymm11, %ymm0, %ymm1
	vunpckhps	%ymm11, %ymm0, %ymm0
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vperm2f128	$49, %ymm0, %ymm1, %ymm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13392(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm3
	vmovups	%ymm2, 8(%r12)
	vmulsd	%xmm7, %xmm1, %xmm1
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13384(%rbp), %xmm2, %xmm2
	vmovups	%ymm0, 40(%r12)
	vmulsd	%xmm8, %xmm2, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13388(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm0, %xmm1
	vsubsd	%xmm11, %xmm3, %xmm3
	vaddsd	%xmm1, %xmm11, %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm4, %xmm3, %xmm3
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm4, %xmm1
	vmulsd	%xmm8, %xmm0, %xmm4
	vmulsd	%xmm7, %xmm0, %xmm0
	vmovss	%xmm3, 72(%r12)
	vmovss	-13320(%rbp), %xmm3
	vmovss	%xmm1, 76(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13380(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm12
	vaddsd	%xmm11, %xmm1, %xmm11
	vaddsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm0, %xmm0
	vmulsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm12, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm12, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm3, %xmm4, %xmm4
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm3, %xmm0
	vaddsd	%xmm1, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm1, %xmm1
	vmovss	%xmm4, 80(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13376(%rbp), %xmm4, %xmm4
	vmulsd	%xmm8, %xmm4, %xmm3
	vmovss	%xmm0, 84(%r12)
	vmovss	-13316(%rbp), %xmm0
	vaddsd	%xmm4, %xmm12, %xmm12
	vaddsd	%xmm1, %xmm4, %xmm1
	vmulsd	%xmm7, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm3, %xmm2
	vsubsd	%xmm3, %xmm11, %xmm11
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm0, %xmm2
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vsubss	%xmm0, %xmm11, %xmm11
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13372(%rbp), %xmm0, %xmm0
	vaddsd	%xmm0, %xmm3, %xmm3
	vmulsd	%xmm7, %xmm0, %xmm7
	vmovss	%xmm2, 92(%r12)
	vmulsd	%xmm8, %xmm0, %xmm2
	vaddsd	%xmm4, %xmm0, %xmm4
	vmovss	%xmm11, 88(%r12)
	vmovss	-13312(%rbp), %xmm11
	vaddsd	%xmm1, %xmm2, %xmm1
	vsubsd	%xmm2, %xmm12, %xmm12
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vaddss	%xmm1, %xmm11, %xmm1
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vsubss	%xmm11, %xmm12, %xmm12
	vmovss	%xmm1, 100(%r12)
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13368(%rbp), %xmm1, %xmm1
	vmulsd	%xmm8, %xmm1, %xmm11
	vmovss	%xmm12, 96(%r12)
	vmovss	-13308(%rbp), %xmm12
	vaddsd	%xmm7, %xmm1, %xmm0
	vsubsd	%xmm11, %xmm3, %xmm3
	vaddsd	%xmm4, %xmm11, %xmm4
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vsubss	%xmm12, %xmm3, %xmm3
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	%xmm4, %xmm12, %xmm4
	vmovss	-13304(%rbp), %xmm12
	vmovss	%xmm3, 104(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13364(%rbp), %xmm3, %xmm3
	vmulsd	%xmm8, %xmm3, %xmm11
	vmovss	%xmm4, 108(%r12)
	vaddsd	%xmm1, %xmm2, %xmm4
	vmulsd	%xmm5, %xmm3, %xmm5
	vsubsd	%xmm11, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm11, %xmm11
	vmulsd	.LC33(%rip), %xmm1, %xmm0
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vsubss	%xmm12, %xmm4, %xmm4
	vcvtsd2ss	%xmm11, %xmm11, %xmm11
	vaddss	%xmm11, %xmm12, %xmm11
	vaddsd	%xmm0, %xmm7, %xmm7
	vmovss	%xmm4, 112(%r12)
	vmovss	-13300(%rbp), %xmm4
	vsubsd	%xmm0, %xmm2, %xmm2
	vmulsd	%xmm6, %xmm3, %xmm0
	vmovss	%xmm11, 116(%r12)
	vaddsd	%xmm5, %xmm7, %xmm7
	vaddsd	%xmm0, %xmm2, %xmm0
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vsubss	%xmm4, %xmm7, %xmm7
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm0, %xmm4, %xmm0
	vmovss	%xmm7, 120(%r12)
	vmovss	%xmm0, 124(%r12)
	movq	%rbx, %r12
	subl	$1, %r14d
	je	.L2023
.L1905:
	movq	(%r12), %rax
	leaq	128(%r12), %rbx
	cmpl	$1, -13496(%rbp)
	movq	%rbx, %rdx
	movq	%rax, (%r15)
	movq	8(%r12), %rax
	movq	%rax, 8(%r15)
	movq	16(%r12), %rax
	movq	%rax, 16(%r15)
	movq	24(%r12), %rax
	movq	%rax, 24(%r15)
	movq	32(%r12), %rax
	movq	%rax, 32(%r15)
	movq	40(%r12), %rax
	movq	%rax, 40(%r15)
	movq	48(%r12), %rax
	movq	%rax, 48(%r15)
	movq	56(%r12), %rax
	movq	%rax, 56(%r15)
	ja	.L1908
	subq	%r13, %rdx
	movq	%rdx, %rax
	shrq	$2, %rax
	jne	.L2024
.L1909:
	cmpb	$0, -13504(%rbp)
	jne	.L1910
	movl	-13404(%rbp), %r8d
	movl	-13400(%rbp), %r9d
	movl	-13396(%rbp), %r10d
	movl	-13392(%rbp), %r11d
	movl	-13372(%rbp), %ecx
	movl	-13368(%rbp), %edx
	movl	-13364(%rbp), %eax
	movl	-13304(%rbp), %edi
	movl	-13300(%rbp), %esi
	vmovss	-13424(%rbp), %xmm0
	vmovss	-13420(%rbp), %xmm4
	vmovss	-13416(%rbp), %xmm3
	vmovss	-13412(%rbp), %xmm2
	vmovss	-13408(%rbp), %xmm15
	vmovss	-13388(%rbp), %xmm12
	vmovss	-13384(%rbp), %xmm7
	vmovss	-13380(%rbp), %xmm6
	vmovss	-13376(%rbp), %xmm1
	vmovss	-13360(%rbp), %xmm13
.L1911:
	vmovsd	.LC20(%rip), %xmm11
	vmovss	%xmm0, (%r12)
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	subq	$-128, %r13
	vmovss	%xmm4, 8(%r12)
	vcvtss2sd	%xmm4, %xmm4, %xmm4
	vmovss	%xmm3, 16(%r12)
	vmulsd	%xmm11, %xmm0, %xmm0
	vmovss	%xmm2, 24(%r12)
	vcvtss2sd	%xmm2, %xmm2, %xmm2
	vmovss	%xmm15, 32(%r12)
	movl	%r8d, 40(%r12)
	movl	%r9d, 48(%r12)
	movl	%r10d, 56(%r12)
	movl	%r11d, -128(%r13)
	vmovss	%xmm12, 72(%r12)
	vmovsd	.LC21(%rip), %xmm12
	vmovss	%xmm7, 80(%r12)
	vmovsd	.LC27(%rip), %xmm7
	vmulsd	%xmm12, %xmm4, %xmm4
	vmovss	%xmm6, 88(%r12)
	vmovsd	.LC26(%rip), %xmm6
	vmovss	%xmm1, 96(%r12)
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	movl	%ecx, 104(%r12)
	movl	%edx, 112(%r12)
	movl	%eax, 120(%r12)
	vaddsd	%xmm4, %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	%xmm3, %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm4
	vsubsd	%xmm4, %xmm0, %xmm3
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13420(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm5
	vmovsd	.LC22(%rip), %xmm0
	vmulsd	%xmm7, %xmm4, %xmm4
	vmulsd	%xmm0, %xmm2, %xmm2
	vaddsd	%xmm2, %xmm3, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13416(%rbp), %xmm3, %xmm3
	vmulsd	%xmm6, %xmm3, %xmm14
	vmulsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm13, %xmm2, %xmm2
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-13424(%rbp), %xmm13, %xmm13
	vmulsd	%xmm7, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm4, %xmm4
	vmovss	%xmm2, 4(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13412(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm15
	vaddsd	%xmm5, %xmm13, %xmm5
	vmulsd	%xmm6, %xmm2, %xmm13
	vmulsd	%xmm7, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm5, %xmm5
	vaddsd	%xmm13, %xmm4, %xmm4
	vsubsd	%xmm15, %xmm5, %xmm5
	vaddsd	%xmm13, %xmm3, %xmm3
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13356(%rbp), %xmm5, %xmm5
	vmovss	%xmm5, 12(%r12)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13408(%rbp), %xmm5, %xmm5
	vmulsd	%xmm0, %xmm5, %xmm15
	vmulsd	%xmm6, %xmm5, %xmm14
	vmulsd	%xmm7, %xmm5, %xmm5
	vsubsd	%xmm15, %xmm4, %xmm4
	vaddsd	%xmm14, %xmm3, %xmm3
	vaddsd	%xmm14, %xmm2, %xmm2
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13352(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 20(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13404(%rbp), %xmm4, %xmm4
	vmulsd	%xmm0, %xmm4, %xmm15
	vmulsd	%xmm6, %xmm4, %xmm13
	vmulsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm15, %xmm3, %xmm3
	vaddsd	%xmm13, %xmm2, %xmm2
	vaddsd	%xmm13, %xmm5, %xmm5
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13348(%rbp), %xmm3, %xmm3
	vmovss	%xmm3, 28(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13400(%rbp), %xmm3, %xmm3
	vmulsd	%xmm0, %xmm3, %xmm15
	vmulsd	%xmm6, %xmm3, %xmm14
	vcvtss2sd	-13392(%rbp), %xmm13, %xmm13
	vmulsd	%xmm7, %xmm3, %xmm3
	vsubsd	%xmm15, %xmm2, %xmm2
	vaddsd	%xmm14, %xmm5, %xmm5
	vaddsd	%xmm14, %xmm4, %xmm4
	vmulsd	%xmm6, %xmm13, %xmm14
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13344(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 36(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13396(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm15
	vsubsd	%xmm15, %xmm5, %xmm5
	vmulsd	%xmm6, %xmm2, %xmm15
	vmulsd	%xmm7, %xmm2, %xmm2
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13340(%rbp), %xmm5, %xmm5
	vaddsd	%xmm15, %xmm4, %xmm4
	vmovss	%xmm5, 44(%r12)
	vmulsd	%xmm0, %xmm13, %xmm5
	vaddsd	%xmm15, %xmm3, %xmm3
	vaddsd	%xmm14, %xmm2, %xmm2
	vmulsd	%xmm7, %xmm13, %xmm13
	vaddsd	%xmm14, %xmm3, %xmm3
	vsubsd	%xmm5, %xmm4, %xmm4
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13388(%rbp), %xmm5, %xmm5
	vmulsd	%xmm6, %xmm5, %xmm15
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13336(%rbp), %xmm4, %xmm4
	vaddsd	%xmm15, %xmm2, %xmm2
	vmovss	%xmm4, 52(%r12)
	vmulsd	%xmm0, %xmm5, %xmm4
	vaddsd	%xmm15, %xmm13, %xmm13
	vmulsd	%xmm7, %xmm5, %xmm5
	vsubsd	%xmm4, %xmm3, %xmm3
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13384(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm14
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13332(%rbp), %xmm3, %xmm3
	vaddsd	%xmm14, %xmm13, %xmm13
	vmovss	%xmm3, 60(%r12)
	vmulsd	%xmm0, %xmm4, %xmm3
	vaddsd	%xmm5, %xmm14, %xmm5
	vmulsd	%xmm7, %xmm4, %xmm4
	vsubsd	%xmm3, %xmm2, %xmm2
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13380(%rbp), %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13328(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 68(%r12)
	vmulsd	%xmm0, %xmm3, %xmm2
	vsubsd	%xmm2, %xmm13, %xmm2
	vmulsd	%xmm6, %xmm3, %xmm13
	vmulsd	%xmm7, %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13324(%rbp), %xmm2, %xmm2
	vaddsd	%xmm13, %xmm5, %xmm5
	vmovss	%xmm2, 76(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13376(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm15
	vmulsd	%xmm6, %xmm2, %xmm14
	vaddsd	%xmm13, %xmm4, %xmm4
	vmulsd	%xmm7, %xmm2, %xmm2
	vsubsd	%xmm15, %xmm5, %xmm5
	vaddsd	%xmm14, %xmm4, %xmm4
	vaddsd	%xmm14, %xmm3, %xmm14
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vaddss	-13320(%rbp), %xmm5, %xmm5
	vmovss	%xmm5, 84(%r12)
	vxorpd	%xmm5, %xmm5, %xmm5
	vcvtss2sd	-13372(%rbp), %xmm5, %xmm5
	vmulsd	%xmm0, %xmm5, %xmm15
	vmulsd	%xmm6, %xmm5, %xmm5
	vsubsd	%xmm15, %xmm4, %xmm4
	vaddsd	%xmm5, %xmm14, %xmm14
	vaddsd	%xmm5, %xmm2, %xmm5
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vaddss	-13316(%rbp), %xmm4, %xmm4
	vmovss	%xmm4, 92(%r12)
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13368(%rbp), %xmm4, %xmm4
	vmulsd	%xmm6, %xmm4, %xmm2
	vmulsd	%xmm0, %xmm4, %xmm13
	vmovd	%ecx, %xmm6
	vxorpd	%xmm4, %xmm4, %xmm4
	vaddsd	%xmm2, %xmm5, %xmm5
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13364(%rbp), %xmm2, %xmm2
	vmulsd	%xmm0, %xmm2, %xmm2
	vsubsd	%xmm13, %xmm14, %xmm3
	vmulsd	%xmm0, %xmm1, %xmm0
	vmulsd	.LC28(%rip), %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vaddss	-13312(%rbp), %xmm3, %xmm3
	vsubsd	%xmm2, %xmm5, %xmm2
	vmovss	%xmm3, 100(%r12)
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vaddss	-13308(%rbp), %xmm2, %xmm2
	vmovss	%xmm2, 108(%r12)
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	%xmm6, %xmm2, %xmm2
	vmulsd	%xmm11, %xmm2, %xmm5
	vmovd	%edx, %xmm6
	vcvtss2sd	%xmm6, %xmm3, %xmm3
	vmulsd	%xmm12, %xmm3, %xmm12
	vmovd	%eax, %xmm6
	vcvtss2sd	%xmm6, %xmm4, %xmm4
	vmulsd	%xmm11, %xmm4, %xmm11
	vmovd	%edi, %xmm6
	vsubsd	%xmm5, %xmm0, %xmm0
	vaddsd	%xmm12, %xmm0, %xmm0
	vaddsd	%xmm11, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vmovd	%esi, %xmm6
	vmovss	%xmm0, 116(%r12)
	vmulsd	.LC29(%rip), %xmm2, %xmm0
	vmovsd	.LC30(%rip), %xmm2
	vmulsd	%xmm2, %xmm4, %xmm4
	vaddsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm2, %xmm3, %xmm0
	vsubsd	%xmm0, %xmm1, %xmm0
	vaddsd	%xmm4, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, 124(%r12)
	movq	%rbx, %r12
	subl	$1, %r14d
	jne	.L1905
.L2023:
	addq	$4096, -13520(%rbp)
	movq	-13520(%rbp), %rax
	cmpq	-13632(%rbp), %rax
	jne	.L1914
	movq	-13656(%rbp), %rax
	testq	%rax, %rax
	je	.L1743
	leaq	-40(%rbp), %rsp
	movq	%rax, %rdi
	popq	%rbx
	popq	%r12
	popq	%r13
	.cfi_remember_state
	.cfi_def_cfa 13, 0
	popq	%r14
	popq	%r15
	popq	%rbp
	leaq	-16(%r13), %rsp
	.cfi_def_cfa 7, 16
	popq	%r13
	.cfi_def_cfa_offset 8
	jmp	_ZdlPv
.L1920:
	.cfi_restore_state
	xorl	%eax, %eax
	jmp	.L1768
.L2003:
	call	_ZSt17__throw_bad_allocv
	.p2align 4,,10
	.p2align 3
.L1822:
	cltq
	movq	%rax, %rcx
	salq	$5, %rcx
	addq	-13520(%rbp), %rcx
	movl	$0x00000000, (%rdx,%rcx,4)
	jmp	.L1823
.L1834:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vmovsd	.LC33(%rip), %xmm3
	vcvtss2sd	-13424(%rbp), %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13420(%rbp), %xmm1, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovss	-13360(%rbp), %xmm11
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13412(%rbp), %xmm7, %xmm7
	vmovss	-13356(%rbp), %xmm9
	vcvtss2sd	%xmm11, %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vmovss	-13352(%rbp), %xmm6
	vcvtss2sd	%xmm6, %xmm4, %xmm4
	vmulsd	%xmm3, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm9, %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13416(%rbp), %xmm0, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm0
	vmovss	-13348(%rbp), %xmm4
	vcvtss2sd	%xmm4, %xmm5, %xmm5
	vmulsd	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubsd	%xmm5, %xmm7, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	jmp	.L1835
.L1845:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	vxorpd	%xmm4, %xmm4, %xmm4
	vmovsd	.LC33(%rip), %xmm3
	vcvtss2sd	-13424(%rbp), %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13420(%rbp), %xmm1, %xmm1
	vxorpd	%xmm5, %xmm5, %xmm5
	vmovss	-13360(%rbp), %xmm11
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13412(%rbp), %xmm7, %xmm7
	vmovss	-13356(%rbp), %xmm9
	vcvtss2sd	%xmm11, %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vmovss	-13352(%rbp), %xmm6
	vcvtss2sd	%xmm6, %xmm4, %xmm4
	vmulsd	%xmm3, %xmm4, %xmm4
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm9, %xmm0, %xmm0
	vmulsd	%xmm3, %xmm0, %xmm0
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13416(%rbp), %xmm0, %xmm0
	vsubsd	%xmm4, %xmm0, %xmm0
	vmovss	-13348(%rbp), %xmm4
	vcvtss2sd	%xmm4, %xmm5, %xmm5
	vmulsd	%xmm3, %xmm5, %xmm5
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vsubsd	%xmm5, %xmm7, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	jmp	.L1846
.L1859:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	movl	-13360(%rbp), %r9d
	vcvtss2sd	-13424(%rbp), %xmm2, %xmm2
	movl	-13356(%rbp), %r8d
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-13420(%rbp), %xmm14, %xmm14
	vxorpd	%xmm8, %xmm8, %xmm8
	movl	-13352(%rbp), %edi
	vcvtss2sd	-13416(%rbp), %xmm8, %xmm8
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	movl	-13348(%rbp), %esi
	vcvtss2sd	-13412(%rbp), %xmm10, %xmm10
	vxorps	%xmm12, %xmm12, %xmm12
	vxorps	%xmm9, %xmm9, %xmm9
	vmovd	%r9d, %xmm6
	movl	-13344(%rbp), %ecx
	vxorps	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%r8d, %xmm6
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	-13396(%rbp), %xmm3, %xmm3
	movl	-13340(%rbp), %edx
	vmulsd	%xmm5, %xmm0, %xmm0
	movl	-13336(%rbp), %eax
	vmovss	-13332(%rbp), %xmm15
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%edi, %xmm6
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm14, %xmm14
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%esi, %xmm6
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubsd	%xmm0, %xmm8, %xmm8
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%ecx, %xmm6
	vmulsd	%xmm5, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmovd	%edx, %xmm6
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubsd	%xmm0, %xmm10, %xmm10
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13408(%rbp), %xmm0, %xmm0
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13404(%rbp), %xmm1, %xmm1
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm12, %xmm12
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%eax, %xmm6
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13400(%rbp), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm9, %xmm9
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm15, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vsubsd	%xmm0, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	jmp	.L1860
.L1870:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	movl	-13360(%rbp), %r9d
	vcvtss2sd	-13424(%rbp), %xmm2, %xmm2
	movl	-13356(%rbp), %r8d
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-13420(%rbp), %xmm14, %xmm14
	vxorpd	%xmm8, %xmm8, %xmm8
	movl	-13352(%rbp), %edi
	vcvtss2sd	-13416(%rbp), %xmm8, %xmm8
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	movl	-13348(%rbp), %esi
	vcvtss2sd	-13412(%rbp), %xmm10, %xmm10
	vxorps	%xmm12, %xmm12, %xmm12
	vxorps	%xmm9, %xmm9, %xmm9
	vmovd	%r9d, %xmm6
	movl	-13344(%rbp), %ecx
	vxorps	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%r8d, %xmm6
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	-13396(%rbp), %xmm3, %xmm3
	movl	-13340(%rbp), %edx
	vmulsd	%xmm5, %xmm0, %xmm0
	movl	-13336(%rbp), %eax
	vmovss	-13332(%rbp), %xmm15
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%edi, %xmm6
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm14, %xmm14
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%esi, %xmm6
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubsd	%xmm0, %xmm8, %xmm8
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%ecx, %xmm6
	vmulsd	%xmm5, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmovd	%edx, %xmm6
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubsd	%xmm0, %xmm10, %xmm10
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13408(%rbp), %xmm0, %xmm0
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13404(%rbp), %xmm1, %xmm1
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm12, %xmm12
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%eax, %xmm6
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13400(%rbp), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm9, %xmm9
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm15, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vsubsd	%xmm0, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	jmp	.L1871
.L1879:
	vxorpd	%xmm0, %xmm0, %xmm0
	vxorpd	%xmm2, %xmm2, %xmm2
	movl	-13360(%rbp), %r9d
	vcvtss2sd	-13424(%rbp), %xmm2, %xmm2
	movl	-13356(%rbp), %r8d
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-13420(%rbp), %xmm14, %xmm14
	vxorpd	%xmm8, %xmm8, %xmm8
	movl	-13352(%rbp), %edi
	vcvtss2sd	-13416(%rbp), %xmm8, %xmm8
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm10, %xmm10, %xmm10
	movl	-13348(%rbp), %esi
	vcvtss2sd	-13412(%rbp), %xmm10, %xmm10
	vxorps	%xmm12, %xmm12, %xmm12
	vxorps	%xmm9, %xmm9, %xmm9
	vmovd	%r9d, %xmm6
	movl	-13344(%rbp), %ecx
	vxorps	%xmm4, %xmm4, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%r8d, %xmm6
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	-13396(%rbp), %xmm3, %xmm3
	movl	-13340(%rbp), %edx
	vmulsd	%xmm5, %xmm0, %xmm0
	movl	-13336(%rbp), %eax
	vmovss	-13332(%rbp), %xmm15
	vsubsd	%xmm0, %xmm2, %xmm2
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%edi, %xmm6
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vsubsd	%xmm0, %xmm14, %xmm14
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%esi, %xmm6
	vcvtsd2ss	%xmm14, %xmm14, %xmm14
	vsubsd	%xmm0, %xmm8, %xmm8
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmovd	%ecx, %xmm6
	vmulsd	%xmm5, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmovd	%edx, %xmm6
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm8, %xmm8, %xmm8
	vsubsd	%xmm0, %xmm10, %xmm10
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13408(%rbp), %xmm0, %xmm0
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13404(%rbp), %xmm1, %xmm1
	vcvtsd2ss	%xmm10, %xmm10, %xmm10
	vcvtsd2ss	%xmm0, %xmm12, %xmm12
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm6, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vmovd	%eax, %xmm6
	vsubsd	%xmm0, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13400(%rbp), %xmm0, %xmm0
	vcvtsd2ss	%xmm1, %xmm9, %xmm9
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	%xmm6, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vsubsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm4, %xmm4
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	%xmm15, %xmm0, %xmm0
	vmulsd	%xmm5, %xmm0, %xmm0
	vsubsd	%xmm0, %xmm3, %xmm3
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	jmp	.L1880
.L2014:
	movq	%r13, %rsi
	vmovapd	%ymm10, -13616(%rbp)
	leaq	-13360(%rbp), %rdi
	vmovapd	%ymm9, -13584(%rbp)
	vmovsd	%xmm8, -13552(%rbp)
	vmovsd	%xmm11, -13512(%rbp)
	call	memcpy
	vmovapd	-13616(%rbp), %ymm10
	vmovapd	-13584(%rbp), %ymm9
	vmovsd	-13552(%rbp), %xmm8
	vmovsd	-13512(%rbp), %xmm11
	jmp	.L1890
.L1887:
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13424(%rbp), %xmm4, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13420(%rbp), %xmm3, %xmm3
	vcvtss2sd	-13348(%rbp), %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13344(%rbp), %xmm7, %xmm7
	vxorpd	%xmm2, %xmm2, %xmm2
	vcvtss2sd	-13416(%rbp), %xmm2, %xmm2
	vxorpd	%xmm15, %xmm15, %xmm15
	vcvtss2sd	-13404(%rbp), %xmm15, %xmm15
	vxorpd	%xmm13, %xmm13, %xmm13
	vcvtss2sd	-13400(%rbp), %xmm13, %xmm13
	vxorpd	%xmm12, %xmm12, %xmm12
	vmovss	-13360(%rbp), %xmm5
	vcvtss2sd	-13396(%rbp), %xmm12, %xmm12
	vxorpd	%xmm14, %xmm14, %xmm14
	vmovsd	.LC33(%rip), %xmm0
	vcvtss2sd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm1
	vmulsd	%xmm0, %xmm6, %xmm6
	vmulsd	%xmm0, %xmm7, %xmm7
	vsubsd	%xmm1, %xmm4, %xmm4
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13356(%rbp), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm1
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm4, -13424(%rbp)
	vsubsd	%xmm1, %xmm3, %xmm3
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13352(%rbp), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -13420(%rbp)
	vsubsd	%xmm1, %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13412(%rbp), %xmm1, %xmm1
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13408(%rbp), %xmm6, %xmm6
	vsubsd	%xmm7, %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13328(%rbp), %xmm7, %xmm7
	vmulsd	%xmm0, %xmm7, %xmm7
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vmovss	%xmm2, -13416(%rbp)
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vmovss	%xmm1, -13412(%rbp)
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -13408(%rbp)
	vmovd	%xmm6, %r10d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13340(%rbp), %xmm6, %xmm6
	vmulsd	%xmm0, %xmm6, %xmm6
	vsubsd	%xmm6, %xmm15, %xmm15
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm15, %xmm6, %xmm6
	vmovd	%xmm6, %edi
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13336(%rbp), %xmm6, %xmm6
	vmulsd	%xmm0, %xmm6, %xmm6
	vxorps	%xmm15, %xmm15, %xmm15
	movl	%edi, -13404(%rbp)
	vsubsd	%xmm6, %xmm13, %xmm13
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm13, %xmm6, %xmm6
	vmovd	%xmm6, %r8d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13332(%rbp), %xmm6, %xmm6
	vmulsd	%xmm0, %xmm6, %xmm6
	movl	%r8d, -13400(%rbp)
	vxorps	%xmm13, %xmm13, %xmm13
	vsubsd	%xmm6, %xmm12, %xmm12
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm12, %xmm6, %xmm6
	vmovd	%xmm6, %r9d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13392(%rbp), %xmm6, %xmm6
	vsubsd	%xmm7, %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13324(%rbp), %xmm7, %xmm7
	vmulsd	%xmm0, %xmm7, %xmm7
	movl	%r9d, -13396(%rbp)
	vxorps	%xmm12, %xmm12, %xmm12
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -13392(%rbp)
	vmovd	%xmm6, %r11d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13388(%rbp), %xmm6, %xmm6
	vsubsd	%xmm7, %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13320(%rbp), %xmm7, %xmm7
	vmulsd	%xmm0, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm15, %xmm15
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13384(%rbp), %xmm6, %xmm6
	vmovss	%xmm15, -13388(%rbp)
	vsubsd	%xmm7, %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm13, %xmm13
	vmovss	%xmm13, -13384(%rbp)
	vcvtss2sd	-13316(%rbp), %xmm7, %xmm7
	vmulsd	%xmm0, %xmm7, %xmm7
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13380(%rbp), %xmm6, %xmm6
	movl	-13304(%rbp), %esi
	movl	-13300(%rbp), %ecx
	vsubsd	%xmm7, %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13312(%rbp), %xmm7, %xmm7
	vmulsd	%xmm0, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm12, %xmm12
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13376(%rbp), %xmm6, %xmm6
	vmovss	%xmm12, -13380(%rbp)
	vsubsd	%xmm7, %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13308(%rbp), %xmm7, %xmm7
	vmulsd	%xmm0, %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovd	%xmm6, %edx
	vmovss	%xmm6, -13376(%rbp)
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13372(%rbp), %xmm6, %xmm6
	vsubsd	%xmm7, %xmm6, %xmm6
	vxorpd	%xmm7, %xmm7, %xmm7
	vcvtss2sd	-13368(%rbp), %xmm7, %xmm7
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovd	%xmm6, %eax
	vmovss	%xmm6, -13372(%rbp)
	vmovd	%esi, %xmm6
	vcvtss2sd	%xmm6, %xmm14, %xmm14
	vmulsd	%xmm0, %xmm14, %xmm14
	vxorps	%xmm6, %xmm6, %xmm6
	vsubsd	%xmm14, %xmm7, %xmm7
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	-13364(%rbp), %xmm14, %xmm14
	vcvtsd2ss	%xmm7, %xmm6, %xmm6
	vmovd	%ecx, %xmm7
	vmovss	%xmm6, -13368(%rbp)
	vcvtss2sd	%xmm7, %xmm7, %xmm7
	vmulsd	%xmm0, %xmm7, %xmm7
	vsubsd	%xmm7, %xmm14, %xmm0
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -13364(%rbp)
	jmp	.L1888
.L2016:
	movq	%r13, %rsi
	vmovapd	%ymm10, -13616(%rbp)
	leaq	-13360(%rbp), %rdi
	vmovapd	%ymm9, -13584(%rbp)
	vmovsd	%xmm8, -13552(%rbp)
	vmovsd	%xmm11, -13512(%rbp)
	call	memcpy
	vmovapd	-13616(%rbp), %ymm10
	vmovapd	-13584(%rbp), %ymm9
	vmovsd	-13552(%rbp), %xmm8
	vmovsd	-13512(%rbp), %xmm11
	jmp	.L1886
.L2018:
	movq	%r14, %rsi
	vmovapd	%ymm9, -13584(%rbp)
	leaq	-13360(%rbp), %rdi
	vmovapd	%ymm10, -13552(%rbp)
	vmovsd	%xmm8, -13512(%rbp)
	call	memcpy
	vmovapd	-13584(%rbp), %ymm9
	vmovapd	-13552(%rbp), %ymm10
	vmovsd	-13512(%rbp), %xmm8
	jmp	.L1903
.L1900:
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13424(%rbp), %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13420(%rbp), %xmm4, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13416(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13412(%rbp), %xmm2, %xmm2
	vcvtss2sd	-13340(%rbp), %xmm6, %xmm6
	vxorpd	%xmm15, %xmm15, %xmm15
	vcvtss2sd	-13408(%rbp), %xmm15, %xmm15
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-13388(%rbp), %xmm12, %xmm12
	vxorpd	%xmm7, %xmm7, %xmm7
	vmovss	-13360(%rbp), %xmm13
	vcvtss2sd	-13384(%rbp), %xmm7, %xmm7
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm14, %xmm14, %xmm14
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	%xmm13, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13356(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -13424(%rbp)
	vsubsd	%xmm1, %xmm4, %xmm4
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13352(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm4, -13420(%rbp)
	vsubsd	%xmm1, %xmm3, %xmm3
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13348(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -13416(%rbp)
	vsubsd	%xmm1, %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13344(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vmovss	%xmm2, -13412(%rbp)
	vsubsd	%xmm1, %xmm15, %xmm15
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13404(%rbp), %xmm1, %xmm1
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vmovss	%xmm15, -13408(%rbp)
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmovd	%xmm6, %r8d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13336(%rbp), %xmm6, %xmm6
	vmulsd	%xmm5, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13400(%rbp), %xmm1, %xmm1
	movl	%r8d, -13404(%rbp)
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmovd	%xmm6, %r9d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13332(%rbp), %xmm6, %xmm6
	vmulsd	%xmm5, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13396(%rbp), %xmm1, %xmm1
	movl	%r9d, -13400(%rbp)
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmovd	%xmm6, %r10d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13328(%rbp), %xmm6, %xmm6
	vmulsd	%xmm5, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13392(%rbp), %xmm1, %xmm1
	movl	%r10d, -13396(%rbp)
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13324(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovd	%xmm6, %r11d
	vxorpd	%xmm6, %xmm6, %xmm6
	movl	%r11d, -13392(%rbp)
	vsubsd	%xmm1, %xmm12, %xmm12
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13320(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vmovss	%xmm12, -13388(%rbp)
	vsubsd	%xmm1, %xmm7, %xmm7
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -13384(%rbp)
	vcvtss2sd	-13316(%rbp), %xmm1, %xmm1
	vcvtss2sd	-13312(%rbp), %xmm11, %xmm11
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtss2sd	-13308(%rbp), %xmm14, %xmm14
	vcvtss2sd	-13380(%rbp), %xmm6, %xmm6
	movl	-13304(%rbp), %edi
	vmulsd	%xmm5, %xmm11, %xmm11
	movl	-13300(%rbp), %esi
	vmulsd	%xmm5, %xmm14, %xmm14
	vsubsd	%xmm1, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13376(%rbp), %xmm1, %xmm1
	vsubsd	%xmm11, %xmm1, %xmm1
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-13372(%rbp), %xmm11, %xmm11
	vsubsd	%xmm14, %xmm11, %xmm11
	vxorps	%xmm14, %xmm14, %xmm14
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -13380(%rbp)
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vmovss	%xmm1, -13376(%rbp)
	vcvtsd2ss	%xmm11, %xmm14, %xmm14
	vmovd	%xmm14, %ecx
	vmovd	%edi, %xmm14
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	vmulsd	%xmm5, %xmm14, %xmm14
	vcvtss2sd	-13368(%rbp), %xmm11, %xmm11
	movl	%ecx, -13372(%rbp)
	vsubsd	%xmm14, %xmm11, %xmm11
	vxorps	%xmm14, %xmm14, %xmm14
	vcvtsd2ss	%xmm11, %xmm14, %xmm14
	vmovd	%esi, %xmm11
	vmovd	%xmm14, %edx
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	%xmm11, %xmm11, %xmm11
	vmulsd	%xmm5, %xmm11, %xmm11
	vcvtss2sd	-13364(%rbp), %xmm14, %xmm14
	movl	%edx, -13368(%rbp)
	vsubsd	%xmm11, %xmm14, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovd	%xmm5, %eax
	vmovss	%xmm5, -13364(%rbp)
	jmp	.L1901
.L2020:
	movq	%r14, %rsi
	vmovapd	%ymm9, -13584(%rbp)
	leaq	-13360(%rbp), %rdi
	vmovapd	%ymm10, -13552(%rbp)
	vmovsd	%xmm8, -13512(%rbp)
	call	memcpy
	vmovapd	-13584(%rbp), %ymm9
	vmovapd	-13552(%rbp), %ymm10
	vmovsd	-13512(%rbp), %xmm8
	jmp	.L1899
.L2022:
	movq	%r13, %rsi
	vmovapd	%ymm9, -13584(%rbp)
	leaq	-13360(%rbp), %rdi
	vmovapd	%ymm10, -13552(%rbp)
	vmovsd	%xmm8, -13512(%rbp)
	call	memcpy
	vmovapd	-13584(%rbp), %ymm9
	vmovapd	-13552(%rbp), %ymm10
	vmovsd	-13512(%rbp), %xmm8
	jmp	.L1913
.L1910:
	vxorpd	%xmm1, %xmm1, %xmm1
	vxorpd	%xmm0, %xmm0, %xmm0
	vcvtss2sd	-13424(%rbp), %xmm0, %xmm0
	vxorpd	%xmm4, %xmm4, %xmm4
	vcvtss2sd	-13420(%rbp), %xmm4, %xmm4
	vxorpd	%xmm3, %xmm3, %xmm3
	vcvtss2sd	-13416(%rbp), %xmm3, %xmm3
	vxorpd	%xmm2, %xmm2, %xmm2
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13412(%rbp), %xmm2, %xmm2
	vcvtss2sd	-13340(%rbp), %xmm6, %xmm6
	vxorpd	%xmm15, %xmm15, %xmm15
	vcvtss2sd	-13408(%rbp), %xmm15, %xmm15
	vxorpd	%xmm12, %xmm12, %xmm12
	vcvtss2sd	-13388(%rbp), %xmm12, %xmm12
	vxorpd	%xmm7, %xmm7, %xmm7
	vmovss	-13360(%rbp), %xmm13
	vcvtss2sd	-13384(%rbp), %xmm7, %xmm7
	vxorpd	%xmm11, %xmm11, %xmm11
	vxorpd	%xmm14, %xmm14, %xmm14
	vmovsd	.LC33(%rip), %xmm5
	vcvtss2sd	%xmm13, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vmulsd	%xmm5, %xmm6, %xmm6
	vsubsd	%xmm1, %xmm0, %xmm0
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13356(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, -13424(%rbp)
	vsubsd	%xmm1, %xmm4, %xmm4
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13352(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm4, %xmm4, %xmm4
	vmovss	%xmm4, -13420(%rbp)
	vsubsd	%xmm1, %xmm3, %xmm3
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13348(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm3, %xmm3, %xmm3
	vmovss	%xmm3, -13416(%rbp)
	vsubsd	%xmm1, %xmm2, %xmm2
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13344(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm2, %xmm2, %xmm2
	vmovss	%xmm2, -13412(%rbp)
	vsubsd	%xmm1, %xmm15, %xmm15
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13404(%rbp), %xmm1, %xmm1
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm15, %xmm15, %xmm15
	vmovss	%xmm15, -13408(%rbp)
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmovd	%xmm6, %r8d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13336(%rbp), %xmm6, %xmm6
	vmulsd	%xmm5, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13400(%rbp), %xmm1, %xmm1
	movl	%r8d, -13404(%rbp)
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmovd	%xmm6, %r9d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13332(%rbp), %xmm6, %xmm6
	vmulsd	%xmm5, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13396(%rbp), %xmm1, %xmm1
	movl	%r9d, -13400(%rbp)
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vmovd	%xmm6, %r10d
	vxorpd	%xmm6, %xmm6, %xmm6
	vcvtss2sd	-13328(%rbp), %xmm6, %xmm6
	vmulsd	%xmm5, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13392(%rbp), %xmm1, %xmm1
	movl	%r10d, -13396(%rbp)
	vsubsd	%xmm6, %xmm1, %xmm1
	vxorps	%xmm6, %xmm6, %xmm6
	vcvtsd2ss	%xmm1, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13324(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vmovd	%xmm6, %r11d
	vxorpd	%xmm6, %xmm6, %xmm6
	movl	%r11d, -13392(%rbp)
	vsubsd	%xmm1, %xmm12, %xmm12
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13320(%rbp), %xmm1, %xmm1
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtsd2ss	%xmm12, %xmm12, %xmm12
	vmovss	%xmm12, -13388(%rbp)
	vsubsd	%xmm1, %xmm7, %xmm7
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtsd2ss	%xmm7, %xmm7, %xmm7
	vmovss	%xmm7, -13384(%rbp)
	vcvtss2sd	-13316(%rbp), %xmm1, %xmm1
	vcvtss2sd	-13312(%rbp), %xmm11, %xmm11
	vmulsd	%xmm5, %xmm1, %xmm1
	vcvtss2sd	-13308(%rbp), %xmm14, %xmm14
	vcvtss2sd	-13380(%rbp), %xmm6, %xmm6
	movl	-13304(%rbp), %edi
	vmulsd	%xmm5, %xmm11, %xmm11
	movl	-13300(%rbp), %esi
	vmulsd	%xmm5, %xmm14, %xmm14
	vsubsd	%xmm1, %xmm6, %xmm6
	vxorpd	%xmm1, %xmm1, %xmm1
	vcvtss2sd	-13376(%rbp), %xmm1, %xmm1
	vsubsd	%xmm11, %xmm1, %xmm1
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	-13372(%rbp), %xmm11, %xmm11
	vsubsd	%xmm14, %xmm11, %xmm11
	vxorps	%xmm14, %xmm14, %xmm14
	vcvtsd2ss	%xmm6, %xmm6, %xmm6
	vmovss	%xmm6, -13380(%rbp)
	vcvtsd2ss	%xmm1, %xmm1, %xmm1
	vmovss	%xmm1, -13376(%rbp)
	vcvtsd2ss	%xmm11, %xmm14, %xmm14
	vmovd	%xmm14, %ecx
	vmovd	%edi, %xmm14
	vxorpd	%xmm11, %xmm11, %xmm11
	vcvtss2sd	%xmm14, %xmm14, %xmm14
	vmulsd	%xmm5, %xmm14, %xmm14
	vcvtss2sd	-13368(%rbp), %xmm11, %xmm11
	movl	%ecx, -13372(%rbp)
	vsubsd	%xmm14, %xmm11, %xmm11
	vxorps	%xmm14, %xmm14, %xmm14
	vcvtsd2ss	%xmm11, %xmm14, %xmm14
	vmovd	%esi, %xmm11
	vmovd	%xmm14, %edx
	vxorpd	%xmm14, %xmm14, %xmm14
	vcvtss2sd	%xmm11, %xmm11, %xmm11
	vmulsd	%xmm5, %xmm11, %xmm11
	vcvtss2sd	-13364(%rbp), %xmm14, %xmm14
	movl	%edx, -13368(%rbp)
	vsubsd	%xmm11, %xmm14, %xmm5
	vcvtsd2ss	%xmm5, %xmm5, %xmm5
	vmovd	%xmm5, %eax
	vmovss	%xmm5, -13364(%rbp)
	jmp	.L1911
.L2024:
	movq	%r13, %rsi
	vmovapd	%ymm9, -13584(%rbp)
	leaq	-13360(%rbp), %rdi
	vmovapd	%ymm10, -13552(%rbp)
	vmovsd	%xmm8, -13512(%rbp)
	call	memcpy
	vmovapd	-13584(%rbp), %ymm9
	vmovapd	-13552(%rbp), %ymm10
	vmovsd	-13512(%rbp), %xmm8
	jmp	.L1909
.L1743:
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	.cfi_def_cfa 13, 0
	popq	%r14
	popq	%r15
	popq	%rbp
	leaq	-16(%r13), %rsp
	.cfi_def_cfa 7, 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1383:
	.size	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi, .-_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi
	.section	.text.unlikely._ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,comdat
.LCOLDE76:
	.section	.text._ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,"axG",@progbits,_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi,comdat
.LHOTE76:
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,comdat
	.align 2
.LCOLDB77:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,comdat
.LHOTB77:
	.align 2
	.p2align 4,,15
	.weak	_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi
	.type	_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi, @function
_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi:
.LFB1390:
	.cfi_startproc
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	xorl	%eax, %eax
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	movl	%ecx, %r12d
	movl	$14, %ecx
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	movl	%esi, %ebp
	movl	$.LC59, %esi
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	movq	%rdi, %rbx
	subq	$120, %rsp
	.cfi_def_cfa_offset 160
	movq	%rsp, %rdi
	rep; stosq
	leaq	266280(%rbx), %rax
	movl	%edx, 8(%rsp)
	movq	%rsp, %rdi
	movq	%rax, (%rsp)
	movl	$112, %edx
	leaq	131104(%rbx), %rax
	movl	$135168, 32(%rsp)
	movq	%rax, 24(%rsp)
	call	inflateInit_
	testl	%eax, %eax
	je	.L2026
.L2027:
	movl	$.LC74, %edi
	call	puts
	call	abort
	.p2align 4,,10
	.p2align 3
.L2026:
	movl	$4, %esi
	movq	%rsp, %rdi
	call	inflate
	testl	%eax, %eax
	je	.L2027
	movq	40(%rsp), %r13
	movq	%rsp, %rdi
	call	inflateEnd
	movzbl	%bpl, %esi
	movl	%r12d, %ecx
	movq	%rbx, %rdi
	movslq	%r13d, %rdx
	call	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi
	addq	$120, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE1390:
	.size	_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi, .-_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi
	.section	.text.unlikely._ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,comdat
.LCOLDE77:
	.section	.text._ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,"axG",@progbits,_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi,comdat
.LHOTE77:
	.weak	_ZTS24WaveletCompressorGenericILi32EfE
	.section	.rodata._ZTS24WaveletCompressorGenericILi32EfE,"aG",@progbits,_ZTS24WaveletCompressorGenericILi32EfE,comdat
	.align 32
	.type	_ZTS24WaveletCompressorGenericILi32EfE, @object
	.size	_ZTS24WaveletCompressorGenericILi32EfE, 35
_ZTS24WaveletCompressorGenericILi32EfE:
	.string	"24WaveletCompressorGenericILi32EfE"
	.weak	_ZTI24WaveletCompressorGenericILi32EfE
	.section	.rodata._ZTI24WaveletCompressorGenericILi32EfE,"aG",@progbits,_ZTI24WaveletCompressorGenericILi32EfE,comdat
	.align 16
	.type	_ZTI24WaveletCompressorGenericILi32EfE, @object
	.size	_ZTI24WaveletCompressorGenericILi32EfE, 16
_ZTI24WaveletCompressorGenericILi32EfE:
	.quad	_ZTVN10__cxxabiv117__class_type_infoE+16
	.quad	_ZTS24WaveletCompressorGenericILi32EfE
	.weak	_ZTS29WaveletCompressorGeneric_zlibILi32EfE
	.section	.rodata._ZTS29WaveletCompressorGeneric_zlibILi32EfE,"aG",@progbits,_ZTS29WaveletCompressorGeneric_zlibILi32EfE,comdat
	.align 32
	.type	_ZTS29WaveletCompressorGeneric_zlibILi32EfE, @object
	.size	_ZTS29WaveletCompressorGeneric_zlibILi32EfE, 40
_ZTS29WaveletCompressorGeneric_zlibILi32EfE:
	.string	"29WaveletCompressorGeneric_zlibILi32EfE"
	.weak	_ZTI29WaveletCompressorGeneric_zlibILi32EfE
	.section	.rodata._ZTI29WaveletCompressorGeneric_zlibILi32EfE,"aG",@progbits,_ZTI29WaveletCompressorGeneric_zlibILi32EfE,comdat
	.align 16
	.type	_ZTI29WaveletCompressorGeneric_zlibILi32EfE, @object
	.size	_ZTI29WaveletCompressorGeneric_zlibILi32EfE, 24
_ZTI29WaveletCompressorGeneric_zlibILi32EfE:
	.quad	_ZTVN10__cxxabiv120__si_class_type_infoE+16
	.quad	_ZTS29WaveletCompressorGeneric_zlibILi32EfE
	.quad	_ZTI24WaveletCompressorGenericILi32EfE
	.weak	_ZTS24WaveletCompressorGenericILi256EfE
	.section	.rodata._ZTS24WaveletCompressorGenericILi256EfE,"aG",@progbits,_ZTS24WaveletCompressorGenericILi256EfE,comdat
	.align 32
	.type	_ZTS24WaveletCompressorGenericILi256EfE, @object
	.size	_ZTS24WaveletCompressorGenericILi256EfE, 36
_ZTS24WaveletCompressorGenericILi256EfE:
	.string	"24WaveletCompressorGenericILi256EfE"
	.weak	_ZTI24WaveletCompressorGenericILi256EfE
	.section	.rodata._ZTI24WaveletCompressorGenericILi256EfE,"aG",@progbits,_ZTI24WaveletCompressorGenericILi256EfE,comdat
	.align 16
	.type	_ZTI24WaveletCompressorGenericILi256EfE, @object
	.size	_ZTI24WaveletCompressorGenericILi256EfE, 16
_ZTI24WaveletCompressorGenericILi256EfE:
	.quad	_ZTVN10__cxxabiv117__class_type_infoE+16
	.quad	_ZTS24WaveletCompressorGenericILi256EfE
	.weak	_ZTS29WaveletCompressorGeneric_zlibILi256EfE
	.section	.rodata._ZTS29WaveletCompressorGeneric_zlibILi256EfE,"aG",@progbits,_ZTS29WaveletCompressorGeneric_zlibILi256EfE,comdat
	.align 32
	.type	_ZTS29WaveletCompressorGeneric_zlibILi256EfE, @object
	.size	_ZTS29WaveletCompressorGeneric_zlibILi256EfE, 41
_ZTS29WaveletCompressorGeneric_zlibILi256EfE:
	.string	"29WaveletCompressorGeneric_zlibILi256EfE"
	.weak	_ZTI29WaveletCompressorGeneric_zlibILi256EfE
	.section	.rodata._ZTI29WaveletCompressorGeneric_zlibILi256EfE,"aG",@progbits,_ZTI29WaveletCompressorGeneric_zlibILi256EfE,comdat
	.align 16
	.type	_ZTI29WaveletCompressorGeneric_zlibILi256EfE, @object
	.size	_ZTI29WaveletCompressorGeneric_zlibILi256EfE, 24
_ZTI29WaveletCompressorGeneric_zlibILi256EfE:
	.quad	_ZTVN10__cxxabiv120__si_class_type_infoE+16
	.quad	_ZTS29WaveletCompressorGeneric_zlibILi256EfE
	.quad	_ZTI24WaveletCompressorGenericILi256EfE
	.weak	_ZTV24WaveletCompressorGenericILi32EfE
	.section	.rodata._ZTV24WaveletCompressorGenericILi32EfE,"aG",@progbits,_ZTV24WaveletCompressorGenericILi32EfE,comdat
	.align 32
	.type	_ZTV24WaveletCompressorGenericILi32EfE, @object
	.size	_ZTV24WaveletCompressorGenericILi32EfE, 56
_ZTV24WaveletCompressorGenericILi32EfE:
	.quad	0
	.quad	_ZTI24WaveletCompressorGenericILi32EfE
	.quad	_ZN24WaveletCompressorGenericILi32EfE15compressed_dataEv
	.quad	_ZN24WaveletCompressorGenericILi32EfE8compressEfbi
	.quad	_ZN24WaveletCompressorGenericILi32EfE8compressEfbbi
	.quad	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmi
	.quad	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f
	.weak	_ZTV29WaveletCompressorGeneric_zlibILi32EfE
	.section	.rodata._ZTV29WaveletCompressorGeneric_zlibILi32EfE,"aG",@progbits,_ZTV29WaveletCompressorGeneric_zlibILi32EfE,comdat
	.align 32
	.type	_ZTV29WaveletCompressorGeneric_zlibILi32EfE, @object
	.size	_ZTV29WaveletCompressorGeneric_zlibILi32EfE, 56
_ZTV29WaveletCompressorGeneric_zlibILi32EfE:
	.quad	0
	.quad	_ZTI29WaveletCompressorGeneric_zlibILi32EfE
	.quad	_ZN29WaveletCompressorGeneric_zlibILi32EfE15compressed_dataEv
	.quad	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbi
	.quad	_ZN29WaveletCompressorGeneric_zlibILi32EfE8compressEfbbi
	.quad	_ZN29WaveletCompressorGeneric_zlibILi32EfE10decompressEbmi
	.quad	_ZN24WaveletCompressorGenericILi32EfE10decompressEbmiPA32_A32_f
	.weak	_ZTV24WaveletCompressorGenericILi256EfE
	.section	.rodata._ZTV24WaveletCompressorGenericILi256EfE,"aG",@progbits,_ZTV24WaveletCompressorGenericILi256EfE,comdat
	.align 32
	.type	_ZTV24WaveletCompressorGenericILi256EfE, @object
	.size	_ZTV24WaveletCompressorGenericILi256EfE, 56
_ZTV24WaveletCompressorGenericILi256EfE:
	.quad	0
	.quad	_ZTI24WaveletCompressorGenericILi256EfE
	.quad	_ZN24WaveletCompressorGenericILi256EfE15compressed_dataEv
	.quad	_ZN24WaveletCompressorGenericILi256EfE8compressEfbi
	.quad	_ZN24WaveletCompressorGenericILi256EfE8compressEfbbi
	.quad	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmi
	.quad	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f
	.weak	_ZTV29WaveletCompressorGeneric_zlibILi256EfE
	.section	.rodata._ZTV29WaveletCompressorGeneric_zlibILi256EfE,"aG",@progbits,_ZTV29WaveletCompressorGeneric_zlibILi256EfE,comdat
	.align 32
	.type	_ZTV29WaveletCompressorGeneric_zlibILi256EfE, @object
	.size	_ZTV29WaveletCompressorGeneric_zlibILi256EfE, 56
_ZTV29WaveletCompressorGeneric_zlibILi256EfE:
	.quad	0
	.quad	_ZTI29WaveletCompressorGeneric_zlibILi256EfE
	.quad	_ZN29WaveletCompressorGeneric_zlibILi256EfE15compressed_dataEv
	.quad	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbi
	.quad	_ZN29WaveletCompressorGeneric_zlibILi256EfE8compressEfbbi
	.quad	_ZN29WaveletCompressorGeneric_zlibILi256EfE10decompressEbmi
	.quad	_ZN24WaveletCompressorGenericILi256EfE10decompressEbmiPA256_A256_f
	.section	.rodata.cst16,"aM",@progbits,16
	.align 16
.LC15:
	.long	2147483647
	.long	0
	.long	0
	.long	0
	.section	.rodata.cst8,"aM",@progbits,8
	.align 8
.LC20:
	.long	0
	.long	1070858240
	.align 8
.LC21:
	.long	0
	.long	1072562176
	.align 8
.LC22:
	.long	0
	.long	1068498944
	.section	.rodata.cst32,"aM",@progbits,32
	.align 32
.LC23:
	.long	0
	.long	-1078984704
	.long	0
	.long	-1078984704
	.long	0
	.long	-1078984704
	.long	0
	.long	-1078984704
	.align 32
.LC24:
	.long	0
	.long	1071775744
	.long	0
	.long	1071775744
	.long	0
	.long	1071775744
	.long	0
	.long	1071775744
	.align 32
.LC25:
	.long	0
	.long	1068498944
	.long	0
	.long	1068498944
	.long	0
	.long	1068498944
	.long	0
	.long	1068498944
	.section	.rodata.cst8
	.align 8
.LC26:
	.long	0
	.long	1071775744
	.align 8
.LC27:
	.long	0
	.long	-1078984704
	.align 8
.LC28:
	.long	0
	.long	-1076625408
	.align 8
.LC29:
	.long	0
	.long	1073020928
	.align 8
.LC30:
	.long	0
	.long	1073840128
	.section	.rodata.cst32
	.align 32
.LC31:
	.long	0
	.long	1071644672
	.long	0
	.long	1071644672
	.long	0
	.long	1071644672
	.long	0
	.long	1071644672
	.align 32
.LC32:
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.section	.rodata.cst8
	.align 8
.LC33:
	.long	0
	.long	1071644672
	.align 8
.LC34:
	.long	0
	.long	1069547520
	.align 8
.LC35:
	.long	0
	.long	1071906816
	.align 8
.LC36:
	.long	0
	.long	1073086464
	.section	.rodata.cst4,"aM",@progbits,4
	.align 4
.LC37:
	.long	1056964608
	.section	.rodata.cst32
	.align 32
.LC38:
	.long	0
	.long	1069547520
	.long	0
	.long	1069547520
	.long	0
	.long	1069547520
	.long	0
	.long	1069547520
	.align 32
.LC39:
	.long	0
	.long	-1077936128
	.long	0
	.long	-1077936128
	.long	0
	.long	-1077936128
	.long	0
	.long	-1077936128
	.section	.rodata.cst8
	.align 8
.LC40:
	.long	0
	.long	-1077936128
	.section	.rodata.cst16
	.align 16
.LC44:
	.long	0
	.long	-1078984704
	.long	0
	.long	-1078984704
	.align 16
.LC45:
	.long	0
	.long	1071775744
	.long	0
	.long	1071775744
	.align 16
.LC46:
	.long	0
	.long	1068498944
	.long	0
	.long	1068498944
	.align 16
.LC47:
	.long	0
	.long	1069547520
	.long	0
	.long	1069547520
	.align 16
.LC48:
	.long	0
	.long	-1077936128
	.long	0
	.long	-1077936128
	.align 16
.LC49:
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.long	1056964608
	.align 16
.LC52:
	.long	0
	.long	1071644672
	.long	0
	.long	1071644672
	.section	.rodata.cst32
	.align 32
.LC54:
	.byte	0
	.byte	2
	.byte	4
	.byte	6
	.byte	8
	.byte	10
	.byte	12
	.byte	14
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	0
	.byte	2
	.byte	4
	.byte	6
	.byte	8
	.byte	10
	.byte	12
	.byte	14
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.align 32
.LC55:
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	0
	.byte	2
	.byte	4
	.byte	6
	.byte	8
	.byte	10
	.byte	12
	.byte	14
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	0
	.byte	2
	.byte	4
	.byte	6
	.byte	8
	.byte	10
	.byte	12
	.byte	14
	.align 32
.LC56:
	.byte	1
	.byte	3
	.byte	5
	.byte	7
	.byte	9
	.byte	11
	.byte	13
	.byte	15
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	1
	.byte	3
	.byte	5
	.byte	7
	.byte	9
	.byte	11
	.byte	13
	.byte	15
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.align 32
.LC57:
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	1
	.byte	3
	.byte	5
	.byte	7
	.byte	9
	.byte	11
	.byte	13
	.byte	15
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	1
	.byte	3
	.byte	5
	.byte	7
	.byte	9
	.byte	11
	.byte	13
	.byte	15
	.align 32
.LC69:
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.value	-32768
	.align 32
.LC70:
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.value	31744
	.align 32
.LC71:
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.value	1023
	.align 32
.LC72:
	.long	112
	.long	112
	.long	112
	.long	112
	.long	112
	.long	112
	.long	112
	.long	112
	.ident	"GCC: (GNU) 4.9.2 20141030 (Cray Inc.)"
	.section	.note.GNU-stack,"",@progbits
